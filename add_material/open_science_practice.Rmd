[//]: # to knit just this document do: rmarkdown::render("./add_material/open_science_practice.Rmd")

# Open Science Practice

# Introduction

## Why is it important? 

From ESDS book:
Open Science is enabled by the FAIR criteria for data use and sharing:

    Findable: Easy to find and well documented.
    Accessible: Long term storage and accessible with standard protocols.
    Interoperatable: Exchangeable and correctly cited.
    Reusable: Sufficiently documented and clear legal terms.

Environmental data is collected from a diversity of methods, including remote sensing, continuous site-scale measurements (e.g., meteorological variables, hydrological variables, ecosystem-atmosphere exchange fluxes of water vapour and CO2), episodic observations (e.g., forest inventories, biodiversity assessments), field campaigns, or experiments in the field or under laboratory conditions. Data from these sources come in different shapes and formats. In this course, we will learn to handle this diversity efficiently to answer research questions that you will likely encounter in the future. In many of the exercises, we will focus on data from continuous site scale observations, where meteorological variables are measured in parallel with ecosystem-atmosphere exchange fluxes of water vapour and CO2, collected by the eddy covariance technique. In addition, you will learn to tap into large environmental data repositories to complement your site-scale data.

At the end of each chapter, you will find an exercise that you can complete using the Renku cloud computing environment and the RStudio integrated development environment.

# Getting public data
Getting public data from widely used portals
e.g., Copernicus data, NASA data, etc. using their APIs
Koen’s package for soil database
ingestr?

# Structure of the project directory
## In a nutshell
- For each project that you are working on, create a separate directory in your home (e.g., ~/my_current_project/). All scripts that you use for the analysis to in here. Outputs from analyses or model runs, go to a separate sub-directory  ~/my_current_project/data/. Figures may go to ~/my_current_project/fig/.
- Separate original data and analysis/model outputs. Original data is what was collected in the field, obtained from collaborators, or downloaded from public sources.
- Create a new git repository for each project folder and sync all source code files in it with your GitHub (or bitbucket) account. 
- To make all your analyses reproducible, place a RMarkdown (for R) or Jupyter Notebook (for Python) file in  ~/my_current_project/ that documents all the steps - from reading in the original data from ~/data/<datasource>/, to data processing steps (document any decision regarding processing you made), and to final published plots and numbers. 
- The outputs (html, PDF, ...) from RMarkdown files or Jupyter Notebooks serve not only as a lab report for yourself, but can also be used to communicate your results with others, or provided as a supplementary material accompanying papers.

## Working environment
Keep things separate and in the right place. Original data, as downloaded from the web, goes into the subdirectory `~/data`. Code for your project (containing all steps from reading original data to final results and figures) goes into your project repository on GitHub. All code files of your project directory are part of a *git* repository that is synced with GitHub (or any other git remote host service, like GitLab, or Bitbucket). For reference, see Chapter 'Git in Project Management'. Note that figures and intermediate output should not go on git. You can collect any particularly useful functions that may be used also in other projects in your own personal utility package/library. An example is the *[rbeni](https://www.notion.so/TBC-rbeni-c7afcfef13cf4c9e838773d5adeeb22f)* package by Prof. Benjamin Stocker. Make sure it's well documented.

The project directory and other utility packages are synced with Github. Using *git*, the code of entire projects can be synced across multiple workstations and with your personal directory on your institution's cluster or cloud services (e.g. the HPC cluster 'Euler' at ETH Zurich). 

![](/Users/fgiardina/esds_book-master/figures/folder_organization_new.png) 



## Where does the data go?
Original data, as downloaded from the web, should always be in the data directory `~/data` and synced with your cluster. When you download data files from a particular source (e.g. accompanying a paper, or a particular satellite missing, etc.), create a subdirectory within `~/data` and place it in there, along with a `README` file (plain text) where you specify when and how the data was obtained (contact person, URL, etc.), what data use policies apply, who obtained it (your name) and how it should be cited. If original data was processed to a different format or re-gridded, please note it in the README, too (but not any further analysis).

Note that intermediate outputs produced by your project's code should not go on git. Git is only to back up code (and small plain-text data files, on the order of a about <100 MB). Still you may want to sync these intermediate outputs with your cluster as a backup of your analysis. 

![](/Users/fgiardina/esds_book-master/figures/data_location_new.png) 

## Reproducibility
Making your analysis reproducible - from the original files you downloaded on the web to final results and figures - is our highest standard. No excuses! Before submission of a paper, you should be able to demonstrate reproducibility. 

The key file here is a "workflow" script written on RMarkdown or Jupyter Notebook, that runs all steps of your analysis in the right order (or at least contains accurate descriptions of how stuff is run outside the project folder). 

A good habit is to copy figures produced by the workflow script to a separate directory synced with you cloud service (e.g. Polybox at ETH Zurich) that also contains the manuscript file. You may also have this as yet another git project directory. Upon publication, all outputs produced by the project directory for the paper are to be uploaded on *Zenodo* (or any other service that generates DOIs). Regularly *tag* your *git* project at important stages (e.g. first submission, final submission, etc.). Tags can form a *release* within Github and releases can be synced with *Zenodo* where each release gets its DOI (see chapter below). Refer to the code and output DOIs in your published paper.

![](/Users/fgiardina/esds_book-master/figures/reproducibility_new.png) 


## Best practice notes
- When working on git, avoid creating new files with “_v2”. Git makes this obsolete. If you want, you can always go back to earlier versions. If you want to keep a stable version while trying out something new, create a new “branch”, while the “master” remains the stable version. If the “_v2” files are the ones that are specific to *your* development and you are afraid of messing up the original code: don’t worry! You have already forked the repo and if the maintainer of the original code doesn’t want to adopt the changes in your fork, they won’t.
- Avoid writing data into programming language-specific formats (e.g. .RData), unless these are temporary files used for intermediate steps.
- Make sure that important figures (e.g. the ones in published papers) can be easily reproduced, starting from original data that were downloaded from (possibly open access-) sources. A good place to describe how to reproduce figures are RMarkdown files.
- When creating .RData files, put only one variable into them and name the .RData the same as the variable. E.g. file ‘df_combined.RData' contains only one variable, namely ‘df_combined’.

# Reporting science: from idea to publication
Add a few lines about creating a RMarkdown file e dire dove va creato according to Koen's structure

- Structure your code into parts that naturally go together. A code file should not contain more than a few hundred lines of code.
- Visually highlight the structure inside your code files.
- RMarkdown is a great tool to give structure to your code and make your code readable. You can implement the entire workflow of a project in one or a few RMarkdown files.
- If additional scripts are used outside of your main workflow script/RMarkdown file, point to them (by file name).
- Consider a hierarchy of sections. Explain briefly what is done by each code chunk.
- Make it easy for someone to reproduce your published Figure X.
- When using RMarkdown or Markdown, please follow the syntax for a nicely readable formatting (see [basics](https://rmarkdown.rstudio.com/authoring_basics.html), and [Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)).

# After publication: share your research

# References and further reading
Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, et al. (2014) *Best Practices for Scientific Computing.* PLoS Biol 12(1): e1001745. https://doi.org/10.1371/journal.pbio.1001745




