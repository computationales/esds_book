[//]: # to knit just this document do: rmarkdown::render("./add_material/open_science_practice.Rmd")

# Open Science Practice

# Introduction

## A bit of history
The importance of good data management transcends the success of a scientific project itself: it guarantees that the reproducibility of results, thus allowing the scientific community to integrate and reuse a workflow, a paramount concept for the advancement of Science. As Isaac Newton said in his 1675 letter: "If I have seen further it is by standing on the shoulders of Giants."

After an attempt to standardize the practice of open science in 2007, it was not until 2016 that representatives of academia, industry, funding agencies and publishers gathered to define a set of principle that would be known as the **FAIR Data Principles**. Later that year, during its 2016 G20 Hangzhou summit, the G20 approved the use of FAIR principle within research. In short, the FAIRification process stands consists in data being:

- **Findable**: data should be easy to find, both by humans and machines. There is a strong emphasis on the use of Metadata and the assignment of data identifiers.
- **Accessible**: the access to data should be open, free and universally implementable. If necessary, an authentication and authorisation procedure should be clearly explained.
- **Interoperatable**: refers to the ease of integration with other data. It should be easy to interoperate withing a workflow for analysis, storage and processing
- **Reusable**: the documentation should be clear enough to allow reusability, which is the ultimate goal of FAIR principles

## Why is it important? 
The EU is leading the way with FOSTER and the European Open Science Cloud (EOSC)

The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research

https://www.orion-openscience.eu/resources/open-science
Open Science (OS) is the movement to make scientific research, data and their dissemination available to any member of an inquiring society, from professionals to citizens. It impinges on principles of scientific growth and public access including practices such as publishing open research and campaigning for open access, with the ultimate aim of making it easier to publish and communicate scientific knowledge. From development to dissemination of knowledge, several concepts belong under the umbrella term of ‘Open Science’.
By broadening the principles of openness to the whole research cycle, OS fosters sharing and collaboration, bringing a systemic change to the way scientific research is done. The transition towards a comprehensive, effective open science is not an easy one; albeit challenging, a multifaceted cultural change remains essential to ensure scientific efforts have a real-world impact.
The Facilitate Open Science Training for European Research (FOSTER) https://www.fosteropenscience.eu/foster-taxonomy/open-science-policies, a European-funded project, has developed an OS taxonomy tree in an attempt to map the open science field.
https://eosc-portal.eu/about/eosc 

A key step in making science open and transparent is open access to research data, also known as open data. As research is more and more data-driven, progress in scientific knowledge becomes intimately tighten to data availability. Open data policy enables researchers to make use of existing knowledge in innovative, complementary ways.

Recent research finds that many published scientific findings might not be reliable. For example, researchers have reported being able to replicate only 40 percent or less of cancer biology results, and a large-scale attempt to replicate 100 recent psychology studies successfully reproduced fewer than half of the original results.

https://futurism.com/what-is-open-science-and-why-some-researchers-want-it
This has come to be called a “reproducibility crisis.” It’s pushed many scientists to look for ways to improve their research practices and increase study reliability. Practicing open science is one way to do so. When scientists share their underlying materials and data, other scientists can more easily evaluate and attempt to replicate them.
Also, open science can help speed scientific discovery. When scientists share their materials and data, others can use and analyze them in new ways, potentially leading to new discoveries. Some journals are specifically dedicated to publishing data sets for reuse (Scientific Data; Journal of Open Psychology Data). A paper in the latter has already been cited 17 times in under three years – nearly all these citations represent new discoveries, sometimes on topics unrelated to the original research.

Wait – open science sounds just like the way I learned in school that science works. How can this be new?

Under the status quo, science is shared through a single vehicle: Researchers publish journal articles summarizing their studies’ methods and results. The key word here is summary; to write a clear and succinct article, important details may be omitted. Journal articles are vetted via the peer review process, in which an editor and a few experts assess them for quality before publication. But – perhaps surprisingly – the primary data and materials underlying the article are almost never reviewed.
Historically, this made some sense because journal pages were limited, and storing and sharing materials and data were difficult. But with computers and the internet, it’s much easier to practice open science. It’s now feasible to store large quantities of information on personal computers, and online repositories to share study materials and data are becoming more common. Recently, some journals have even begun to require or reward open science practices like publicly posting materials and data.

Why isn’t open science the default? What incentives work against open science practices?

Two major forces work against adoption of open science practices: habits and reward structures. First, most established researchers have been practicing closed science for years, even decades, and changing these old habits requires some upfront time and effort. Technology is helping speed this process of adopting open habits, but behavioral change is hard.
Second, scientists, like other humans, tend to repeat behaviors that are rewarded and avoid those that are punished. Journal editors have tended to favor publishing papers that tell a tidy story with perfectly clear results. This has led researchers to craft their papers to be free from blemish, omitting “failed” studies that don’t clearly support their theories. But real data are often messy, so being fully transparent can open up researchers to critique.
Additionally, some researchers are afraid of being “scooped” – they worry someone will steal their idea and publish first. Or they fear that others will unfairly benefit from using shared data or materials without putting in as much effort.
Taken together, some researchers worry they will be punished for their openness and are skeptical that the perceived increase in workload that comes with adopting open science habits is needed and worthwhile. We believe scientists must continue to develop systems to allay fears and reward openness.

I’m Not a Scientist; Why Should I Care?
Science benefits everyone. If you’re reading this article now on a computer, or have ever benefited from an antibiotic, or kicked a bad habit following a psychologist’s advice, then you are a consumer of science. Open science (and its cousin, open access) means that anyone – including teachers, policymakers, journalists and other nonscientists – can access and evaluate study information.
Considering automatic enrollment in a 401k at work or whether to have that elective screening procedure at the doctor? Want to ensure your tax dollars are spent on policies and programs that actually work? Access to high-quality research evidence matters to you. Open materials and open data facilitate reuse of scientific products, increasing the value of every tax dollar invested. Improving science’s reliability and speed benefits us all.

### benefits
key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research.
Open science promotes a more accurate verification of scientific results. By combining the tools of science and information technologies, scientific enquiry and discovery can be sped up for the benefit of society. 
Open science promotes citizens’ trust in science. Greater citizen engagement leads to active participation in scientific experiments and data collection.
Practicing open science means that research outputs are accessible to all – not stuck behind pay walls. This helps to ensure that all researchers, and other stakeholders, have access to information regardless of their location or economic situation. It means that the research process can be accelerated and new knowledge can be more quickly generated and built upon to help solve grand challenges.
Open science offers a better return on investment from research funded by public money and contributes to better economic growth. https://www.openaire.eu/ec-policies-and-mandates 
For a researcher, opening the research outputs (e.g., research data) is a scientific merit. In Finland, e.g., in the CV templates of the Academy of Finland and the Finnish National Board on Research Integrity, promoting open science and research and, for example, responsible distribution and reuse of research material and datasets count as scientific and societal impact in research work, and are, therefore, worth adding to your CV.
By sharing your articles, data, code and methods, you are even multiplying the number of citable outputs for every project you work on. Your research will be more visible and understandable to others, which may mean that you might see your citation rate increase. If people can find and access your research, the potential impact of your research increases. In addition, practicing open science can foster new collaborations and research partnerships. All of which can help you to advance in your career.
Open science strengthens economies through developing a strong and independent national science base

### challenges
These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect early career researchers (ECRs) acutely.

Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research.

# Getting public data
Getting public data from widely used portals
e.g., Copernicus data, NASA data, etc. using their APIs
Koen’s package for soil database
ingestr?

# Structure of the project directory
## In a nutshell
- For each project that you are working on, create a separate directory in your home (e.g., ~/my_current_project/). All scripts that you use for the analysis to in here. Outputs from analyses or model runs, go to a separate sub-directory  ~/my_current_project/data/. Figures may go to ~/my_current_project/fig/.
- Separate original data and analysis/model outputs. Original data is what was collected in the field, obtained from collaborators, or downloaded from public sources.
- Create a new git repository for each project folder and sync all source code files in it with your GitHub (or bitbucket) account. 
- To make all your analyses reproducible, place a RMarkdown (for R) or Jupyter Notebook (for Python) file in  ~/my_current_project/ that documents all the steps - from reading in the original data from ~/data/<datasource>/, to data processing steps (document any decision regarding processing you made), and to final published plots and numbers. 
- The outputs (html, PDF, ...) from RMarkdown files or Jupyter Notebooks serve not only as a lab report for yourself, but can also be used to communicate your results with others, or provided as a supplementary material accompanying papers.

## Working environment
Keep things separate and in the right place. Original data, as downloaded from the web, goes into the subdirectory `~/data`. Code for your project (containing all steps from reading original data to final results and figures) goes into your project repository on GitHub. All code files of your project directory are part of a *git* repository that is synced with GitHub (or any other git remote host service, like GitLab, or Bitbucket). For reference, see Chapter 'Git in Project Management'. Note that figures and intermediate output should not go on git. You can collect any particularly useful functions that may be used also in other projects in your own personal utility package/library. An example is the *[rbeni](https://www.notion.so/TBC-rbeni-c7afcfef13cf4c9e838773d5adeeb22f)* package by Prof. Benjamin Stocker. Make sure it's well documented.

The project directory and other utility packages are synced with Github. Using *git*, the code of entire projects can be synced across multiple workstations and with your personal directory on your institution's cluster or cloud services (e.g. the HPC cluster 'Euler' at ETH Zurich). 

![](/Users/fgiardina/esds_book-master/figures/folder_organization_new.png) 



## Where does the data go?
Original data, as downloaded from the web, should always be in the data directory `~/data` and synced with your cluster. When you download data files from a particular source (e.g. accompanying a paper, or a particular satellite missing, etc.), create a subdirectory within `~/data` and place it in there, along with a `README` file (plain text) where you specify when and how the data was obtained (contact person, URL, etc.), what data use policies apply, who obtained it (your name) and how it should be cited. If original data was processed to a different format or re-gridded, please note it in the README, too (but not any further analysis).

Note that intermediate outputs produced by your project's code should not go on git. Git is only to back up code (and small plain-text data files, on the order of a about <100 MB). Still you may want to sync these intermediate outputs with your cluster as a backup of your analysis. 

![](/Users/fgiardina/esds_book-master/figures/data_location_new.png) 

## Reproducibility
Making your analysis reproducible - from the original files you downloaded on the web to final results and figures - is our highest standard. No excuses! Before submission of a paper, you should be able to demonstrate reproducibility. 

The key file here is a "workflow" script written on RMarkdown or Jupyter Notebook, that runs all steps of your analysis in the right order (or at least contains accurate descriptions of how stuff is run outside the project folder). 

A good habit is to copy figures produced by the workflow script to a separate directory synced with you cloud service (e.g. Polybox at ETH Zurich) that also contains the manuscript file. You may also have this as yet another git project directory. Upon publication, all outputs produced by the project directory for the paper are to be uploaded on *Zenodo* (or any other service that generates DOIs). Regularly *tag* your *git* project at important stages (e.g. first submission, final submission, etc.). Tags can form a *release* within Github and releases can be synced with *Zenodo* where each release gets its DOI (see chapter below). Refer to the code and output DOIs in your published paper.

![](/Users/fgiardina/esds_book-master/figures/reproducibility_new.png) 


## Best practice notes
- When working on git, avoid creating new files with “_v2”. Git makes this obsolete. If you want, you can always go back to earlier versions. If you want to keep a stable version while trying out something new, create a new “branch”, while the “master” remains the stable version. If the “_v2” files are the ones that are specific to *your* development and you are afraid of messing up the original code: don’t worry! You have already forked the repo and if the maintainer of the original code doesn’t want to adopt the changes in your fork, they won’t.
- Avoid writing data into programming language-specific formats (e.g. .RData), unless these are temporary files used for intermediate steps.
- Make sure that important figures (e.g. the ones in published papers) can be easily reproduced, starting from original data that were downloaded from (possibly open access-) sources. A good place to describe how to reproduce figures are RMarkdown files.
- When creating .RData files, put only one variable into them and name the .RData the same as the variable. E.g. file ‘df_combined.RData' contains only one variable, namely ‘df_combined’.

# Reporting science: from idea to publication
Add a few lines about creating a RMarkdown file e dire dove va creato according to Koen's structure

- Structure your code into parts that naturally go together. A code file should not contain more than a few hundred lines of code.
- Visually highlight the structure inside your code files.
- RMarkdown is a great tool to give structure to your code and make your code readable. You can implement the entire workflow of a project in one or a few RMarkdown files.
- If additional scripts are used outside of your main workflow script/RMarkdown file, point to them (by file name).
- Consider a hierarchy of sections. Explain briefly what is done by each code chunk.
- Make it easy for someone to reproduce your published Figure X.
- When using RMarkdown or Markdown, please follow the syntax for a nicely readable formatting (see [basics](https://rmarkdown.rstudio.com/authoring_basics.html), and [Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)).

# After publication: share your research

# References and further reading
1. Allen C, Mehler DMA (2019) Open science challenges, benefits and tips in early career and beyond. PLOS Biology 17(12): e3000587. https://doi.org/10.1371/journal.pbio.3000587
2. Wilkinson, Mark D et al. “The FAIR Guiding Principles for scientific data management and stewardship.” *Scientific data* vol. 3 160018. 15 Mar. 2016, doi:10.1038/sdata.2016.18
3. Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, et al. (2014) Best Practices for Scientific Computing. *PLoS Biol* 12(1): e1001745. https://doi.org/10.1371/journal.pbio.1001745




