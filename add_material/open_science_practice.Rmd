[//]: # to knit just this document do: rmarkdown::render("./add_material/open_science_practice.Rmd")

# Open Science Practice

# Introduction

## A bit of history
The importance of good data management transcends the success of a scientific project itself: it guarantees the reproducibility of results, thus encouraging the scientific community to reuse a workflow, a paramount concept for the advancement of Science. 

After an attempt to standardize the practice of open science in 2007, it was not until 2016 that representatives of academia, industry, funding agencies and publishers gathered to define a set of principles that would be known as the **FAIR Data Principles**. Later that year, during its 2016 G20 Hangzhou summit, the G20 approved the use of FAIR principle within research. In short, the FAIRification process consists in data being:

- **Findable**: data should be easy to find, both by humans and machines. There is a strong emphasis on the use of Metadata and the assignment of data identifiers.
- **Accessible**: the access to data should be open, free and universally implementable. If necessary, an authentication and authorization procedure should be clearly explained.
- **Interoperatable**: refers to the ease of integration with other data. It should be easy to interoperate within a workflow for analysis, storage and processing
- **Reusable**: the documentation should be clear enough to allow reusability, which is the ultimate goal of FAIR principles

## Why is it important?
Open Science refers to the effort to make scientific research available to the public, to foster the  dissemination of findings and the development of knowledge. It involves a methodical change to the research cycle, promoting more exchanges and collaborations. The shift to an open science practice is not trivial, and it will probably encompass a cultural change, but it will be essential to strengthen the link between scientific research and impacts in the real world. Contributing towards this effort, the Eropean Union is leading the way with its initiatives:

- [The Facilitate Open Science Training for European Research (FOSTER)](https://www.fosteropenscience.eu/foster-taxonomy/open-science-policies)
- [The European Open Science Cloud (EOSC)](https://eosc-portal.eu/about/eosc)
- [OpenAIRE](https://www.openaire.eu)

The transition to open science of the past few years has been motivated by a "reproducibility crisis", i.e. the failure to replicate scientific results. Recent evidence has found that less than half of scientific findings may not be reliable. Practicing open science offers a solution to this problem as well, as sharing materials and data eases the replication of original studies by other scientists. 
Open science is made possible by **open data**, i.e. when the data itself freely accessible to the public. Policies enabling open data allow researchers to innovate starting from existing knowledge. Other essential benefits for scientists include reputational gains, increased chances of publication, and a broader increase in the reliability of research, as it entails a meticulous verification of scientific results. This in turn boosts citizens' trust in science, which also leads to a more active participation in studies and experiments.  
The revolution of open science is not relegated to the academic world alone. The entire society is virtually a consumer of Science: from the computer you're reading this article on, to the vaccines that are taking the world out of the current pandemic.                           
Another key advantage of scientific results being accessible without a fee is that literally anyone on the planet can benefit from them, regardless of their location or economic situation. This is especially important for the scientific community in developing countries. In the end, open science speeds up the circulation of new information and helps generate solutions to the current great challenges. As Isaac Newton said in his 1675 letter: "If I have seen further it is by standing on the shoulders of Giants."  
Last but not least, making your code, data and methods available, you are creating more citable items per every published project. This will grant more visibility to your research, as well as possibly increasing the total number of times your work get referenced. It can trigger new collaborations and help building the final impact of your research. 

## What are the major obstacles
One could argue that what stated above must lay at the foundation of Science. However, there are some drawbacks that have been hindering the development of open science. These trace back to when journals were still in paper format, and storing and sharing materials and data was difficult. Luckily for us, technology is making it easier for us to practice open science.           
Another problem is that journal articles are basically a summary of long months (and sometimes years) of research. This means that in the final report, some part of it must be neglected, for the sake of brevity. Sometimes, the revision process itself does not check the primary data and materials. Moreover, by experience, papers with a clean story and clear results tend to be published more easily. This has pushed scientists to disregard studies that did not bring such results. But in the end, this phenomenon causes a impoverishment of science, as other scientists can benefit from those "failed" attempts. Being fully transparent is thus an opportunity to foster critical thinking too, as real data can (very) often be chaotic.    
Finally, another obstacle is the time constraint imposed by the current incentive structure. In the current academic world, publications are the easiest way for scientists to propel their careers forward. In this frame of things, it is thus hard to dedicate more time to a process which may not bring direct benefits. However, considering the bigger picture as described above, open science bring advantages both to the single researcher and the scientific community. 

# Getting public data
Getting public data from widely used portals
e.g., Copernicus data, NASA data, etc. using their APIs
Koen’s package for soil database
ingestr?

# Structure of the project directory
## In a nutshell
- For each project that you are working on, create a separate directory in your home (e.g., ~/my_current_project/). All scripts that you use for the analysis to in here. Outputs from analyses or model runs, go to a separate sub-directory  ~/my_current_project/data/. Figures may go to ~/my_current_project/fig/.
- Separate original data and analysis/model outputs. Original data is what was collected in the field, obtained from collaborators, or downloaded from public sources.
- Create a new git repository for each project folder and sync all source code files in it with your GitHub (or bitbucket) account. 
- To make all your analyses reproducible, place a RMarkdown (for R) or Jupyter Notebook (for Python) file in  ~/my_current_project/ that documents all the steps - from reading in the original data from ~/data/<datasource>/, to data processing steps (document any decision regarding processing you made), and to final published plots and numbers. 
- The outputs (html, PDF, ...) from RMarkdown files or Jupyter Notebooks serve not only as a lab report for yourself, but can also be used to communicate your results with others, or provided as a supplementary material accompanying papers.

## Working environment
Keep things separate and in the right place. Original data, as downloaded from the web, goes into the subdirectory `~/data`. Code for your project (containing all steps from reading original data to final results and figures) goes into your project repository on GitHub. All code files of your project directory are part of a *git* repository that is synced with GitHub (or any other git remote host service, like GitLab, or Bitbucket). For reference, see Chapter 'Git in Project Management'. Note that figures and intermediate output should not go on git. You can collect any particularly useful functions that may be used also in other projects in your own personal utility package/library. An example is the *[rbeni](https://www.notion.so/TBC-rbeni-c7afcfef13cf4c9e838773d5adeeb22f)* package by Prof. Benjamin Stocker. Make sure it's well documented.

The project directory and other utility packages are synced with Github. Using *git*, the code of entire projects can be synced across multiple workstations and with your personal directory on your institution's cluster or cloud services (e.g. the HPC cluster 'Euler' at ETH Zurich). 

![](/Users/fgiardina/esds_book-master/figures/folder_organization_new.png) 



## Where does the data go?
Original data, as downloaded from the web, should always be in the data directory `~/data` and synced with your cluster. When you download data files from a particular source (e.g. accompanying a paper, or a particular satellite missing, etc.), create a subdirectory within `~/data` and place it in there, along with a `README` file (plain text) where you specify when and how the data was obtained (contact person, URL, etc.), what data use policies apply, who obtained it (your name) and how it should be cited. If original data was processed to a different format or re-gridded, please note it in the README, too (but not any further analysis).

Note that intermediate outputs produced by your project's code should not go on git. Git is only to back up code (and small plain-text data files, on the order of a about <100 MB). Still you may want to sync these intermediate outputs with your cluster as a backup of your analysis. 

![](/Users/fgiardina/esds_book-master/figures/data_location_new.png) 

## Reproducibility
Making your analysis reproducible - from the original files you downloaded on the web to final results and figures - is our highest standard. No excuses! Before submission of a paper, you should be able to demonstrate reproducibility. 

The key file here is a "workflow" script written on RMarkdown or Jupyter Notebook, that runs all steps of your analysis in the right order (or at least contains accurate descriptions of how stuff is run outside the project folder). 

A good habit is to copy figures produced by the workflow script to a separate directory synced with you cloud service (e.g. Polybox at ETH Zurich) that also contains the manuscript file. You may also have this as yet another git project directory. Upon publication, all outputs produced by the project directory for the paper are to be uploaded on *Zenodo* (or any other service that generates DOIs). Regularly *tag* your *git* project at important stages (e.g. first submission, final submission, etc.). Tags can form a *release* within Github and releases can be synced with *Zenodo* where each release gets its DOI (see chapter below). Refer to the code and output DOIs in your published paper.

![](/Users/fgiardina/esds_book-master/figures/reproducibility_new.png) 


## Best practice notes
- When working on git, avoid creating new files with “_v2”. Git makes this obsolete. If you want, you can always go back to earlier versions. If you want to keep a stable version while trying out something new, create a new “branch”, while the “master” remains the stable version. If the “_v2” files are the ones that are specific to *your* development and you are afraid of messing up the original code: don’t worry! You have already forked the repo and if the maintainer of the original code doesn’t want to adopt the changes in your fork, they won’t.
- Avoid writing data into programming language-specific formats (e.g. .RData), unless these are temporary files used for intermediate steps.
- Make sure that important figures (e.g. the ones in published papers) can be easily reproduced, starting from original data that were downloaded from (possibly open access-) sources. A good place to describe how to reproduce figures are RMarkdown files.
- When creating .RData files, put only one variable into them and name the .RData the same as the variable. E.g. file ‘df_combined.RData' contains only one variable, namely ‘df_combined’.

# Reporting science: from idea to publication
Add a few lines about creating a RMarkdown file e dire dove va creato according to Koen's structure

- Structure your code into parts that naturally go together. A code file should not contain more than a few hundred lines of code.
- Visually highlight the structure inside your code files.
- RMarkdown is a great tool to give structure to your code and make your code readable. You can implement the entire workflow of a project in one or a few RMarkdown files.
- If additional scripts are used outside of your main workflow script/RMarkdown file, point to them (by file name).
- Consider a hierarchy of sections. Explain briefly what is done by each code chunk.
- Make it easy for someone to reproduce your published Figure X.
- When using RMarkdown or Markdown, please follow the syntax for a nicely readable formatting 

# After publication: share your research

# References and further reading
1. Allen C, Mehler DMA (2019) Open science challenges, benefits and tips in early career and beyond. PLOS Biology 17(12): e3000587. https://doi.org/10.1371/journal.pbio.3000587
2. Wilkinson, Mark D et al. “The FAIR Guiding Principles for scientific data management and stewardship.” *Scientific data* vol. 3 160018. 15 Mar. 2016, doi:10.1038/sdata.2016.18
3. Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, et al. (2014) Best Practices for Scientific Computing. *PLoS Biol* 12(1): e1001745. https://doi.org/10.1371/journal.pbio.1001745
4. RMarkdown resources:
[basics](https://rmarkdown.rstudio.com/authoring_basics.html), [PDF Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf), [Another longer cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)



