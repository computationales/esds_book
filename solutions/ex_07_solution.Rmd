---
title: "Exercise 07 - Solution"
output: html_document
---

# Supervised Machine Learning II

To start off, let's load all packages and data that will be needed:

```{r message=FALSE}
library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(caret)
library(recipes)
library(broom)

load("../data/ddf_allsites.RData")
```

**1. Adjusted-$R^2$**

a. Implement the formula for the adjusted-$R^2$ using simple "low-level" functions and $R^2$ defined as the coefficient of determination and, alternatively, the square of the Pearson's correlation coefficient. Compare the values of the two with what is returned by `summary(lm(..))$adj.r.squared`. Which one of the two is used by `summary(lm(..))`? For your investigations, generate correlated random data and apply functions on it.

```{r}
## 1. Generate random data
df_demo <- tibble(x = rnorm(100)) %>% 
  mutate(y_obs = x + rnorm(100),
         y_pred = x)

## 1a. Implement low-level functions

calc_myrsq_determination <- function(mod, obs){
  result <- 1 - sum((mod - obs)^2) / sum((obs - mean(obs))^2)
  return(result)
}

calc_myrsq_pearsons <- function(mod, obs){
  result <- (sum((mod - mean(mod))*(obs - mean(obs))))^2/
    (sum((obs - mean(obs))^2)*sum((mod - mean(mod))^2)) 
  return(result)
}

calc_myrsq_adjusted <- function(rsq, ndata, npred){
  result <- 1 - (1 - rsq) * (ndata - 1)/(ndata - npred - 1)
  return(result)
}

## 1b. Comparison of our own functions to *`summary()`
## output of summary
summary(lm(y_obs ~ y_pred, data = df_demo))$adj.r.squared

## using own function based on squared pearson's
myrsq_pearsons <- calc_myrsq_pearsons(df_demo$y_pred, df_demo$y_obs)
calc_myrsq_adjusted(myrsq_pearsons, ndata = nrow(df_demo), npred = 1)

## using own function based on coef of determination
myrsq_determination <- calc_myrsq_determination(df_demo$y_pred, df_demo$y_obs)
calc_myrsq_adjusted(myrsq_determination, ndata = nrow(df_demo), npred = 1)
```

*__Important:__ The output of `summary()` for the adjusted R2 is based on the squared Pearson's correlation coefficient, not the coefficient of determination.*

***
**2. Cross validation**

Assess the generalisability of three alternatives of a linear regression model for `GPP_NT_VUT_REF`:

a. with one predictor (`PPFD_IN`),
b. three predictors (`PPFD_IN`, `VPD_F`, and `TA_F,`),
c. all available predictors.

Which model works best in terms of its generalisability, assessed by 5-fold cross-validation and using the RMSE as the loss function?

Use the caret function `train()` with RMSE as the loss function and use the data from file `ddf_ch_lae.RData` (avoid `TIMESTAMP` and `NEE_VUT_REF_QC` as predictors!).

**Hint:**

To get the generalisation performance assessed during cross-validation (and not on the data held out from an initial split - which is not done here), you can use `summary(resamples(list(model1 = output_train1, ...)))`, where `output_train1` is the ouput of a `train()` function call with resampling. We see that by including all predictors, we get the best model performance in terms of it cross-validation results, i.e., the lowest root mean square error (RMSE) and the highest $R^2$, averaged across folds.

*We implement the model training as follows.*
```{r}
library(caret)

## Load daily data and rename for use below
load("../data/ddf_ch_lae.RData") # user-set path
df <- ddf_ch_lae %>% 
  select(-NEE_VUT_REF_QC, -TIMESTAMP) %>% 
  drop_na() 

set.seed(123)  # for reproducibility

## single predictor
cv_model1 <- train(
  GPP_NT_VUT_REF ~ PPFD_IN, 
  data = df, 
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5)
)

## three predictors
cv_model2 <- train(
  GPP_NT_VUT_REF ~ PPFD_IN + VPD_F + TA_F, 
  data = df, 
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5)
)

# all available predictors
cv_model3 <- train(
  GPP_NT_VUT_REF ~ ., 
  data = df, 
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5)
)
```

*You're  asked to assess generalisability based on the cross validation results. As indicated by the hint, we use the objects returned by the `train()` function as:*
```{r}
# Extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1, 
  model2 = cv_model2, 
  model3 = cv_model3
)))
```

*We see that by including all predictors, we get the best model performance in terms of it cross-validation results, i.e., the lowest root mean square error (RMSE) and the highest $R^2$, averaged across folds. This exercise it to demonstrate the application of the function `train()` and how to get results from the validation averaged resamples. The results shown above may not correspond to yours exactly. This is due to randomness. However, by setting the same seed (`set.seed()`), you should be able to get the exact same numbers. Do you?*

***
**3. Exploratory modelling**

Using the same data set as above (daily data for CH-Lae), find the model with the best performance, measured by evaluation against testing data held out from an initial split (30-70%). Use your own creativity to pre-process features and select variables, and chose between either a KNN or a multivariate linear regression, or any other ML model if you like (see models available with caret [here](https://topepo.github.io/caret/available-models.html)). Use all methods you've learned so far and impress your peers. Who gets the best generalisable model? Document and justify all steps you take. Use data from file `ddf_ch_lae.RData` (avoid `TIMESTAMP` and `NEE_VUT_REF_QC` as predictors).

*No example solution provided.*

***
**4. Resampling - Bonus problem**

a. Define a resampling function of the form `my_resample_folds_nested(df, k)` generating $k$ folds and taking a data frame `df` as arguments. This function should return a nested data frame with $k$ rows and two columns `train` and `test` that contain nested dataframes for training and testing data with each fold. Use data from file `ddf_ch_lae.RData` (avoid `TIMESTAMP` and `NEE_VUT_REF_QC` as predictors).
b. Print the dimensions of the whole data frame and run head() of fold two of the column 'train' of the nested data frame (this is just to make the peer review easier).

*"Relief option:"* You may create two separate flat data frames, one with testing, and one with training data, where one column specifies which fold each row belongs to. 

**Steps:**

1. Use the caret function `createFolds` to split a vector of row indices into a list of length corresponding to the number of folds, where each list element cotains the indices of the training resamples for the respective fold.
2. Use the dplyr function `slice()` to subset a dataframe by row numbers.
3. Use `group_by()` in combination with `nest()` for nesting data frames for each of the $k$ folds and end up with a dataframe that has $k$ rows.

```{r message=FALSE}
library(tidyverse)
library(caret)

# ## get df (should be made available directly on Renku as dataset for this session)
# load("04_data_wrangling/ddf_ch_lae.RData")
# df <- ddf_ch_lae

my_resample_folds_nested <- function(df, k){
  idx_test <- createFolds(1:nrow(df), k)
  purrr::map(as.list(seq(k)), ~slice(df, idx_test[[.]])) %>% 
    bind_rows(.id = "fold") %>% 
    group_by(fold) %>% 
    nest() %>% 
    rename(test = data) %>% 
    left_join(
      map(as.list(seq(k)), ~slice(df, -idx_test[[.]])) %>% 
        bind_rows(.id = "fold") %>% 
        group_by(fold) %>% 
        nest() %>% 
        rename(train = data),
      by = "fold"
    )
}

## check if it works
my_resample_folds_nested(df, 5)
```

4. Fit a linear model of the form `GPP_NT_VUT_REF ~ PPFD_IN`
    - directly on all the data in `df` and
    - on the resampled **training** folds. 
5. Evaluate the models by calculating metrics (squared Pearson's correlation coefficient, and RMSE) of predictions made
    - for the full dataframe `df`
    - for each respective **testing** fold separately. For this, report the mean across metrics calculated on each testing fold.

**Hints:**
*Feel free to come up with a solution that doesn't make use of these hints.*

- You may use `mutate( newcol = purrr::map( nested_col, ~function_name(.)) )` to apply a function `function_name()` on each of the nested data frames in column `nested_col`. The `.` refers to the position at which the argument `nested_col` goes.
- You may use the broom function `augment()` to add a column `.fitted` to a data frame, given a model object that was generated using predictors that are available as columns in the respective data frame.
- You may use `mutate( newcol = purrr::map2( arg1, arg2, ~function_name(.x, .y)) )` to apply a function `function_name()` that takes two arguments from other columns in the same data frame. The `.x` and `.y` refer to the position at which the two arguments go.

```{r message=FALSE}
library(yardstick)
library(broom)
library(purrr)

set.seed(123)

## (i) directly on all data
df <- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = df) %>% 
  augment(., newdata = df)

## (ii) on resampling folds separately
## fit model on training and predict on testing (complementing the testing df with augment)
df_metrics_resampled <- my_resample_folds_nested(df, 5) %>% 
  mutate(linmod_train = purrr::map(train, ~lm(GPP_NT_VUT_REF ~ PPFD_IN, data = .))) %>%
  mutate(test = purrr::map2(linmod_train, test, ~augment(.x, newdata = .y))) %>% 
  select(fold, test) %>% 
  unnest(test) %>% 
  group_by(fold) %>% 
  metrics(GPP_NT_VUT_REF, .fitted) %>% 
  group_by(.metric) %>% 
  summarise(.estimate = mean(.estimate))

df_metrics <- metrics(df, GPP_NT_VUT_REF, .fitted)

df_metrics_resampled
df_metrics

# df %>% analyse_modobs(".fitted", "GPP_NT_VUT_REF") # optional, to look at results, source the function defined in the tutorial 7
```

*Note that the cross-validation results and the metrics quantified directly on all the data (which is used for both training and testing), are quite similar. This is because linear regression models are on the "high bias" side of the bias-variance trade-off and therefore not strongly prone to overfitting. Cross validation results would deviate between (i) and (ii) if the model for (ii) is overfitted.*