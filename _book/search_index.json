[["data-scraping-in-progress.html", "Chapter 5 Data Scraping [in progress] 5.1 Introduction 5.2 Theory 5.3 Web Scraping Applied 5.4 Case Study: Species Richness and Red List species proportions 5.5 References 5.6 Exercise", " Chapter 5 Data Scraping [in progress] 5.1 Introduction This chapter covers data scraping in R and yes, this is not a typo. Scraping means to gather or to extract whereas scrapping means to get rid of something. The goal of the sections below is to gather and convert online data into a structured format that can be easily accessed and modified in R. As a case study, we will scrape data from a website containing various information about fish species, called FishBase. We will first learn how to access the website and how to extract useful useful information from plain text. Then, we will look at how to access a table that is embedded in the website and how to generate a new table in R which holds all information we want to extract. Next up is cleaning up the scraped data for visualization and analysis. At the end we will conduct a case study where we create a model to make predictions about fish species richness. 5.2 Theory Let us first have a brief review of the main concept of the lecture. The web is the largest source of information and often free to access. In some cases the information is already presented in a nice format and can be easily copied into an excel spreadsheet. However, this is often not the case and data-sets are only presented in a format that is not easy to download or modify. Manually copying data from different sub pages and sections is a tedious, slow and error prone approach. We therefore need a more automated and efficient technique to access such data. Web scraping is a popular technique to extract information directly from a website by accessing its underlying HTML code. Scraping allows us to gather this unstructured data from many websites and put it all together in a ready-to-use data format. This structured data can then be further used as training, validation or test data sets for our machine learning algorithms. Important Concepts of Websites HTTP: The Hypertext Transfer Protocol is a widely used protocol for information systems. Hypertext documents include hyperlinks that can be clicked to access other resources. Put simply, HTTP is the foundation of how information, i.e., data, is communicated in the world wide web. HTML: Hypertext Markup Language is the coding language in which most websites are written. This includes elements like formatting or structuring and is often combined using CSS or JavaScript. HTML is organized using tags, which are surrounded by &lt; &gt;. Each HTML document is made of elements that are specified using tags. HTML elements and HTML tags are often confused. Tags are used to open and close the object, whereas elements include both tags and its content. Let’s consider an example with the &lt;h1&gt; tag: &lt;h1&gt; Title of the document &lt;/h1&gt; is an element, and &lt;h1&gt;, &lt;/h1&gt; - are the tags enclosing it. The &lt;&gt; symbol is used to open a tag or an element and &lt;/&gt; is used to close it. HTML documents have a hierarchical or tree like structure with different types of nodes as described in 5.1. The rectangular boxes are referred to as nodes. The Text node is also called as a child node of the Element node and also a leaf node as it only contains text and no links to further nodes. Both of the Element nodes that are attached to the Root Element node are called sibling nodes of the Root Element node. When we do web scraping we go through such a hierarchical structure to get the content (here text in the Test node). Keep in mind that every HTML document can vary with respect to its structure. This example is to just to provide you some knowledge about HTML document and its structure. Figure 5.1: Visualization of the structure of an HTML document. XML: The Extensible Markup Language has some similarities to HTML but the format is generally easier to read for machines. While HTML has a number of pre-defined tags, one can create (“extend”) new tags as needed . This allows to define and control the meaning of the elements contained in a document or text. API: An Application Programming Interface is a set of procedures and communication protocols that provide access to the data of an application, operating system or other services. Both APIs and web scraping are used to retrieve data from websites, but their methodology differs substantially. APIs give us direct access to the data we would want, but they are limited to the corresponding website. As a result, we might find us in a scenario where there might not be an API to access the data we want. In these scenarios, web scrapping would allow us to access the data as long as it is available on a website. Hence APIs are very source/website specific and we can only do what is already implemented but in a clean fashion, while scrapping is more flexible and can be applied (nearly) everywhere but we have to handle all the formatting, inconsistencies, extraction, etc. by yourself. Next, we will demonstrate the principles of web scraping using a simple case study. We will extract fish occurrence data from an online database and perform some basic correlations with the obtained data. 5.3 Web Scraping Applied 5.3.1 R-Packages and Functions We will now load all packages necessary to perform web scraping on FishBase, namely RCurl and XML. RCurl provides functions to allow us to compose general HTTP requests and provides convenient functions to fetch URLs via get and post requests and process the results returned by the web server. XML give us approaches for both reading (get request) and creating (post request) XML and HTML documents, both locally and on the web via HTTP. Later in this tutorial, we will create some spatial visualization and load the packages raster, sf and rgdal to do so. Before we proceed further to accessing FishBase, we will have a quick look at the useful functions lapply() and sapply(). lib_vec &lt;- c(&quot;RCurl&quot;, &quot;XML&quot;, &quot;raster&quot;, &quot;rgdal&quot;, &quot;rfishbase&quot;, &quot;tidyverse&quot;, &quot;sf&quot;) sapply(lib_vec, library, character.only = TRUE) # See below for how sapply() works help(package = &quot;XML&quot;) # Use this code line in the RStudio console to learn more about loaded packages help(package = &quot;XML&quot;) # Use this code line in the RStudio console to learn more about loaded packages Sidenote: For data scraping in R one can also use ‘rjson’ to convert R objects into JSON objects and vice-versa. Another option we will use later are APIs. To get information about the loaded packages we can type the following command in the console: lapply() lapply() allows us to apply the same function to a vector of objects. The respective arguments are X (the vector or object we give as input) and FUN (the function which is applied to each element of X). The output of lapply() is a list of the same length as X where each element of X is the result of applying FUN to the corresponding element of X. X is a vector (atomic, list or data frame) or an expression object. The l in lapply() thus stands for list. A simple example is to change the string value of a matrix to lower case with the R-function tolower() function. First, we set up a vector with string objects that we want to apply the function on. Then we create a pipe that feeds this vector into lapply() which applies tolower. The output is a list of the original two string objects but now written in lower case. fish &lt;- c(&quot;FAMILY&quot;,&quot;SPECIES&quot;) fish %&gt;% lapply(tolower) # Note that tolower() must not be written with brackets ## [[1]] ## [1] &quot;family&quot; ## ## [[2]] ## [1] &quot;species&quot; sapply() sapply() takes a list, vector or data frame as input and gives a vector or matrix as output. This is very useful if we have to use another function which only accepts a vector as input but does not accept a list. Note that we can apply the same function using lapply() or sapply but their outputs are not the same. As an example we can have a look at the cars data where the speed and respective stopping distance are saved. We can apply the min() function to both features of the data set to get their minimal values. If we are using lapply(), we get a list that is accessible using $. With sapply() we get a named vector which can be seen at the headings speed and dist in the output below (those headings are of course the same as the variable names in the outputted list of lapply()). (df &lt;- head(cars)) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 df %&gt;% sapply(min) ## speed dist ## 4 2 df %&gt;% lapply(min) ## $speed ## [1] 4 ## ## $dist ## [1] 2 5.3.2 The FishBase website FishBase is a global species database of fish species. It provides data of various fish species, including information on taxonomy, geographical distribution, biometrics and morphology, behaviour and habitats, ecology and population dynamics as well as reproductive, metabolic and genetic data. Different tools, such as trophic pyramids, identification keys, biogeographical modelling and fishery statistics can be accessed on the website. Furthermore, direct species level links to information in other databases such as LarvalBase, GenBank, the IUCN Red List and the Catalog of Fishes exist. As of November 2018, FishBase included descriptions of 34,000 species and subspecies. Figure 5.2: Screenshot of the FishBase webpage for the species Coregonus lavaretus, a member of the family Salmonidae. As can be read in the distribution paragraph, it is widespread in freshwater systems from central and northwest Europe to Siberia. As shown in Figure 5.2, the website contains lots of information for all the different species and this information is stored in various different data types. A few examples of these data types are: Numbers in different formats and units (temperature ranges, latitudinal distribution) Text blocks (description of the distribution) Tables (fecundity, larvae information) Pictures and videos (of the species, embedded into HTML code) For a better understanding of the following R code, you are strongly encouraged to have a look at the FishBase website in your browser. In the next sections you will learn how to deal with these different complex data types. A careful approach is required when extracting and downloading such information. Understanding the structure of the website is an important first step and it will save you coding time. So, always identify the relevant information you want to extract first and then write a suitable extraction code which fits the needs of your machine learning algorithm. *** ### Accessing FishBase We are now ready to extract data from the Coregonus lavaretus website on FishBase. For this we first create a string object which holds the name of the fish profile we want to access. Using a variable to do this, adds flexibility in functions as will be shown below. If we want to get data about other species we can simply assign a new value to the object xwith the desired species name. x &lt;- &quot;Coregonus-lavaretus&quot; x ## [1] &quot;Coregonus-lavaretus&quot; Next, we use the function paste() to create an object holding the URL to the fish profile. Attaching strings like this is called concatenate and will be used quite often in later tutorials, so remember this wording. To create the URL we do not have to add any separation between the arguments, so we use sep = \"\". Using the flexibility of using x allows us here to add any species name directly into paste() instead of deleting and adding another Latin name every time. url &lt;- paste(&quot;http://www.fishbase.de/summary/&quot;, x ,&quot;.html&quot;,sep=&quot;&quot;) url ## [1] &quot;http://www.fishbase.de/summary/Coregonus-lavaretus.html&quot; For url we could also directly use http://www.fishbase.de/summary/Coregonus-lavaretus but then we would loose flexibility if we want to look for information about other species. Checkpoint Try to do the same for the URL of other species by changing the code above. Solution # Change the object x_end to desired species name x_ex&lt;- &quot;Salvelinus alpinus&quot; # Concatenate the URK url_ex &lt;- paste(&quot;http://www.fishbase.de/summary/&quot;,x_ex,&quot;.html&quot;,sep=&quot;&quot;) To access the URL and its HTML documents, we will use various functions from the XML package. First, we want to get the URL content using getURLContent() which takes a URL as input argument. Then, we use htmlParse() to read the HTML document from the URL content into an R object. To get help on functions we can always use help(\"htmlParse\"). fishbase &lt;- url %&gt;% getURLContent(followlocation = TRUE) %&gt;% htmlParse() The HTML document saved in fishbase is quite long to show it entirely but a snapshot is displayed in Figure 5.3. Figure 5.3: Extract from the HTML document saved in the variable fishbase*. Somewhere in this gigantic object we can find the relevant information we are looking for. For example, Figure 5.4 below shows the extract where the maximal lenght of the fish is documented. Check out the online profile to double check whether the value is correct. Figure 5.4: Extract of fishbase* documenting maximal length of Coregonus lavaretus. Highlighted in red are the opening and closing tags of the element where the information on the maximal length is stored. Knowing these tags, we can identify where our information is stored and how to extract it. In this example, we need to target either the span or \\div tags. In order to extract the information inside these two tags, we use the function getNodeSet(). This function finds XML nodes that match a particular criterion. span is used for a small chunk of HTML inside a line whereas div is used to group larger chunks of code. fishbase_div &lt;- fishbase %&gt;% getNodeSet(&quot;//div &quot;) fishbase_span &lt;- fishbase %&gt;% getNodeSet(&quot;//span &quot;) In the next two Figures 5.5 and 5.6 we can see the differences of what is stored in variable fishbase and fishbase_div. The difference to fishbase_span is similar. We see that instead of having this one long unstructured extract in Figure 5.5, we have now a list of objects that we easily access. In Figure 5.6, the list can be identified by the [[22]] in the top left corner which marks the position of the object within the list. In short, getNodeSet() identifies all the sections for a given tag (i.e., nodes), separates them and gathers them in a list. Figure 5.5: Fishbase before the command getNodeSet() Figure 5.6: Fishbase after the command getNodeSet() 5.3.3 Scraping Numbers Next, we now want to extract the maximal body length of Coregonus lavaretus out of the list of nodes we created in the previous section. For this, we will use the function xmlValue() which allows us to access the text in the nodes by converting them into strings. To proceed further, we briefly have a look at regular expressions. A regular expression is basically a sequence of characters (think of a certain word or a number) that can be searched for in a string (which is nothing else but a sequence of characters). The regexec() (for regular expression) function allows us to search for a pattern within a string. The output of regexec() gives you different information. For now it enough to know that it returns a list where the first object holds the position of the pattern in the string or simply a -1 if the pattern is not found. In our case, we are going to look for the pattern “Max length” because after this pattern, the value we are looking for is saved (see Figure 5.7). Figure 5.7: In blue highlighted is the regular expression on the FishBase website which we are looking for. The code to identify the position where the regular expression Max Length is located is rather complex, so here is a step-by-step breakdown. You can run each part of the pipe below one-by-one to better understand what is happening. pos &lt;- fishbase_span %&gt;% lapply(xmlValue) %&gt;% # 1. Convert all list objects in fishbase_span into strings regexec(pattern =&quot; Max length&quot;) %&gt;% # 2. Search strings for pattern which.max() # 3. Return position in list where highest number is found # (just one way to identify the node with relevant information) pos ## [1] 42 Now we know that our regular expression Max length can be found in node number 42. To get access to the text in this node we simply access fishbase_span via the list notation [[ ]] and turn the object into a string and save it as fish_length. As you can see in the output, the information on max length has been found correctly. fish_length &lt;- fishbase_span[[pos]] %&gt;% xmlValue() fish_length ## [1] &quot;\\r\\n\\t\\t\\t\\t\\tMaturity: Lm27.1, range 40 - ? cm Max length : 73.0 cm TL male/unsexed; (Ref. 40637); max. published weight: 10.0 kg (Ref. 35388)\\t\\t\\t\\t&quot; We now use the function substr(), which you already encountered in previous chapters. It allows to extract a section of a string that lies between two characters. Think of this as extracting a number interval in a series of numbers. Unfortuntaly, identifying this start and end has to be done somewhat by hand. In Figure 5.7 you can see that, starting from the ‘M’ in Max Length, there are 13 characters until the value begins with a 7 and 16 characters until it ends with a 0. Keep in mind that every blank space is also counted as character. Knowing this, we can first use regexec() to get the character position where Max Length starts and just add 13, respectively 16 to the value. Two things to note here: (i) Keep in mind that every blank space is also counted. (ii) Note that the output of regexec() is a list where the first object is the character position where the pattern begins in the string. Thus, we need to access this position number by using [[1]][1]. Another way instead of using regexec() is just by typing in some numbers into substr() and find the relevant characters by trial and error. start_M &lt;- fish_length %&gt;% regexec(pattern= &quot;Max length&quot;) max_length &lt;- fish_length %&gt;% substr(start_M[[1]][1] + 13, start_M[[1]][1] + 16) max_length ## [1] &quot;73.0&quot; At last, we have to convert the value of the maximal length (which is a string of characters) into a number using the function as.double(). This converts the value into a machine readable format which makes things easier later. We see from the output below that we correctly exctracted a maximal length of Coregonus lavaretus of 73 cm. Now, we can start thinking about looping this approach to extract the maximal length of various species and collect them in a nicely formatted data frame. To demonstrate fexibility in data reading: This entire approach can also be used to for example to identify any numeric value that is followed by any one or enclosed by any two characters. max_length &lt;- as.double(max_length) max_length ## [1] 73 5.3.4 Scraping Text Snippetrs International Union for Conservation of Nature Another interesting information on FishBase is the IUCN (International Union for Conservation of Nature) status of each species. The IUCN Red List of threatened Species has evolved to become the world’s most comprehensive information source on the global extinction risk status of animal, fungus and plant species. The IUCN Red List is a critical indicator of world’s biodiversity and has been established in 1994. It contains explicit criteria and categories to classify the conservation status of individual species on the basis of their probability of extinction. After a species is evaluated by the IUCN, it is placed into one of eight categories based on its current conservation status as shown in the figure. Far more than a list of species and their status, it is a powerful tool to inform and catalyze action for biodiversity conservation and policy change, critical to protecting the natural resources we depend on for survival. The IUCN also provides information about the range, population size, habitat and ecology, use and/or trade, threats, and conservation actions that will help inform necessary conservation decisions. Figure 5.8: Structure of IUCN categories. Extracting IUCN Status Now that we learned how to extract values, we can move on to extracting text snippets! In this part, we are going to get the IUCN Status of Coregonus lavaretus. In contrary to above, we are going to use the function which() (not which.max()) to find the position of the elements we are looking for. We use regexec() to search for matches of our pattern within each element of a character vector. In this case, the pattern is simply IUCN. As for the maximal length, we can look up this pattern on the website we are scraping from. IUCN_pos &lt;- which( # which() gives us the numbers of nodes where pattern was found fishbase_div %&gt;% # pipe from above to find pattern put into which() lapply(xmlValue) %&gt;% regexec(pattern = &quot;IUCN&quot;) # end of pipe &gt; 0 # check for which() to only give positive numbers for nodes ) # we do this because if pattern is not found, regexec returns a -1 IUCN_pos # pattern can be found in nodes 5, 14 and 24 ## [1] 5 14 24 If the pattern cannot be found in fishbase_div, the variable w_IUCN will be empty (we use fishbase_div here because the pattern was not found in fishbase_span). To avoid complications later, we better do a quick check and set the variable to NA if it is empty and else use xmlValue() to get the value at the last node where the pattern was found. if(length(IUCN_pos)==0){ # if which() from above returned 0 (no node with &gt;0) IUCN_stat = &quot;NA&quot; # set status to NA } else { # else IUCN_xml &lt;- fishbase_div[[ # access fishbase_div nodes with [[ IUCN_pos[length(IUCN_pos)]]] %&gt;% # access last node where pattern was found, length(IUCN_pos) = 24 xmlValue() # feed node into xmlValue() to return readable string } IUCN_xml ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tIUCN Red List Status (Ref. 120744)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Vulnerable (VU) (D2); Date assessed: 01 January 2008\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; The IUCN_xml looks a little confusing and we only need a tiny part of it, namely VU. We can see that in this node VU has a unique character pattern which does not appear twice: The closing bracket )is right after U, an alphabetical character. We can use this pattern to extract VU! Here, the str_extract() function from tidyverse comes in very handy. We can directly specify the pattern we are looking for and extract the substring from the original string. Note that for generalization, we need to specify [[:alpha:]] for any alphabetical character +[)] for the closing bracket. However, if we do so, we still extract the bracket as well. To get rid of it, we can simply remove it by replacing it with “nothing” using str_replace(). Instead of specifying the closing bracket, we use [:punct:] which generalizes punctuations like . , : ; ? ! etc. As we see in the output below, the IUCN status VU has been correctly extracted - nice! Plus, we don’t need to convert it and can use it as string later on. IUCN_sta &lt;- IUCN_xml %&gt;% str_extract(pattern = &quot;[:alpha:]+[)]&quot;) %&gt;% str_replace(pattern = &quot;[:punct:]&quot;, replacement = &quot;&quot;) IUCN_sta ## [1] &quot;VU&quot; 5.3.5 Scraping Tables The next step is to read a table from a website. Here, we are going to get information on the eggs of Coregonus lavaretus. Have a look at its profile. In section Life cycle and mating behavior you can see that the table for information on eggs is a link and not a table directly. Foruntatley for us, we can use the function getHTMLLinks() to retrieve links within an HTML document or the collection of names of external files referenced in an HTML document. In Figure 5.9, an extract is shown of the list with more than 100 links found on the profile page. We see that either we could directly access link number 100 or search for the substring FishEggInfoSummary to look for other links. In fact, we will find two links with this substring (see Figure 5.10. For this reason, we quickly check if the links are identical of if they lead to different pages. We can do this by using the logic operator ==. The link comparison gives TRUE as output meaning that both links are completely identical and it does not matter which one we use. Let us save the first one in egg_link. egg_link &lt;- fishbase %&gt;% getHTMLLinks() %&gt;% str_subset(pattern = &quot;FishEggInfoSummary&quot;) egg_link[1] == egg_link[2] ## [1] TRUE egg_link &lt;- egg_link[1] Figure 5.9: Extract of the list of links found on the FishBase profile of Coregonus lavaretus. Figure 5.10: List of links that have the substring FishEggInfoSummary. Checkpoint Call the variable egg_link, you will notice the link begins with ‘..’ (see above 5.10). Try to remove these dots so that the link begins with ‘/Reproduction/..’. As a hint, you can use the function str_replace() as was done above. You will need this code later, so make sure that you run it. # your code Solution egg_link &lt;- egg_link %&gt;% str_replace(&quot;..&quot;, &quot;&quot; ) egg_link ## [1] &quot;/Reproduction/FishEggInfoSummary.php?ID=232&amp;GenusName=Coregonus&amp;SpeciesName=lavaretus&amp;fc=76&amp;StockCode=246&quot; As you can see, this is no proper URL yet. Thus, we first need to create a working URL to access its content. Similarly to what we did previously, we do this using the function getURLContent(). Since we know that this link leads to a table, we can pipe the URL content into readHTMLTable(). The output here is a list and we have access the first object as done below. egg_tab &lt;- paste(&quot;http://www.fishbase.de/&quot;, egg_link, sep=&quot;&quot;) %&gt;% getURLContent(followlocation=TRUE, .encoding=&quot;CE_UTF8&quot;) %&gt;% readHTMLTable() egg_tab &lt;- egg_tab[[1]] # To save the list object as a data frame egg_tab ## Main Ref. ## 1 Place of Development ## 2 Shape of Egg ## 3 Attributes ## 4 Color of Eggs ## 5 Color of Oil Globule ## 6 Additional Characters ## 7 Get Information on Scirus Now we can extract information from this table by using the function which(). We want to find the position in this table, where Shape of Egg is defined. To identify the right row, we look in the first column for the respective string and extract the value stored in the second column of this row. To automatize, we add a check to see whether any information was extracted at all from the table. If not (as it is the case here), we just set egg_shape to NA. egg_shape &lt;- egg_tab[ # Accessing rows of egg_tab (remember: df[rows, cols]) which(egg_tab[, 1] == &quot;Shape of Egg&quot;), # Get number of row where &quot;Shape of Egg&quot; appears in the first column 2] # Access the second column egg_shape ## [1] &quot;&quot; if(egg_shape == &quot;&quot;) {egg_shape = &quot;NA&quot;} egg_shape ## [1] &quot;NA&quot; Now we can put all the information into a data frame for the entry Coregonus lavaretus by using the function tibble(). Having done this entire data scraping for one species, we can start to automatize the process for multiple species and simply append the data to our data frame. species_df &lt;- tibble(Species = &quot;Coregonus-lavaretus&quot;, Length = max_length, IUCN = IUCN_sta, Egg = egg_shape) species_df ## # A tibble: 1 x 4 ## Species Length IUCN Egg ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Coregonus-lavaretus 73 VU NA Checkpoint Try to create and add a new variable egg_color to the data frame and feed it with the respective information from our egg_table (follow the same process used for ‘egg_shape’). # your code Solution egg_color = egg_tab[which(egg_tab[, 1] == &quot;Color of Eggs&quot;), 2] # extract information if(egg_color == &quot;&quot;){egg_color = &quot;NA&quot;} # set missing information to NA species_df &lt;- tibble(species_df, Color = egg_color) # Add new variable to data frane species_df ## # A tibble: 1 x 5 ## Species Length IUCN Egg Color ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Coregonus-lavaretus 73 VU NA NA 5.3.6 The Fishbase Package Using an API As we saw in the previous sections, web scraping can be tedious. In this subsection, we want to get data using an API which is a much easier way. We use the R package rfishbase to get data from https://www.fishbase.de. This package makes it very easy to look up for information on the most well-known fish species. It simplifies the data extraction process but also has some limits as we will see below. You ca have a look at the functions built into rfishbase by typing help(package = \"rfishbase\") into your RStudio console or have a look at the RDocumentation. Most functions have straight forward names and have a sring as input which holds the Latin name of the wanted fish species. Let us get some information on Coregonus lavaretus with this new package! We see that the super short code species(\"Coregonus lavaretus\")provides us with 101 variables with entries for our fish species - think about the hand-written web scraping code this would require! To directly asses the maximal length of the species, we can directly use the $ notation. As you might recall, this is the same length as we got above. CL &lt;- species(&quot;Coregonus lavaretus&quot;) CL$Length ## [1] 73 In the next step we are interested to get information about the diet of Coregonus lavaretus by using the function diet_items(). It allows us to access the table on the food items of the chosen species. To extract information on food, we can save the diet items in a table and use the tidyverse notation plus the respective variables FoodI, FoodII: food &lt;- diet_items(&quot;Coregonus lavaretus&quot;) food %&gt;% select(FoodI, FoodII) %&gt;% head() ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.34.1 [/Users/pascalschneider/Library/Application ## # Support/org.R-project.R/R/rfishbase/database/sqlite.sqlite] ## FoodI FoodII ## &lt;chr&gt; &lt;chr&gt; ## 1 zoobenthos mollusks ## 2 nekton finfish ## 3 zoobenthos benth. crust. ## 4 zoobenthos worms ## 5 zoobenthos benth. crust. ## 6 zoobenthos benth. crust. Checkpoint Now your next task is to get information on predators (hint: the function is exactly named like that). # your code Solution # use function &#39;predators()&#39; for Coregonus lavaretus pre &lt;- predators(&quot;Coregonus lavaretus&quot;) pre %&gt;% select(PredatorName) %&gt;% head() ## # A tibble: 6 x 1 ## PredatorName ## &lt;chr&gt; ## 1 Coregonus peled ## 2 Esox lucius ## 3 Salmo trutta trutta ## 4 Sander lucioperca ## 5 Phryganea sp. ## 6 Coregonus lavaretus Limits of APIs One of the major limitations of an API is that we can only use already implemented functions. For example, in our case, we cannot get the IUCN Status of a given species because it is not built into rfishbase. In order to obtain the IUCN Status we must use another API, namely the package rredlist. Unfortunately, to have access to the rredlist API we need an authentication key which we only get for a small fee. As it is with other things in life, either you try to put in the work yourself or you can pay someone to do so (except for the amazing open source community). 5.3.7 Summary In this third section we learned how to extract data from a website. We saw that this can be done either with web scraping or using APIs. We saw that web scraping can be tedious, but we learned about some limitations of APIs. 5.4 Case Study: Species Richness and Red List species proportions Please note that the code below is written in base R and not tidyverse which makes the codes a bit more difficult to read. But do not worry about this and simply just enjoy the beauty of what R is capable of. Next, we look at another case study on species richness and red list species proportions. To proceed with the case study we need to prepare our dataset. First, we need to get a list of species for the dataset. In this subsection we will see how to get all the species in a given family. 5.4.1 Creating List of Species In this section, we want to get all the species of the family of Salmonidae. As we did above, for flexibility we will create an object x with the name of the family and use paste() to get the link. We then use getURLContent() to get the content of the link url and save it in con_sal. x &lt;- &quot;Salmonidae&quot; url &lt;- paste(&quot;http://www.fishbase.de/Nomenclature/FamilySearchList.php?Family=&quot;, x, sep=&quot;&quot;) con_sal &lt;- getURLContent(url, followlocation = TRUE) Next we create a dataframe using data.frame() and get a list of variables. We will extract the variables with the same number of rows and unique row names. The function readHTMLTable() (from earlier) helps to extract data from HTML tables in an HTML document. Then we can extract the species from the given family with as.character(). We use z[,1] to get the first column which is the column with the scientific names of species. Now we can print the first element of the column with the scientific names. df &lt;- data.frame(readHTMLTable(con_sal)) sp_per_family &lt;- as.character(df[,1]) sp_per_family[1] ## [1] &quot;Brachymystax lenok&quot; Using str_replace() function we can substitute the empty space between the Genus and the Species with a \"-\". Finally we can print the first element of sp_per_family again. sp_per_family &lt;- str_replace(sp_per_family, &quot; &quot;, &quot;-&quot;) sp_per_family[1] ## [1] &quot;Brachymystax-lenok&quot; 5.4.2 Extracting IUCN Status for all species In this section, we are going to get the IUCN Status for a given List of species using a ‘for loop’. We will extract the IUCN Status of all the species from the Netherlands. First, we are going to upload the dataset containing the list of the species and some other data that is going to be useful for the next sections. We will do that by using the function read_csv(). This data is taken from this nature article by cropping it to Western Europe. As we only need data related to Netherlands, we will use the function grep() and pass Netherlands as an argument. dataset &lt;- read_csv(&quot;./data/dataset2.csv&quot;) subset &lt;- dataset[grep(&quot;Netherlands&quot;, dataset$Country),] Let us first briefly discuss how to construct a ‘for loop’, since it’s been a while since you used it in previous chapters. To get the IUCN Status of a list of species we always change the value of x (the species) and run the code, but if we have to do that for many species it will be very long and tedious. In this case ‘for loops’ come in handy. In a ‘for loop’ the variable x runs over the vector (here each species) and returns the IUCN Status. Before getting the IUCN status we will go over an easy example, such as printing the integers from 1 to 10 using a ‘for loop’. In this case, we iterate over the vector 1:10. for(j in 1:10) { print(j) # this prints the value of j for that given loop } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 Now we can make a ‘for loop’ in order to get the IUCN status of all the species in the above subset (Netherlands) of the initial dataset. We are going to iterate over the column of the dataset with the valid FishBase species names. The code lines inside the loop are exactly a copy-paste of what we had for the Coregonus lavaretus, but in this case, we have to look for other species. In the last line of the code below, we created a new column in the data frame subset in order to save the IUCN Status. The ‘i’ in subset$IUCN[i] is used to save the IUCN status of the species we are iterating over in the ‘for loop’. It will save the results of the species one by one. i &lt;- 0 for(x in subset$X6.Fishbase.Valid.Species.Name) { i &lt;- i + 1 url &lt;- paste(&quot;http://www.fishbase.de/summary/&quot;,x,&quot;.html&quot;,sep=&quot;&quot;) # we call the url fish_species &lt;- htmlParse(getURLContent(url, followlocation=TRUE)) # get the content fish_species_div &lt;-getNodeSet(fish_species, &quot;//div &quot;) # get the nodes with species w_IUCN &lt;- which(sapply(lapply(fish_species_div,xmlValue),function(x) # look for the IUCN pattern {regexec(pattern=&quot;IUCN&quot;, x)[[1]][1]})&gt;0) if(length(w_IUCN)==0){ # NA if no IUCN status IUCN_status=&quot;NA&quot; } else { # else access information d1_IUCN &lt;- xmlValue(fish_species_div[[w_IUCN[length(w_IUCN)]]]) IUCN &lt;- unlist(regmatches(d1_IUCN,gregexpr(pattern= &quot;[[:alpha:]]+)&quot;, d1_IUCN))) IUCN_status &lt;- sub(pattern=&quot;[[:punct:]]&quot;,replacement=&quot;&quot;,IUCN[1] ) } print(IUCN_status) subset$IUCN[i] &lt;- IUCN_status # make a new column in &#39;subset&#39; containing the IUCN status } ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;VU&quot; ## [1] NA ## [1] NA We can see which IUCN statuses are present in Netherlands by using the unique() function. There are LC for least concern, VU for vulnerable and of course NA for unknown values. unique(subset$IUCN) ## [1] &quot;LC&quot; &quot;VU&quot; NA 5.4.3 Proportion of species in Netherlands We will now plot the proportion of species in each category for the Netherlands. So let us calculate the number of species in each category (from above we just have 2 outputs,* LC &amp; NT). In general in other countries, we also have other IUCN Status, for example, VU. In this section, we will focus only on the two listed statuses. Using the function length() we can obtain the number of species in each category. number_lc &lt;- length(which(subset$IUCN == &quot;LC&quot;)) number_vu &lt;- length(which(subset$IUCN == &quot;VU&quot;)) number_na &lt;- length(which(is.na(subset$IUCN))) # print the values for NT, VU and NA paste0(&quot;LC:&quot;, &quot; &quot;, number_lc) ## [1] &quot;LC: 6&quot; paste0(&quot;VU:&quot;, &quot; &quot;, number_vu) ## [1] &quot;VU: 1&quot; paste0(&quot;NA:&quot;, &quot; &quot;, number_na) ## [1] &quot;NA: 2&quot; We are ready to plot this as pie chart. slices &lt;- c(number_lc, number_vu, number_na) lbls &lt;- c(&quot;LC&quot;,&quot;NT&quot;, &quot;NA&quot;) pie(slices, labels = lbls, font.main = 1, main = &quot;Proportion of species per IUCN Status in Netherlands&quot;, col=c(&quot;red&quot;, &quot;yellow&quot;, &quot;grey&quot;)) *** ### Cleaning data with IUCN Status In this part of the tutorial, we have to clean the data that we will use for the correlations. We have provided the dataset already with the IUCN status since the code needs a lot of time to run, but the procedure is exactly the same as we did in the previous sections. So, let us load the dataset, it’s called datasetIUCN. In the previous sections, we saw that for some species we did not have information on the IUCN status. In these cases, we set the IUCN status as NA. We can check for possible NA values by calling the unique() function. dataset_IUCN &lt;- read_csv(&quot;./data/datasetIUCN.csv&quot;) unique(dataset_IUCN$IUCN) ## [1] &quot;LC&quot; &quot;CR&quot; &quot;lc&quot; &quot;VU&quot; NA &quot;EX&quot; &quot;NT&quot; &quot;DD&quot; &quot;EN&quot; &quot;EW&quot; Checkpoint Now your task is to remove the row with the IUCN status as NA. You can use the function subset() to get the subset of the dataset with information about the IUCN status. # your code Solution # subset the data without NAs (!=NA; not equal to NA), so this effectively removes NAs dataset_IUCN_NA &lt;- subset(dataset_IUCN, dataset_IUCN$IUCN != &quot;NA&quot;) # check if it worked unique(dataset_IUCN_NA$IUCN) ## [1] &quot;LC&quot; &quot;CR&quot; &quot;lc&quot; &quot;VU&quot; &quot;EX&quot; &quot;NT&quot; &quot;DD&quot; &quot;EN&quot; &quot;EW&quot; Next, we will load the shapefile of glacial basins using the package rgdal and the function readOGR(). We fetch the data from different fish basins across Europe in the basin_shapefile from the data stored in the basins folder. In order to plot the basins on the map we will use fortify() function on the basin_shapefile. This function helps to convert a lines-and-points object into a data frame for ggplot. We will store this dataframe as fort_basin. basin_shapefile &lt;- readOGR(&quot;./data/basins&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/pascalschneider/Polybox/Shared/Data Science Lecture Planning - shared folder/4 Datasets/basins&quot;, layer: &quot;basins&quot; ## with 3119 features ## It has 9 fields fort_basin &lt;- fortify(basin_shapefile) head(fort_basin) ## long lat order hole piece id group ## 1 -43.00000 -22.55000 1 FALSE 1 0 0.1 ## 2 -43.03750 -22.69583 2 FALSE 1 0 0.1 ## 3 -43.05325 -22.69523 3 FALSE 1 0 0.1 ## 4 -43.05507 -22.68816 4 FALSE 1 0 0.1 ## 5 -43.06888 -22.68693 5 FALSE 1 0 0.1 ## 6 -43.07279 -22.68390 6 FALSE 1 0 0.1 We can visualise our basin_shapefile using ggplot(). ggplot() + geom_polygon(data = fort_basin, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) Figure 5.11: Plot of fort_basin. 5.4.4 Maps of species richness and Red List species proportions Preparing data We now want to map the species richness and Red List species proportions. Let us first calculate the species richness and proportion of Red List species per basin. Species richness is the number of species pro basin and the proportion of red list species is the ratio between the number of species in the red list and the number of species pro basin. We will iterate over the vector with the basin names and we will use the function nrow() to find the number of occurrences. for (x in as.character(basin_shapefile$BasinName)) { # we now restrict to dataset to the basin x dataset_basins &lt;- dataset_IUCN[dataset_IUCN$X1.Basin.Name==x,] # number of species in the given country x n1 &lt;- nrow(dataset_basins) # we now restrict to dataset to the country x and Red List dataset_c_IUCN &lt;- dataset_basins[grep(&quot;VU|EN|EX|EW|CR&quot;, dataset_basins$IUCN),] # number of species in the given country x with given IUCN Status n2 &lt;- nrow(dataset_c_IUCN) # compute the proportion basin_shapefile$proportion[basin_shapefile$BasinName==x] &lt;- n2/n1 basin_shapefile$richness[basin_shapefile$BasinName==x] &lt;- n1 } We can see the newly computed data for the columns ‘proportion’ and ‘richness’ with respect to each basin in the basin_shapefile. head(basin_shapefile@data) ## BasinName Country Ecoregion Endorheic Out_Longit Out_Latit Med_Longit ## 0 Cachoeirinha Brazil Neotropic &lt;NA&gt; -43.10344 -22.693658 -43.06899 ## 1 Comprido Brazil Neotropic &lt;NA&gt; -47.07734 -24.451571 -47.15300 ## 2 Arroyo.Walker Argentina Neotropic &lt;NA&gt; -62.29922 -40.628898 -62.59625 ## 3 Aconcagua Chile Neotropic &lt;NA&gt; -71.53604 -32.912822 -70.65645 ## 4 Amazon Brazil Neotropic &lt;NA&gt; -52.23409 -1.619426 -64.57286 ## 5 Andalien Chile Neotropic &lt;NA&gt; -73.09136 -36.664541 -72.81968 ## Med_Latit Surf_area proportion richness ## 0 -22.595111 228.8151 NaN 0 ## 1 -24.465831 204.7977 NaN 0 ## 2 -40.507344 969.1561 NaN 0 ## 3 -32.770645 7318.8768 NaN 0 ## 4 -6.714857 5888416.9156 NaN 0 ## 5 -36.824925 767.4691 NaN 0 Next we get the map of Europe. We will read the data in continent_shapefile and then will extract the continent ‘Europe’ map. If you call the variable europe you will see that the dataframe is empty. This is because it extracts only the map data which we can simply plot by passing the europe as an argument in plot() function. We will do it in the upcoming code cells. continents &lt;- readOGR(&#39;./data/continent_shapefile&#39;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/pascalschneider/Polybox/Shared/Data Science Lecture Planning - shared folder/4 Datasets/continent_shapefile&quot;, layer: &quot;continent&quot; ## with 8 features ## It has 1 fields europe &lt;- continents[continents$CONTINENT == &#39;Europe&#39;,] Warning message in readOGR(&quot;../data/continent_shapefile&quot;): “First layer europe_map read; multiple layers present in /work/04_data_scraping/data/continent_shapefile, check layers with ogrListLayers()” OGR data source with driver: ESRI Shapefile Source: &quot;/work/04_data_scraping/data/continent_shapefile&quot;, layer: &quot;europe_map&quot; with 53 features It has 94 fields Plotting data So now let us plot the proportions of Red List species. First, we will create a new column ‘proportion_colour’ and in this column, we will store the colours. Then we will break the proportions into 10 different parts by grouping the values in the proportion column into 10 using the cut() function. Then we get rid of the index vector using as.numeric() and get all the values as numeric values. We used the rev() function to reverse the colours, red colour indicates the species that are getting distinct and have very less proportion pro basin and yellow indicates the species with comparatively more proportion in the basin. Finally, we store these colours to the column proportion_colour. The colour indicates the proportion of Red List species occurring in the corresponding basin. We do the same for the species richness. basin_shapefile$proportion_colour &lt;- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$proportion, breaks = 10))] Now we create a new column in the fort_basin dataframe and map the values of ‘proportion_colour’ into the new color column. We are doing this to have the colour (species proportion divided into 10 parts) and lat-long values in one datframe which helps to plot the graph using ggplot(). Remember, above we extracted the continent ‘europe’ from the continent_shapefile, here we will use fortiy() on the europe dataset to plot it. In detail, we will plot the fort_europe and fort_basin data on the map. fort_basin$color &lt;- fort_basin$id # create a new column color for (i in as.numeric(unique(fort_basin$id))){ # map the values into color column by id fort_basin$color[fort_basin$id == i] &lt;- basin_shapefile@data$proportion_colour[i+1] } fort_europe &lt;- fortify(europe) ggplot(fort_basin, aes(x = long, y = lat, group = group)) + geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = &#39;white&#39;) + geom_polygon(fill = fort_basin$color, colour = &quot;black&quot;)+ xlim(-25, 28) Figure 5.12: Proportions of Red List species in Europe. From the Figure 5.12, we see that the proportion of Red List species is highest in South-Western Europe. The aim of the Red List is to inform decision-makers about potentially endangered species, i.e. species whose population size has been rapidly declining during the last decades or the species that only occur in small numbers at present. The proportion of species on the Red List of each region, therefore, gives an indication of the risk of species going extinct in a region and represents an important tool for conservation strategies. Repeat for mapping species richness We will repeat the above steps all together for plotting the species richness on map. # break the richness of the species into 10 parts and then we assign colors to each part basin_shapefile$richness_colour &lt;- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$richness, breaks = 10))] fort_basin$rich_color &lt;- fort_basin$id # create a new column rich_color # mapping the values from basin_shapefile to fort_basin for (i in as.numeric(unique(fort_basin$id))) { fort_basin$rich_color[fort_basin$id == i] &lt;- basin_shapefile@data$richness_colour[i+1] } # plot ggplot(fort_basin, aes(x = long, y = lat, group = group)) + geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = &#39;white&#39;) + geom_polygon(fill = fort_basin$rich_color, colour = &quot;black&quot;)+ xlim(-25, 28) Figure 5.13: Species richness in Europe. 5.4.5 Relation of basin size and species richness In the previous section, you observed the spatial patterns of fish diversity across Europe. In this section, we will try to explain these patterns. To achieve this, we will correlate the fish species richness of each basin to the surface area of the corresponding area to see how the species richness varies with respect to the surface area of the basin. We begin with plotting a simple scatterplot. We will log transform the data we have on surface area as it helps to make data conform to normality and also helps to deal with the outliers and skeweness in the data. Then we will plot this data against species richness. Since the data-deficient basins show zero observations in the dataframe, we will remove those first. Now we will create a new dataframe with the surface area and richness. We’ll rename the columns as ‘Basin_area’ and ‘Species_richness’ respectively. We make a simple scatterplot with a regression line to visualise the relationship. basin_shapefile &lt;- basin_shapefile[basin_shapefile$richness!=0,] bs_sr &lt;- tibble(basin_shapefile@data$Surf_area, basin_shapefile@data$richness ) names(bs_sr)[1]&lt;- &#39;Basin_area&#39; names(bs_sr)[2]&lt;- &#39;Species_richness&#39; ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) + geom_point() + geom_smooth(method=&#39;lm&#39;, color=&quot;red&quot;, size=0.5, se=FALSE)+ xlab(&quot;Basin Area&quot;) + ylab(&quot;Species Richness&quot;) + theme_classic() ## `geom_smooth()` using formula &#39;y ~ x&#39; The plot shows a positive correlation between basin area and fish richness, meaning that we expect to see a higher richness in larger basins. This pattern is commonly observed in ecology and one explanation for this is that larger areas provide different habitat types (niches), which allows more species to co-exist. As a next step, we will create a simple model of this relationship. This allows us to make predictions on the richness of fish species in other basins based on the basin area. richness_model &lt;- lm(richness~log(Surf_area), data=basin_shapefile@data) #create a linear model We now want to calculate the confidence intervals of the model to have a better idea of the uncertainty of the model. We will then coerce the basin surface, the model fit and the confidence interval into a new data frame called model_df. #calculate the confidence intervals model_df &lt;- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval=&#39;confidence&#39;))) names(model_df)[1] &lt;- &#39;log_area&#39; head(model_df) ## log_area fit lwr upr ## 174 8.274449 28.39050 26.57731 30.20369 ## 291 9.960755 37.89576 35.13808 40.65343 ## 335 7.949240 26.55739 24.82776 28.28701 ## 340 5.953703 15.30907 13.07901 17.53914 ## 392 9.074369 32.89944 30.71907 35.07981 ## 393 5.538369 12.96794 10.47659 15.45930 The final step is to plot the model fit and prediction intervals over the data. To get nice lines in the plot, the model_df dataframe needs to be ordered by the basin area first. ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) + geom_point() + geom_line(aes(x = model_df$log_area, y= model_df$fit ), color = &quot;red&quot;)+ geom_line(aes(x = model_df$log_area, y= model_df$lwr ), color = &quot;red&quot;, linetype = &quot;dotted&quot;)+ geom_line(aes(x = model_df$log_area, y= model_df$upr ), color = &quot;red&quot;, linetype = &quot;dotted&quot;)+ xlab(&quot;Basin Area&quot;) + ylab(&quot;Species Richness&quot;) + theme_classic() Checkpoint You have now seen how you can calculate and plot the confidence interval of your data. Based on your model you can also create a so-called prediction interval which gives an estimate of the range in which the model will most likely predict the y-values (for a given x-value). In our case the prediction interval would indicate in which range the model would expect the fish richness to be for a given basin surface area. Do you think that these prediction intervals will be broader or narrower than the confidence interval? Try to write the code for calculating and plotting the prediction intervals by yourself. # your code Solution #prediction intervals # make a dataframe of area and richness model predictions model_df_prediction &lt;- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval=&#39;prediction&#39;))) names(model_df_prediction)[1] &lt;- &#39;log_area&#39; # order the basins by size model_df_prediction &lt;- model_df_prediction[order(model_df_prediction$log_area),] # plot the model plot(log(basin_shapefile$Surf_area), basin_shapefile$richness, pch=16, cex=1, col=rgb(0,0,0,0.6), xlab=&#39;Basin area&#39;, ylab=&#39;Basin species richness&#39;) lines(model_df_prediction$log_area, model_df_prediction$fit, col=&#39;skyblue3&#39;, lwd=2) lines(model_df_prediction$log_area, model_df_prediction$lwr, col=&#39;skyblue2&#39;, lwd=2, lty=3) lines(model_df_prediction$log_area, model_df_prediction$upr, col=&#39;skyblue2&#39;, lwd=2, lty=3) 5.5 References Automated Data Collection with R, S. Munzert, C. Rubba, P. Meißner and D. Nyhuis XML and Web Technologies for Data Sciences with R, D. Nolan, D. Temple Lang http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/styled-4/styled-6/code-13/ https://ourcodingclub.github.io/tutorials/webscraping/ https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/ 5.6 Exercise For this week’s exercise open up the Rstudio environment. Remember to save all your changes to this notebook using git status, git add , git commit -m “your comment”, git push. Today’s exercise is about getting data from the web and extracting useful insights from it. Get in touch with your teaching assistant if you have any further questions. "]]
