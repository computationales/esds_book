[["index.html", "Environmental Systems Data Science Prerequisites Course Description Course Objectives Content Useful Prerequisites", " Environmental Systems Data Science Loïc Pellissier, Joshua Payne, Benjamin Stocker 2021-12-30 Prerequisites Course Description In a series of tutorials with code examples (R) from environmental systems and exercises, students are introduced to methods for implementing each step of a typical data science workflow - including data management, data processing, modelling, visualization and communication. The course enables students to plan their own data science project in their specialization and to acquire more domain-specific methods independently or in further courses. Course Objectives The students are able to frame a data science problem and build a hypothesis describe the steps of a typical data science project workflow conduct selected steps of a workflow on specifically prepared datasets, with a focus on choosing, fitting and evaluating appropriate algorithms and models critically think about the limits and implications of a method visualise data and results throughout the workflow access online resources to keep up with the latest data science methodology and deepen their understanding Content The data science workflow Access and handle (large) datasets Prepare and clean data Analysis: data exploratory steps Analysis: machine learning and computational methods Evaluate results and analyse uncertainty Visualisation and communication Useful Prerequisites 252-0840-02L Anwendungsnahes Programmieren mit Python 401-0624-00L Mathematik IV: Statistik 401-6215-00L Using R for Data Analysis and Graphics (Part I) 401-6217-00L Using R for Data Analysis and Graphics (Part II) 701-0105-00L Mathematik VI: Angewandte Statistik für Umweltnaturwissenschaften "],["ch-01.html", "Chapter 1 Primers 1.1 Introduction 1.2 Tutorial 1.3 Exercise", " Chapter 1 Primers 1.1 Introduction This tutorial gets you started to working with R and Git for a data science project. This chapter also includes some basics for programming in R (reading and writing data, types of objects, simple vizualisations). If you’ve had previous experience with R, you may skip this part. After you’ve gone through the lecture and solved the exercises, you should be able to: Understand the definition of data science. Identify the different steps of a data science workflow. Recall a few typical examples of data science applications toward environmental problems. Explain what a system is and why system understanding is needed in environmental sciences. Define data, understand the structure of data, and list examples of environmental data. List sources of environmental data and explain the methodological difference in their collection. Execute R and load a dataset for data analyses. Apply basic Git commands for version control. Plot data analysis outputs and write them on the disk. 1.1.1 Key Points from the lecture The four ‘Vs’ of Big Data: Volume: Data volume is simply the amount of data in a file or database. Velocity: Data velocity is the lag between time of observation and data ingest for analysis. Variety: Data variety is the diversity of data in a data collection or problem space. Veracity: Data veracity is the degree to which data is accurate and precise in comparison with a measured phenomenon Open Science is enabled by the FAIR criteria for data use and sharing: Findable: Easy to find and well documented. Accessible: Long term storage and accessible with standard protocols. Interoperatable: Exchangeable and correctly cited. Reusable: Sufficiently documented and clear legal terms. Environmental data is collected from a diversity of methods, including remote sensing, continuous site-scale measurements (e.g., meteorological variables, hydrological variables, ecosystem-atmosphere exchange fluxes of water vapour and CO2), episodic observations (e.g., forest inventories, biodiversity assessments), field campaigns, or experiments in the field or under laboratory conditions. Data from these sources come in different shapes and formats. In this course, we will learn to handle this diversity efficiently to answer research questions that you will likely encounter in the future. In many of the exercises, we will focus on data from continuous site scale observations, where meteorological variables are measured in parallel with ecosystem-atmosphere exchange fluxes of water vapour and CO2, collected by the eddy covariance technique. In addition, you will learn to tap into large environmental data repositories to complement your site-scale data. At the end of each chapter, you will find an exercise that you can complete using the Renku cloud computing environment and the RStudio integrated development environment. 1.2 Tutorial 1.2.1 Working with RStudio on Renku Cloud computing is becoming increasingly popular as it allows users to connect to the cloud system at any time from almost anywhere. This allows the bulk of features and files to be stored on a cloud-hosted server rather than on the users own computer. Renku is an open cloud computing platform that aims to make ‘Multidisciplinary Data Science Collaborations’ easier to navigate. It stores the data and code allowing multiple people to work on the same projects and keeps track of the various steps and versions of a project. By keeping track of the changes, there is a history and timeline of each persons contributions and results. Through Renku, its developers hope to make data science more reproducible, reusable and repeatable and hope to encourages and simplify collaborations. RStudio on Renku is similar to using Rstudio on any laptop or computer, except it is hosted by the platform. Start by selecting the correct project in Renku. Then press ‘fork’ to create a copy of the project, which serves as your personal project that you can modify. Enter the title and namespace of the project and press ‘fork’ again. This starts forking the project. After the forking you are ready to start working on the project. In the following will work with a fork simply called ‘01_primers’. Once you forked the project, select the tab ‘Environments’ and click ‘New’ to start a new interactive environment. Make sure to set the Default Environment to /rstudio. The other options can be left on the default settings. The box commit will show you when the latest commit was pushed and who authored it. Press Start environment. This starts loading the environment and can take a while. Once it is ready (signalised by a green tick within a white circle), click Connect. An online RStudio interface will open in a new browser tab. Once RStudio is running, it works as on a laptop or computer. Saving the files will differs when using RStudio on Renku, this will be explained in the section below called Git. Once your progress has been saved an active environment can be closed by clicking on the three dots next to the connect button back on the Renku page and then selecting Stop. Do this before logging out of and closing Renku. 1.2.2 Git Git is a program for code version control and managing collaborative code development. The basic unit of a Git project is a repository, which usually contains multiple source code files that may be organised in (sub) directories. Git keeps track of the entire history of a repository in steps of commits. A commit is a “bundle” of multiple edits and additions of new files and is specified by the user. Since Git knows the entire history of all the files that have ever been added or removed to and from a repository, it allows you to recover previous versions of files (previous commits) and track changes between them. Git also allows you to have multiple versions of your repository (branches) for parallel developments on the same code base, but we will not use this feature. Since Git stores the entire history, the content of a Git repository should always be kept light. For example, it is advisable to only add code and other (small, &lt;10 MB) plain text files to a repository, but not data, and no other outputs that is created by the code of the repository. A Git repository exists locally, that is, on your computer. But the power of Git plays out when your local repository has a counterpart in the cloud, for example on GitHub, or on ETH’s GitLab. Git lets you push (upload) your commits to your repository’s remote copy, and pull (download) changes that have been uploaded by your collaborator(s) or yourself from a different clone (local copy) of your repository. 1.2.2.1 Git commands in RStudio Unless Git has been installed on a laptop and connected to RStudio, the Git features in Renkus RStudio will not be visible to a user. We will now introduce you to the main commands you will need to make use of Git. To save your work you will want to commit it. A commit saves the current version of the file or project you are working on. By continually adding commits you will get a timeline of your project as it develops. All commits are saved to the local repository or environment. To commit, click one of the two commit button or press Ctr + Alt + M. RStudio will open up a new window, showing changes to the selected file, the history and a box where you can add a commit message. The commit message is a way to explain what has been changed in the file since the last commit. Under the section Status each file with either have a yellow box with a ? for ‘unkown’ files , a blue box containing an M for ‘modified’ or a green box containing an A for ‘added’. Added mean the file is ready to be commited. The box in the column Staged must be ticked for the file to be committed. Then click Commit. If it worked, a message such as the following will appear: Remember, this file is still only saved to the local repository or environment. RStudio shows the user this with a little message: To upload it to the remote repository, we will need to push it. To do this click either Push Branch or Push to the right of the green arrows facing up. With this command, we upload our changes to the remote Git repository, e.g. on GitHub or GitLab. If the push is successful, the little information saying the branch is ahead will have disappeared and a message such as the following appears. It is good practice to commit and push any changes before you interrupt your work or finish a session. Else those changes will be lost. Renku has an autosave function in case you forget but it can lead to conflicts when picking up your work again. Once you start your work again, either stop the environment in which you are working within Renku and open a new one or start the next session the blue arrow in the Git tab that says Pull. As its name suggests, with it we can pull changes in our document from the Git repository. This might seem like an unnecessary step, when you are working alone on the document. However, in projects where you might be working together with a peer and need to make sure you integrate her/his changes as well. If you are working together, make sure, that you do not work on the same document at the same time, which could lead to merge conflicts. If at some point you encounter a merge conflict that Git cannot resolve automatically you will have to do so manually. At the end of this tutorial we show how this is done. However, it is unlikely that you will encounter such a problem in this course, which is why we do not explain it here. 1.2.3 Libraries This tutorial uses a set of libraries. Libraries provide a set of functions and/or other objects targeted for specific applications and are not available through base R. To run commands of this tutorial, make sure to install and load all packages on your computer. This can be done by: list_pkgs &lt;- c(&quot;tidyverse&quot;) new_pkgs &lt;- list_pkgs[!(list_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) library(tidyverse) In each chapter of this tutorial, you’ll find a similar statement to specify which package the respective chapter’s tutorial will use and to install them if not available. 1.2.4 R scripts A single statement can be entered in the R console. For example, to calculate the mean of 1, 2, and 3, we enter: mean(c(1,2,3)) ## [1] 2 The output of a statement can be saved as a new object: my_new_object &lt;- mean(c(1,2,3)) The value of an object can be accessed or printed by referring to its name: my_new_object ## [1] 2 You will learn more about different types of objects in R further below. The set of objects defined during an R session are referred to as the environment. Usually, multiple statements are needed to get, e.g., from reading data into R to final numbers and figures that make up a further analysis. Together, these multiple statments constitute a workflow. It is essential that all workflows that underlie results of publications are reproducible. After closing an existing R session (e.g. after quitting RStudio), the environment defined by the user during that session, will not be saved automatically and will be lost. To make a workflow reproducible, the sequence of statements that you needed to carry out your analysis and produce outputs can be saved as an R script. Instead of saving the environment, we should save the steps (i.e. the script) to reproduce the same environment - starting from reading in the (raw) data. This will be the objective of an exercise at the end of this session. An R script is basically a text file, named with the suffix .R to indicate that it is executable by R. Executing the script is running each statement, line-by-line, starting from the top, and can be done in R by: source(&quot;my_r_script.R&quot;) You can find more useful information about scripts and workflows in R for Data Science (Hadley Wickham and Grolemund 2017). We should always strive to write nice scripts and good code. Good code is clean, readable, consistent, and extensible (easily modified or adapted). To achieve this, here are a few points to consider - inspired by best practices for coding and by the Tidyverse style guide (Hadley Wickham, n.d.). 1.2.4.1 Variable naming It is preferable to use concise and descriptive variable names. Different variable naming styles are being used. In this course, we use lowercase letters, and underscores (_) to separate words within a name (_). Avoid (.) as they are reserved for S3 objects (base R). Also, you should avoid naming your objects with names of common functions and variables since your re-definition will mask already defined object names. For example, df_daily is a data frame with data at a daily resolution. Or clean_daily is a function that cleans daily data. Note that a verb is used as a name for a function and an underscore (_) is used to separate words. It is also recommendable, to avoid variable names consisting of only one character. This makes it practically impossible to search for that variable. # Good day_01 # Bad DayOne day.one first_day_of_the_month djm1 # Very bad mean &lt;- function(x) sum(x)/length(x) # mean() itself is already a function T &lt;- FALSE # T is an abbreviation of TRUE c &lt;- 10 # c() is used to create a vector (example &lt;- c(1, 2, 3)) 1.2.4.2 Comments Adding comments in the code helps to explain exactly what the code is doing and why. This makes it easy to understand and modify the code, and can be key when debugging. In R source files, comments are prefixed with a #, which means that all what is right of the # is not interpreted by R. Avoid obsolete comments like ## take the mean myvar_mean &lt;- mean(myvar) 1.2.4.3 Add spaces and breaks Adding enough white spaces and line breakes in the right locations greatly helps the legibility of any code. Cramping it up too much leads to an unintelligible sequence of characters and it will not be clear what parts go together (operators, variable names, brackets). Therefore, consider the following points: Use spaces around operators (=, +, -, &lt;-, &gt;, etc.). Use &lt;-, not =, for allocating a value to a variable. An opening curly bracket ({) should be followed by a line break and never stand alone on a line. A closing curly bracket (}) should stand alone on a line unless followed by else. Code inside curly brackets should be indented (recommended: two white spaces at the beginning of each line for each indentation level - don’t use tabs). For example, well written code looks like this: if (temp &gt; 5.0){ growth_temp &lt;- growth_temp + temp } 1.2.4.4 Structure your script At the beginning of each file add a header as a fully commented text section, describing what the code contains, and how it fits into the larger analysis framework. Note that Git stores all meta information about the file, including who created it, who modified it and when. This information should not be added to the header. Then, load all libraries needed within the script. Then, source any scripts and load data, and only then, start with the sequence of statements. To visually separate parts, break up your code using, commented lines. For example, a script could look like this: ##//////////////////////////////////////// ## Demonstrating script structure ##--------------------------------------- library(tidyverse) source(&quot;R/my_functions.R&quot;) my_df &lt;- read_csv(&quot;data/my_df.csv&quot;) ##--------------------------------------- ## Main part ##--------------------------------------- ## convert units my_df$temp &lt;- my_df$temp + 273.15 # deg C -&gt; K ##--------------------------------------- ## Writing output ##--------------------------------------- filn &lt;- &quot;data/my_df_kelvin.csv&quot; print(paste(&quot;Writing file&quot;, filn, &quot;...&quot;)) write_csv(my_df, filn) 1.2.5 RMarkdown RMarkdown files are an enhanced version of scripts. They combine formatted text and executable code chunks. They can either be compiled (knitted) into an HTML or PDF output, where code chunks are executed upon compilation and visualization outputs are directly placed into the output, or they can be run like a script entirely or each code chunk separately. When run (not knitted), objects defined by the executed code are available in the environment. Text can be formatted using the Markdown syntax. For example, a top-level section title is specified by # and a title of a section one level lower by ##. RMarkdown documents are also the basis of this book, with each chapter written in a separate RMarkdown file. This lets you use the book in an interactive fashion. When opened in RStudio, you can knit an RMarkdown document by clicking the “Knit” button: You can run all chunks by selecting clicking “Run All” from the drop-down menu available under “Run”: Individual chunks can be executed by clicking the green right-pointing triangle in the upper right corner of the chunk: 1.2.6 Functions Often, analyses require many steps and your scripts may get excessively long. Over 2000 lines of code in one file are hard to digest. An important aspect of good programming is to avoid duplicating code. If the same sequence of multiple statements or functions are to be applied repeatedly to different objects, then it is usually advisable to bundle them into a new function and apply this single function to each object. This also has the advantage that if some requirement or variable name changes, it has to be edited only in one place. A further advantage of writing functions is that you can give the function an intuitively understandable name, so that your code reads like a sequence of orders given to a human, For example, the following code, converting temperature values provided in Fahrenheit to degrees Celsius, could be made into a function. ## NOT ADVISABLE temp_soil &lt;- (temp_soil - 32) * 5 / 9 temp_air &lt;- (temp_air - 32) * 5 / 9 temp_leaf &lt;- (temp_leaf - 32) * 5 / 9 The same, but using our own function convert_fahrenheit_to_celsius(): ## ADVISABLE convert_fahrenheit_to_celsius &lt;- function(temp_f){ temp_c &lt;- (temp_f - 32) * 5 / 9 } temp_soil &lt;- convert_fahrenheit_to_celsius(temp_soil) temp_air &lt;- convert_fahrenheit_to_celsius(temp_air) temp_leaf &lt;- convert_fahrenheit_to_celsius(temp_leaf) Functions (particularly long ones) can be written to separate source file (text files containing executable code). 1.2.7 Tidy data Similarly as code, data can be tidy or not. Even if a data file (for example an Excel spreadsheet) may look visually appealing, its structure determines how easy it is to perform further processing steps and calculations on it. The concept of tidy data (Hadley Wickham and Grolemund 2017) can be defined by the following rules (Hadley Wickham 2014) (http://www.jstatsoft.org/v59/i10/paper): Each column is a variable. Each row is an observation. Each value has its own cell. (Hadley Wickham and Grolemund 2017) Having data in a tidy format greatly facilitates all steps of data wrangling. When creating a spreadsheet, remember to design it in a tidy structure from the beginning. For example, put variable names in the first row, avoid merged cells, create separate tabs for each table, and make it machine readable. For example, avoid entries like “&gt;10 g m-2 s-1”. A cell should only contain one value. Here, this is “10”. Create a separate column for units, and enter the information there as a character string (“g m-2 s-1”). The information conveyed by the greater-than (“&gt;”) symbol should be encoded in a machine readable manner. This may not be straight forward. A possibility could be to add a column “is_minimum” and add the entry “TRUE” in the respective cell. The concept of tidy data can even be taken further by understanding a “value” as any object type, e.g. a list. This leads to a list “nested” within a data frame. You will learn more about this in Chapter 2. In the exercises at the end of this chapter, you will transform a visually appealing but poorly formatted data file into tidy and machine readable data. 1.2.8 R projects Using R projects in combination with Git is the essence of efficient workspace management in R. All files that belong together are organised within one directory. This can be regarded as the project directory and is typically congruent with what belongs to the respective Git repository. When working in RStudio, you can create a new R project or switch to an existing project by navigating the drop-down menu in the top right corner of RStudio. When starting a new project, a file &lt;project_name.Rproj&gt; is created. It sits in the project directory and stores information about your last session (settings, open files, etc.) and optionally (not recommended) the environment of that session. The use of R projects also automatically enables useful features in RStudio for easy package, website, or book building and lets you manage Git for the repository corresponding to the project. It’s advisable to write files, created by the code of your project, to sub-directories within the project directory. For example, keep source files where R functions are defined in ./R (where . refers to the project directory), data files in ./data and visualizations in ./fig. To read and write from/to files should be done using relative paths: source(&quot;./R/my_r_script.R&quot;) or equivalently: source(&quot;R/my_r_script.R&quot;) A project directory should only contain code and outputs that belong to this one project. Stuff that may belong to multiple projects should be kept somewhere else. For example, keep original data (e.g., the raw data files that you created when collecting the data in the field, or data files you downloaded from the web) outside the project directory. It is advisable to create a separate data directory outside (e.g., ~/data/) that holds all the original data you ever downloaded, or obtained from peers, or gathered yourself. Within such a data directory, you can put files from different sources into separate sub-directories and add a description file (e.g., ~/data/some_data_source/README) defining who, from where and when the data was obtained and defining data use policy. 1.2.9 Working with data frames Now, let’s get our hands on actual data for demonstrating how data is read and written. As most of the code displayed in this book, the code chunks below are executable. You can try it out by opening the the book’s R project in RStudio. We are going to work with data from ecosystem flux measurements, taken by the eddy covariance technique, and provided as part of the FLUXNET2015 dataset (Pastorello et al. 2020) see. The data we’re using below comes from a flux tower near Zürich (CH-Lae, located on the Laegern mountain between Regensberg and Baden and run by our colleagues here at ETH). The data is stored as a Comma Separated Values file (.csv). This is a plain-text, and therefore a non-proprietary format. To follow the FAIR data principles, distribute your data in a format that is non-proprietary and readable across platforms and applications. For example, avoid distributing your data as an Excel spreadsheat (.xlsx), or a Matlab data object (.mat), or an R data object (.RData, or .rds). 1.2.9.1 Reading data To import the data into the R workspace (environment), we use the function read_csv() from the tidyverse package. In other R code, you will also encounter the base R read.csv() function. However, read_csv() is much faster and reads data into a tidyverse-data frame (a tibble) which has some useful additional characteristics, on top of a common R data frame. To tell the function where the data is located, pass the data’s path as an argument. You can either use an absolute path, starting from C:/ on a Windows computer or ~/ on a Mac or Linux. Or, alternatively, you can provide a relative path, where ./ points to the present working directory and ../ is one level up, or ../../ is two levels up, etc. # use a relative path to read the data df &lt;- read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-3.csv&quot;) print(df) # to print an overview of the data frame ## # A tibble: 4,018 × 334 ## TIMESTAMP TA_F_MDS TA_F_MDS_QC TA_F_MDS_NIGHT TA_F_MDS_NIGHT_SD ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20040101 -9999 -9999 -9999 -9999 ## 2 20040102 -9999 -9999 -9999 -9999 ## 3 20040103 -9999 -9999 -9999 -9999 ## 4 20040104 -9999 -9999 -9999 -9999 ## 5 20040105 -9999 -9999 -9999 -9999 ## 6 20040106 -9999 -9999 -9999 -9999 ## 7 20040107 -9999 -9999 -9999 -9999 ## 8 20040108 -9999 -9999 -9999 -9999 ## 9 20040109 -9999 -9999 -9999 -9999 ## 10 20040110 -9999 -9999 -9999 -9999 ## # … with 4,008 more rows, and 329 more variables: TA_F_MDS_NIGHT_QC &lt;dbl&gt;, ## # TA_F_MDS_DAY &lt;dbl&gt;, TA_F_MDS_DAY_SD &lt;dbl&gt;, TA_F_MDS_DAY_QC &lt;dbl&gt;, ## # TA_ERA &lt;dbl&gt;, TA_ERA_NIGHT &lt;dbl&gt;, TA_ERA_NIGHT_SD &lt;dbl&gt;, TA_ERA_DAY &lt;dbl&gt;, ## # TA_ERA_DAY_SD &lt;dbl&gt;, TA_F &lt;dbl&gt;, TA_F_QC &lt;dbl&gt;, TA_F_NIGHT &lt;dbl&gt;, ## # TA_F_NIGHT_SD &lt;dbl&gt;, TA_F_NIGHT_QC &lt;dbl&gt;, TA_F_DAY &lt;dbl&gt;, ## # TA_F_DAY_SD &lt;dbl&gt;, TA_F_DAY_QC &lt;dbl&gt;, SW_IN_POT &lt;dbl&gt;, SW_IN_F_MDS &lt;dbl&gt;, ## # SW_IN_F_MDS_QC &lt;dbl&gt;, SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, SW_IN_F_QC &lt;dbl&gt;, … The file is automatically machine-readable because we have: Only one header row, containing the column (variable) names. Variables organised by columns, and observations by rows. Each column consists of a single data type (e.g., character, numeric, logical; see below for more info) - Here, all columns are interpreted as numeric (`’). One value per cell. No merged cells. In short, the data frame is tidy. To understand the sort of object we work with, i.e. the class, we can do: class(df) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Fundamentally, df is a data.frame. In addition, it is also of some other classes (spec_tbl_df\",\"tbl_df\", \"tbl\") which gives it additional features. 1.2.9.2 Understanding the data structure There are several base R functions to help you understand the structure of a data frame. Here is a non-exhaustive list of of them: Size dim() - Returns the size of the dimensions of an object (here: number of rows and columns). nrow() - Returns the number of rows of an object. ncol() - Returns the number of columns of an object. Content head() - Returns the first 6 rows. tail() - Returns the last 6 rows. View() - look at the entire data set in the form of a table (It is not supported by the Jupyter environment. In RStudio however, it works). Names names() - Returns the column names (for data.frame-objects it is synonymous to colnames()). rownames() - Returns the row names. Summary class() - Returns the class of an object. str() - Returns the structure of an object and information about the class, length and content of each column. summary() - Returns generic statistics information, depending on the class of the object. For example, the data frame df has 4018 rows and 334 columns: dim(df) ## [1] 4018 334 A description of standardized FLUXNET data variables is available here. A selection of available variables that we will use in subsequent chapters are: GPP (gC m\\(^{−2}\\) s\\(^{-1}\\)): Gross primary production WS (m s\\(^{-1}\\)): horizontal wind speed USTAR (m s\\(^{-1}\\)): friction velocity TA (deg C): air temperature RH (%): relative humidity (range 0–100%) PA (kPa): atmospheric pressure G (W m\\(^{−2}\\)): ground heat flux, not mandatory, but needed for the energy balance closure calculations NETRAD (W m\\(^{−2}\\)): net radiation, not mandatory, but needed for the energy balance closure calculations SW_IN (W m\\(^{−2}\\)): incoming shortwave radiation SW_IN_POT (W m\\(^−2\\)): potential incoming shortwave radiation (top of atmosphere theoretical maximum radiation) PPFD_IN (\\(\\mu\\)mol photons m\\(^{−2}\\) s\\(^{-1}\\)): incoming photosynthetic photon flux density P (mm): precipitation total of each 30 or 60 minute period LW_IN (W m\\(^{−2}\\)): incoming (down-welling) longwave radiation SWC (%): soil water content (volumetric), range 0–100% TS (deg C): soil temperature CO2 (\\(\\mu\\)molCO2 mol\\(^{-1}\\)): Carbon Dioxide (CO\\(_2\\)) mole fraction in moist air 1.2.9.3 Selecting data and entering the tidyverse df is a data frame. This is similar to a matrix and has two dimensions (rows and columns). If we want to extract specific data from it, we specify the indices, i.e. the “coordinates”, of the data. For two-dimensional objects (data frames, matrices), the first index refers to rows and the second to columns. For example, to refer to the element on the third row in the first column, we write: df[3,1] ## # A tibble: 1 × 1 ## TIMESTAMP ## &lt;dbl&gt; ## 1 20040103 Reducing a data frame (tibble) to only the first columns can be done by: df[, 1] ## # A tibble: 4,018 × 1 ## TIMESTAMP ## &lt;dbl&gt; ## 1 20040101 ## 2 20040102 ## 3 20040103 ## 4 20040104 ## 5 20040105 ## 6 20040106 ## 7 20040107 ## 8 20040108 ## 9 20040109 ## 10 20040110 ## # … with 4,008 more rows The method of selecting parts of a data frame by index is quite flexible. For example, we may require the information in the third column for the first three rows. Putting a colon between two numbers, e.g. [1:3,], indicates we want to select the rows numbers starting at the first and ending with the second number. So here [1:3,] will give us rows one, two and three. df[1:3, 3] # reduces the data frame (tibble) to its first three rows and the 3rd column ## # A tibble: 3 × 1 ## TA_F_MDS_QC ## &lt;dbl&gt; ## 1 -9999 ## 2 -9999 ## 3 -9999 To reduce the data frame (tibble) to several columns, the function c() is used. c() stands for concatenate, which means to link together in a series or chain. This outputs the data frame (tibble) reduced to the selected row or column numbers inside c(). df[, c(1,4,7)] ## # A tibble: 4,018 × 3 ## TIMESTAMP TA_F_MDS_NIGHT TA_F_MDS_DAY ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20040101 -9999 -9999 ## 2 20040102 -9999 -9999 ## 3 20040103 -9999 -9999 ## 4 20040104 -9999 -9999 ## 5 20040105 -9999 -9999 ## 6 20040106 -9999 -9999 ## 7 20040107 -9999 -9999 ## 8 20040108 -9999 -9999 ## 9 20040109 -9999 -9999 ## 10 20040110 -9999 -9999 ## # … with 4,008 more rows Another method is to select the columns by column names, i.e. giving as input a string vector with the name of each column we want to select (again, this is Base R notation). This is especially useful if the columns we want to select are not contiguous. For example: # Selecting data by name in base R ================================ df[,c(&quot;TIMESTAMP&quot;, &quot;TA_F_MDS&quot;, &quot;TA_F_MDS_QC&quot;)] ## # A tibble: 4,018 × 3 ## TIMESTAMP TA_F_MDS TA_F_MDS_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20040101 -9999 -9999 ## 2 20040102 -9999 -9999 ## 3 20040103 -9999 -9999 ## 4 20040104 -9999 -9999 ## 5 20040105 -9999 -9999 ## 6 20040106 -9999 -9999 ## 7 20040107 -9999 -9999 ## 8 20040108 -9999 -9999 ## 9 20040109 -9999 -9999 ## 10 20040110 -9999 -9999 ## # … with 4,008 more rows In Chapter 2 of this tutorial tutorial, we will use the tidyverse, which is a set of R packages designed for working with tidy data and writing code in such a way as to emphasize and better understand the “workflow aspect” of it. A code chunk which does the same as above, but is written for the tidyverse can read as follows. select(df, 1) # reduces the data frame (tibble) to its first column ## # A tibble: 4,018 × 1 ## TIMESTAMP ## &lt;dbl&gt; ## 1 20040101 ## 2 20040102 ## 3 20040103 ## 4 20040104 ## 5 20040105 ## 6 20040106 ## 7 20040107 ## 8 20040108 ## 9 20040109 ## 10 20040110 ## # … with 4,008 more rows select(df, TIMESTAMP, TA_F_MDS, TA_F_MDS_QC) # reduces the data frame to columns specified by names ## # A tibble: 4,018 × 3 ## TIMESTAMP TA_F_MDS TA_F_MDS_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20040101 -9999 -9999 ## 2 20040102 -9999 -9999 ## 3 20040103 -9999 -9999 ## 4 20040104 -9999 -9999 ## 5 20040105 -9999 -9999 ## 6 20040106 -9999 -9999 ## 7 20040107 -9999 -9999 ## 8 20040108 -9999 -9999 ## 9 20040109 -9999 -9999 ## 10 20040110 -9999 -9999 ## # … with 4,008 more rows As a further shortcut in tidyverse, we can use the pipe %&gt;% operator. The data frame is still reduced to its first column: df %&gt;% select(1) ## # A tibble: 4,018 × 1 ## TIMESTAMP ## &lt;dbl&gt; ## 1 20040101 ## 2 20040102 ## 3 20040103 ## 4 20040104 ## 5 20040105 ## 6 20040106 ## 7 20040107 ## 8 20040108 ## 9 20040109 ## 10 20040110 ## # … with 4,008 more rows We pipe the object df into the select() function with argument 1. Note that the %&gt;% operator can be used on any function. It tells the function to interpret what’s coming from the left of %&gt;% as its first argument. For the remainder of the tutorial several variables will be required. The methods of variable selection demonstrated above will be utilised below to get the desired variables. df_small &lt;- df %&gt;% select(TIMESTAMP, TA_F, PPFD_IN) Note: In the code above, an indentation was used to highlight which parts go together and make the code easy to understand. Indentations and line breaks take no effect in R per se (unlike in other programming languages, e.g., Matlab, Python), but help to make the code easier to read. 1.2.9.4 Renaming TIMESTAMP_START, TA_F and PPFD_IN as variable names may be hard to remember and in this section you will have to type them a lot. Therefore we change their names to something more intelligle. df_small &lt;- df_small %&gt;% rename(time = TIMESTAMP, temp = TA_F, ppfd = PPFD_IN) 1.2.9.5 Writing data A data frame can be written to a CSV file by: write_csv(df_small, path = &quot;data/df_small.csv&quot;) The function saveRDS() allows you save individual objects of any form (not just a data frame). saveRDS() creates a binary file that is fast to write and read, but only intelligible to R. Such files are commonly identified by the suffix .rds. It is recommended to name the .rds files according to the single object they contain. For example: saveRDS(temp_sum, file = &quot;data/temp_sum.rds&quot;) This file can then be read into the R workspace. Sometimes, it is useful to give it a new name, e.g.: temp_sum &lt;- readRDS(&quot;data/temp_sum.rds&quot;) Note that making a file publicly available as a .rds file violates the FAIR principles. It is not interoperable. Therefore, whenever possible, save your data in a format that is readable across platforms without requiring proprietary software. Hence use write_csv() whenever possible. We will encounter other non-proprietary formats that let you save and share more complex data structures in chapter 2. 1.2.10 R objects Each object in R is of a certain class, specifying some attributes and how functions act upon it. The most basic classes are: numeric. any number (except complex numbers) -&gt; 2.375 integer (int) - integer numbers -&gt; 2 character (chr) - any string -&gt; “fluxes” logical (logi) - boolean -&gt; TRUE FALSE factor (Factor) - categorical data, the variable can only be one of a defined amount of options -&gt; female/male/other function (function) - a set of statements organized to perform a specific task -&gt; sum() 1.2.10.1 Data frames We have learned that the object df is a data frame. That is, it is of class \"data.frame\". You can think of a data frame as a table. Columns need to be of the same length and all values in a column need to be of the same data type. The most common data types in R are: However, there are many more data types. For now, we will mainly consider those mentioned above. 1.2.10.2 Vectors and classes The contents of a data frame column is a vector. It can be “extracted” from its data frame by pull. Next, we are going to extract the values of the column temp and return the class of the entries in this vector. This type of sequence of commands (actually, a small workflow) can be implemented as a sequence of pipes: df_small %&gt;% pull(temp) %&gt;% class() ## [1] &quot;numeric&quot; Sometimes you need to convert a numeric number into a character. The following takes only the first row of the data frame fluxes_subset using the slice() function with argument 1 (for the first row), extracts the single temperature value, converts it to an integer, and then to a character: df_small %&gt;% slice(1) %&gt;% pull(temp) %&gt;% as.integer() %&gt;% as.character() ## [1] &quot;-2&quot; You notice that now the values are in quotes ““. R interprets them as a text and you will not be able to do any numeric calculations with them anymore. Writing each function command on a separate line, connected with the pipe operator %&gt;%, allows for good readability of the entire workflow. Note that in R, a line break has no effect. Compare the above sequence of pipes to the following equivalent part of the code to understand the advantage of the tidyverse syntax: as.character(as.integer(df_small[1,&quot;temp&quot;])) ## [1] &quot;-2&quot; Arguably, a sequence of pipes is more easily legible than a complex nesting of brackets. An object of class logical can only be TRUE or FALSE. If I ask which temperatures &lt; 0 for example, R will return a vector of class logical. temperatures &lt;- df_small %&gt;% slice(1:10) %&gt;% pull(temp) temperatures &lt; 0 ## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE Indeed, the first 10 temperature values are all below freezing. 1.2.10.3 Lists Lists are extremely flexible. They allow us to store different types of data, even if they are of different lengths. Here is an example where each element of the list is named. mylist &lt;- list( temperatures = c(2.234, 1.987, 4.345), my_favourite_function = mean, best_course = &quot;Environmental Systems Data Science&quot; ) Similar to data frames, we can extract elements from lists, either by index [[1]] or by the name [[\"temperatures\"]]. Note the double [[]] here, indicating an element of a list as opposed to [] indicating an element of a vector. To get the entire vector of temperatures, do either of the two: mylist[[1]] ## [1] 2.234 1.987 4.345 mylist[[&quot;temperatures&quot;]] ## [1] 2.234 1.987 4.345 And to get the first temperature value: mylist[[&quot;temperatures&quot;]][1] ## [1] 2.234 You can also append elements to the list (either way is possible): mylist[[&quot;my_second_favourite_function&quot;]] &lt;- median mylist$my_second_favourite_function &lt;- median Checkpoint In mylist, we have saved a function called my_favourite_function and a numeric vector called temperatures. Use the function stored in the list and apply it on the vector temperatures, stored in the same list. Add the result in the list as a new variable and call it mean_temperature. The mean temperature should be equal to 2.85533333333333. Solution mylist$mean_temperature &lt;- mylist[[&quot;my_favourite_function&quot;]](mylist[[&quot;temperatures&quot;]]) mylist$mean_temperature ## [1] 2.855333 This was a very condensed introduction to vectors and lists. A more complete introduction is given here. 1.2.11 Data visualisation Visualising data is an integral part of any data science workflow. In this chapter, we introduce just the very basics. In later chapters, you will get introduced to additional methods for visualising data. Our data frame fluxes_subset contains three variables, one of which is time. In other words, we are dealing with a time series. Let’s look at the temporal course of temperature in the first 1440 time steps (corresponding to 30 days) as a line plot (type = \"l\"). plot(1:1440, df_small$temp[1:1440], type = &quot;l&quot;) Another useful way of looking, not at a temporal course, but rather at the distribution of your data, is to display a histogram. A histogram visualises the frequency or proportion of data that has a metric value that falls within a certain interval known as a ‘bin’. Below you will see the temperature on the x-axis split into these ‘bins’ ranging across 2°. The number of times a data point falls between say 2° to 4° is then tallied and displayed as the frequency on the y-axis. Here there are around 1500 temperature values between 2° and 4°. hist(df_small$temp, xlab = &quot;Temperature (°C)&quot;) Plots can be saved as files, as long as the file size does not get too large.It will write vector graphics as outputs, i.e. PDF. In base-R, this can be done by: pdf(&quot;./figures/filename.pdf&quot;) hist(df_small$temp) Checkpoint Create a scatter-plot (points) of temperature versus the incoming photosynthetic photon flux density using all data points in df_small. Can this plot intuitively be interpreted? Solution plot(df_small$temp, df_small$ppfd) 1.2.12 Conditionals In cases where certain statements are executed or not, depending on a criterion, we can use conditional statements if, else if, and else. Conditionals are an essential feature of programming and available in all languages. The R syntax for conditionals looks like this: if (temp &lt; 0.0){ is_frozen &lt;- TRUE } else { is_frozen &lt;- FALSE } The evaluation of the criterion (here (temp &lt; 0.0)) has to return either TRUE or FALSE. 1.2.13 Loops Loops are another essential essential feature of programming. for and while loops exist in probably all programming languages. We introduce them here because they provide an essential and powerful functionality for solving many common tasks. You will encounter them later again (see Chapter 5. for and while loops let us repeatedly execute the same set of commands, while changing an index, or counter variable to take a sequence of different values. The following example calculates a temperature sum of the first ten values, by iteratively adding them together. Of course, this is equivalent to just using the sum() function. temp_sum &lt;- 0 for (i in 1:10){ temp_sum &lt;- temp_sum + df_small$temp[i] } print(temp_sum) ## [1] -12.97 print(sum(df_small$temp[1:10])) ## [1] -12.97 Instead of directly telling R how many iterations it should do we can also define a condition. As long as the condition is TRUE, R will continue iterating. As soon as it is FALSE, R stops the loop. The following lines of code do the same operation as the for loop we just wrote. What’s different? What is the same? i = 1 temp_sum &lt;- 0 while (i &lt;= 10){ temp_sum &lt;- temp_sum + df_small$temp[i] i = i+1 } print(temp_sum) ## [1] -12.97 print(sum(df_small$temp[1:10])) ## [1] -12.97 1.2.14 Where to find help The material covered in this course will give you a solid basis for your future projects. Even more so, it provides you with code examples that you can adapt to your own purposes. Naturally, you will face problems we did not cover in the course and you will need to learn more as you go. The good news is, you do not have to. Many people make their code available online and often others have faced similar problems. Modifying existing code might make it easier for you to get started. 1.2.14.1 Within R “I know the name of a function that might help solve the problem but I do not know how to use it.” Typing a ? in front of the function will open the documentation of the function, giving lots of information on the uses and options a function has. You have learned a few things about plots but you may not know how to make a boxplot: ?boxplot Running the above code will open the information on making boxplots in R. If you do know how a function works but need to be reminded of the arguments it takes, simply type: args(boxplot) “There must be a function that does task X but I do not know which one.” Typing ?? will call the function help.search(). Maybe you want to save a plot as a JPEG but you do not know how: ??jpeg Note that it only looks through your installed packages. 1.2.14.2 Online To search in the entire library of R go to the website rdocumentation.org or turn to a search engine of your choice. It will send you to the appropriate function documentation or a helpful forum where someone has already asked a similar question. Most of the time you will end up on stackoverflow.com, a forum where most questions have already been answered. 1.2.14.3 Error messages If you do not understand the error message, start by searching the web. Be aware, that this is not always useful as developers rely on the error catching provided by R. To be more specific add the name of the function and package you are using, to get a more detailed answer. 1.2.14.4 Asking for help If you cannot find a solution online, start by asking your friends and colleagues. Someone with more experience than you might be able and willing to help you. When asking for help it is important to think about how you state the problem. The key to receiving help is to make it as easy as possible to understand the issue your facing. Try to reduce what does not work to a simple example. Reproduce a problem with a simple data frame instead of one with thousands of rows. Generalize it in a way that people who do not do research in your field can understand the problem. If you are asking a question online in a forum include the output of sessionInfo() (it provides information about the R version, packages your using,…) and other information that can be helpful to understand the problem. stackoverflow.com has its own guidelines on how to ask a good question, which you should follow. If your question is well crafted and has not been answered before you can sometimes get an answer within 5 minutes. https://stackoverflow.com/help/how-to-ask Finally, many packages have a mailing list or allow you to open a query on the code repository, where you can ask specific questions. The same is true for R itself. The R-Help mailing list https://stat.ethz.ch/mailman/listinfo/r-help is read by many people. However, the tone of such mailing lists can be pretty dry and unwelcoming to new users. Be sure to use the right terminology or else you might get an answer pointing out your misuse of language instead of your problem. Also, be sure your question is valid. Or else you won’t get an answer. 1.2.15 Key points from the tutorial RStudio on Renku and Git commands: - Regularly commit any changes you make to your work. To do this tick the Stage box, add a commit message and press commit. - Then upload the changes to the remote repository by pushing it. - Remember to close any active environments when you have finished your work on Renku. R scripts: Comments are any code prefixed by at least one hastag (#) will not be interpreted by R. Comments should help you understand your script. Add spaces around operators and breaks to make code easier to read. Use &lt;-, not =, for allocating a value to a variable. Begin files with a header to describe what the code contains and does. Load all the libraries needed in the script at the beginning. Functions: - Use functions to simplify a script with many steps or for duplicated code sections. - Functions (particularly long ones) can be written to separate source file (text files containing executable code). Tidy data is a concept that follows these rules: 1. Each column is a variable. 2. Each row is an observation. 3. Each value has its own cell. Working with data frames: - To get an idea of the structure of a data frame use functions such as: 1. Size: dim(), nrow(), ncol() 2. Content: head(), tail(), view() 3. Names: names(), rownames() 4. Summary: class(), str(), summary() Selecting data and entering the tidyverse: Data frames consist of rows and columns. To extract information either use df[row(s), column(s)] or with tidyverse df %&gt;% select(column). Vectors and classes: The simplest data structure in R is a vector and can be numeric or character vectors. Lists: Lists allow data of different types to be stored. To extract elements use double brackets [[]]. Data visualisation: Visualising data can provide a unique way to see and understand the data. Loops: Loops (such as for and while) are another essential essential feature of programming and lets us repeatedly execute the same set of commands. Where to find help: First, look within R (e.g. ?pch), then use the internet or ask colleagues. Remember to include sessionInfo() to help others to understand the version of R and packages you are using. 1.2.16 Further reading A complete tutorial on using R for Data Science in general is the freely available online-book by Grolemund &amp; Wickham. The material covered in this chapter should serve as a “primer”. That is, its purpose is to get you started with R and Git and make sure you know the basics. If you feel that you are still missing some basic concepts of R, a good starting point to fill up your gaps is the Chapter Workflow: basics in Grolemund &amp; Wickham. 1.3 Exercise This exercise will have you working on an Excel file and then in Renku’s Rstudio. You will need to download the provided datafile from moodle. Your task will be to tidy the data and then read it into Renku’s Rstudio. Download data from Groenigen et al., 2014, containing soil organic matter content data from a meta analysis of CO2 experiments, and available on Moodle. Open the file in Excel and navigate to the tab ‘Database S1’. You will find a short description in the top-left cell: “Database S1. Overview of CO2 enrichment studies reporting soil C contents that were used in our analysis.”. There is an issue with this dataset. Of course, .xls files are not easily readable into R without an extra package. In addition, even after saving the tab ‘Database S1’ as a CSV file, the table you get is not machine-readable into a data frame that we can work with in R. The way the data is organised into cells does not follow the structure of a dataframe and is not tidy. Recall the tidy data rules from the 01_primers.ipynb tutorial. Your task is to: Manually manipulate the .xls file to make it tidy. Save the data as a .csv file (comma-separated-values). Read the .csv file into RStudio. Calculate the logarithmic response ratio as the logarithm of the ratio of soil C contents at elevated CO2 divided by soil C contents at ambient CO2, for each data point (experiment and sample date). Visualise the distribution of the response ratio and save the plot as a .pdf file. Implement steps 3.-5. in an RMarkdown (.Rmd), applying some of the points for good coding practices. For the peer review round, share your code and the figure file (as html file via the knit button at the top, left of the Git menu, see Figure below) with your partner. References "],["ch-02.html", "Chapter 2 Data wrangling 2.1 Introduction 2.2 Tutorial 2.3 Exercise", " Chapter 2 Data wrangling 2.1 Introduction In this chapter, you will learn to efficiently explore data. This includes understanding how the data is structured, what “dimensions” are in a dataset, how to manipulate the data and how to visualise it. Efficient data exploration and wrangling are the basis for generating hypotheses, testing them, and repeating the wrangling-visualisation-hypothesis circle over and over. This is science. The aim of data wrangling is to transform the data from the raw data into a cleansed, more relevant format. This process typically involves steps such as exploring the data, sorting the data, filtering out irrelevant or redundant variables, structuring the data in new ways and visualising the data. It is common for scientists to spend the majority of their time ‘wrangling’ the data. If the data wrangling process is done well, the analysis will be considerably more efficient. Data wrangling can be in a number of programs. R offers very useful functionalities for achieving efficient data wrangling and visualisation, particularly using functions from the tidyverse. The aim of the tidyverse is to have a collection of functions and packages that share a common design philosophy and follow a set workflow, in order to make the work of data scientists more productive and reproducible. The steps in this workflow are import the data, tidy the data , understand the data (through transformation, visualisation and modelling), and finally to communicate the results. You have already used some of them in the previous chapter. This chapter will introduce some more of the basic and most important tidyverse functions, including ggplot. The contents of this tutorial are inspired by the (freely available online) book R for Data Science by Grolemund &amp; Wickham. 2.1.1 Learning objectives After you have gone through the lecture and solved the exercises you should be able to: Define data, understand the structure of data and list examples of environmental data. Implement common pre-processing steps. Understand tidy data and data dimensions. Visualise the multiple dimensions of data. 2.1.2 Key points from the lecture The lectures introduced the basics of data transformation with dplyr and data visualisation with ggplot2. The following is a recap of the most important points. The tutorial below then provides an introduction to implementing these with example data. 2.1.2.1 Data transformation with dplyr In data science when considering the number of features, variables or attributes these are referred to as the data dimensionality. A simple data set containing two features such as temperature and elevation would be considered two dimesional data. To plot the data a 2D graph showing elevation on the x-axis and temperature on the y- axis and the observations within, would be straight forward. If another variable called precipitation, plotting the whole data set would require a 3D graph. Most data sets contain many attributes, making plotting in dimensions impossible. Therefore, dimensions need to be choosen or reduced. Which dimensions we choose will vary depending on the research question we have for our data set. Data variation is defined as how much data points differ between observations. It may also be called the spread or dispersion of the data. An example would be the range of temperatures annually along an elevational gradient. Below are some of the essential functions of the tidyverse package dplyr to navigate dimensions and variation of a data set: Selecting observations by their values: filter() Selecting variables by their names: select() Creating new variables: mutate() Aggregating multiple values down to a single summary: summarise() Remember also that dplyr functions (sometimes, referred to as “verbs”) all work similarly: The first argument is the data frame. When using pipes (%&gt;%, see Chapter 1), the first argument specifying the data frame is omitted and the function takes its place. What is being piped into it, “coming” from the left side of %&gt;%. The remaining arguments specify what to do with the data frame (without quotes (\"\") on variable names). The output of the function is again a data frame. 2.1.2.2 Data visualisation with ggplot2 Data visualisation aims to convert data values into visual elements. This is done by ‘projecting’ data values onto quantifiable features of the graph, known as the aestetics. Variation along each ‘dimension’ is plotted onto one aesthetic. In other words, a column such as date will become the x-axis and then mean temperature along the y-axis. To add further ‘dimensions’ onto the asthetics, the temperature points could be coloured according to how warm it was. Visualising different dimensions of a data set can help data become more accessible and easier to understand. In the lecture we learned about data visualisation with the tidyverse package ggplot2 (Hadley Wickham 2016). Remember the steps for creating a figure with ggplot2: We will start by calling the function ggplot(). The first argument is the data frame that contains the values that are to be displayed in a figure. The second argument is the aesthetics “mapping” argument and always comes in the form of aes(...). Inside the brackets of aes(...), we usually indicate the column (variable) that specifies the coordinate of a visualisation element (e.g., a point) along the x-axis with x = ..., and along the y-axis with y = .... Then, add an additional function call to the initial ggplot(), with a + to specify the type of visualisation element (e.g., points, or lines, etc.) that maps the variable values to the plot coordinate space (for example x-y). The ggplot-+ works a bit like the pipe operator %&gt;%. This function call now specifies the type of plot to create. The name of this function starts with geom_. For example, to plot points of temperature at a given time from a data frame df into x-y space (a scatterplot), we would write something like: ggplot(data = df, aes(x = time, y = temperature)) + geom_point() 2.2 Tutorial After learning about some basic concepts and functions for data wrangling and visualisation, we will apply some of the tidyverse functions on the data. As in Chapter 1, we will be using the time series data from eddy covariance flux measurements and meteorological variables measured in parallel. In this sub-section, we start with half-hourly data from a flux tower near Zürich (CH-Lae, located on the Lägern mountain between Regensberg and Baden and run by our colleagues here at ETH). The data covers years 2004-2014 at a half-hourly time step. That is a large amount of data which can be tricky to work with in Excel. While Excel is a popular go to programm to work on data, it is less equipped to deal with very large data sets, does not have nearly as many options tow work with as in R and lacks in making a reproducible workflow. In this course, we will teach you to do data wrangling completely outside of Excel and show you how it improves your life as a (data) scientist considerably. Every research project starts with a broad overall question. In this tutorial, our aim to investigate the variations and controls of ecosystem-level gross primary production (GPP). GPP is the gross carbon assimilation by photosynthesis of all plants in the “footprint” of an eddy covariance tower and can be derived from the measurement of the vertical turbulent net flux of CO2 (on the basis of vertical air movement and parallel CO2 concentration measurements). “Gross” because plants simultaneously respire CO2 as they assimilate it. Several assumptions have to be made to get from raw measurements (simultaneous measurements of CO2 concentration and velocities of air movement in vertical direction, typically taken at 50 Hz) to the final GPP time series. In our dataset, different GPP time series are available and are derived using different assumptions. Below, we will work with the one called GPP_NT_VUT_REF. See Pastorello et al. (2020) (https://www.nature.com/articles/s41597-020-0534-3) for a comprehensive description of GPP estimation methods. Now that we roughly know what to expect from the contents of our dataset and we have a research question in mind (controls and variations of GPP), we can start searching for answers by reading, transforming, visualising, and modelling our data. Based on what we learn from this initial exploratory data analysis, we will refine our research question, re-focus it, and follow it up with the next level of data analysis and modelling in later chapters. 2.2.1 Libraries Install missing packages for this tutorial. list_pkgs &lt;- c(&quot;tidyverse&quot;, &quot;conflicted&quot;) new_pkgs &lt;- list_pkgs[!(list_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) library(tidyverse) library(conflicted) tidyverse (Hadley Wickham et al. 2019) is a collection of packages and library(tidyverse) loads them all. For this tutorial we use in particular: dplyr (Hadley Wickham et al. 2021) tidyr (Hadley Wickham 2021) purrr (Henry and Wickham 2020) readr (Hadley Wickham and Hester 2020) lubridate (Grolemund and Wickham 2011) stringr (Hadley Wickham 2019b) ggplot2 (Hadley Wickham 2016) It can happen that different functions with the same name are available from different packages. To avoid conflicts and make sure we use the preferred ones, we can use the conflicted package (Hadley Wickham 2019a) and specify: conflicted::conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;) conflicted::conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;) 2.2.2 Variables in a data frame We will start by reading in the half-hourly data from the eddy-covariance site CH-Lae again (as we did already in Chapter 1) and start to explore the data. We use the function read_csv() from the readr package (part of tidyverse) here for reading the CSV since it is faster than the base-R read.csv() and generates a nicely readable output when printing the object. It is also more reproducible as it avoids inheriting behaviour from your operating system, which may mean the code only wokrs on your laptop. ‘readr’ has many options to customise the read_csv() function, while avoiding . For example if you have lines with meta data information at the top of the data using skip = n or comments = \"#\" within the function will stop R from reading those lines. Equally, if the data is missing column names add col_names = FALSE or specify column names with col_names = c(\"x\", \"y\", \"z\"). For more details on read_csv() click here. library(tidyverse) hhdf &lt;- read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-3.csv&quot;) hhdf ## # A tibble: 192,864 × 235 ## TIMESTAMP_START TIMESTAMP_END TA_F_MDS TA_F_MDS_QC TA_ERA TA_F TA_F_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 200401010030 -9999 -9999 -2.22 -2.22 2 ## 2 200401010030 200401010100 -9999 -9999 -2.25 -2.25 2 ## 3 200401010100 200401010130 -9999 -9999 -2.28 -2.28 2 ## 4 200401010130 200401010200 -9999 -9999 -2.50 -2.50 2 ## 5 200401010200 200401010230 -9999 -9999 -2.72 -2.72 2 ## 6 200401010230 200401010300 -9999 -9999 -2.94 -2.94 2 ## 7 200401010300 200401010330 -9999 -9999 -3.17 -3.17 2 ## 8 200401010330 200401010400 -9999 -9999 -3.39 -3.39 2 ## 9 200401010400 200401010430 -9999 -9999 -3.61 -3.61 2 ## 10 200401010430 200401010500 -9999 -9999 -3.59 -3.59 2 ## # … with 192,854 more rows, and 228 more variables: SW_IN_POT &lt;dbl&gt;, ## # SW_IN_F_MDS &lt;dbl&gt;, SW_IN_F_MDS_QC &lt;dbl&gt;, SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, ## # SW_IN_F_QC &lt;dbl&gt;, LW_IN_F_MDS &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, LW_IN_ERA &lt;dbl&gt;, ## # LW_IN_F &lt;dbl&gt;, LW_IN_F_QC &lt;dbl&gt;, LW_IN_JSB &lt;dbl&gt;, LW_IN_JSB_QC &lt;dbl&gt;, ## # LW_IN_JSB_ERA &lt;dbl&gt;, LW_IN_JSB_F &lt;dbl&gt;, LW_IN_JSB_F_QC &lt;dbl&gt;, ## # VPD_F_MDS &lt;dbl&gt;, VPD_F_MDS_QC &lt;dbl&gt;, VPD_ERA &lt;dbl&gt;, VPD_F &lt;dbl&gt;, ## # VPD_F_QC &lt;dbl&gt;, PA &lt;dbl&gt;, PA_ERA &lt;dbl&gt;, PA_F &lt;dbl&gt;, PA_F_QC &lt;dbl&gt;, … You have already inspected the size and dimensions of this data frame in Chapter 1 with functions, such as dim(), nrow(), ncol(), head(), names() etc. For our further data exploration, we will reduce the data frame we are working with and select only certain variables. Reducing the dataset can have the advantage of speeding up further processing steps, especially when the data is large. Variable selection is not only a matter of improving processing efficiency, but is an important step for analysis and modelling and should be guided by our understanding of the data. I general, we select the variables that are expected to influence the phenomena that we are investigating based on the existing knowledge. The selection we decide upon must be documented for publications to aid reproducibility. More on variable selection will be taught in Chapter @ref(#ch-08). Here, many of the variables in the data frame record the same information, but are derived with slightly different assumptions and gap-filling techniques. This is indicated by the suffices of the variable names, for example some variable vontain _REF (such as GPP_NT_VUT_REF), indicating it is the most representative reference value for this variable. A SUBSET version of the data, with less variables, is also provided for ’non-expert users. See Pastorello et al. (2020) for a comprehensive description of alternative methods. For the further steps in this chapter we will now subset our original data. We select the following variables: Time of the measurement (all variables with names starting with TIMESTAMP) All meteorological variables following the final gap-filled method (all variables with names ending with _F) A gap-filled version of the CO2 concentration (CO2_F_MDS) The incoming photosynthetic photon flux density (PPFD_IN). This variable strongly covaries, but is not equal, to the shortwave incoming radiation (SW_IN) GPP estimates are based on the nighttime decomposition method, using the “most representative” of different gap-filling versions, after having applied the variable u-star filtering method (GPP_NT_VUT_REF) Soil water measured at different depths (variables starting with SWC_F_MDS) Quality flag of the CO2 flux measurement (NEE_VUT_REF_QC, for half-hourly data: 0 = measured, 1 = good quality gap-fill, 2 = medium, 3 = poor; for daily data: the fraction of good quality gap-filled half-hourly data is used for aggregation to daily data.) Other variables: Wind speed (WS), wind direction (WD), friction velocity (USTAR), relative humidity (RH) Quality flags for different variables: (ending with QC) Do not use any radiation variables derived with the “JSBACH” algorithm (not with a name that contains the string JSB) Flag indicating whether a time step is at night (NIGHT) This is implemented by: hhdf &lt;- hhdf %&gt;% select( starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), CO2_F_MDS, PPFD_IN, GPP_NT_VUT_REF, starts_with(&quot;SWC_F_MDS&quot;), NEE_VUT_REF_QC, WS, WD, USTAR, RH, ends_with(&quot;QC&quot;), -contains(&quot;JSB&quot;), NIGHT ) This reduces our dataset from 235 available variables to 68 variables. Our data set now only contains the columns we will need in our further analysis. As you can see, select() is a powerful tool to apply multiple selection criteria on your data frame in one step. It takes many functions that make filtering the columns easier. For example, criteria can be formulated based on the variable names with starts_with(), ends_with, contains(), matches(), etc. Using these functions within select() can help if several column names start with the same characters or contain the same pattern and all need to be selected. If a minus (-) is added in front of a column name or one of the mentioned functions within select(), then R will not include the stated column(s). Note that the selection criteria are evaluated in the order we write them in the select() function call. You can find the complete reference for selecting variables here. 2.2.3 Time objects The automatic interpretation of the variables TIMESTAMP_START and TIMESTAMP_END by the function read_csv() is not optimal: class(hhdf$TIMESTAMP_START[[1]]) ## [1] &quot;numeric&quot; as.character(hhdf$TIMESTAMP_START[[1]]) ## [1] &quot;200401010000&quot; As we can see, it is considered by R as a numeric variable with 12 digits (“double-precision”, occupying 64 bits in computer memory). After printing the variable as a string, we can guess that the format is: YYYYMMDDhhmm. The lubridate (Grolemund and Wickham 2011) is a package designed to help processing date and time objects. Knowing the format of the timestamp variables in our dataset, we can use ymd_hm() to convert them to actual date-time objects. library(lubridate) dates &lt;- ymd_hm(hhdf$TIMESTAMP_START) head(dates) ## [1] &quot;2004-01-01 00:00:00 UTC&quot; &quot;2004-01-01 00:30:00 UTC&quot; ## [3] &quot;2004-01-01 01:00:00 UTC&quot; &quot;2004-01-01 01:30:00 UTC&quot; ## [5] &quot;2004-01-01 02:00:00 UTC&quot; &quot;2004-01-01 02:30:00 UTC&quot; Working with such date-time objects greatly facilitates typical operations on time series. For example, adding one day can be done by: nextday &lt;- dates + days(1) head(nextday) ## [1] &quot;2004-01-02 00:00:00 UTC&quot; &quot;2004-01-02 00:30:00 UTC&quot; ## [3] &quot;2004-01-02 01:00:00 UTC&quot; &quot;2004-01-02 01:30:00 UTC&quot; ## [5] &quot;2004-01-02 02:00:00 UTC&quot; &quot;2004-01-02 02:30:00 UTC&quot; The following returns the month of each date object: month_of_year &lt;- month(dates) head(month_of_year) ## [1] 1 1 1 1 1 1 The number 1 stands for the month of the year, i.e. January. You can find more information on formatting dates and time within the tidyverse here, and a complete reference of the lubridate package is available here. 2.2.4 Variable (re-) definition Time can be noted in a multitide of different ways, which can lead to confusion. It is important to know what system was used to record times and dates and convert it to a format useful for the overall research question. Sometimes the months are the defining timescale, somtimes years. There are packages in R that can help convert timestamps and objects. We just saw an example of this using lubridate to process date and time objects. But we have not applied the conversion of the timestamp columns to date-time objects in our data frame hhdf yet. In base-R, we would do this by: hhdf$TIMESTAMP_START &lt;- ymd_hm(hhdf$TIMESTAMP_START) Modifying existing or creating new variables (columns) in a data frame is done in the tidyverse using the function mutate(). hhdf %&gt;% mutate(TIMESTAMP_START == ymd_hm(TIMESTAMP_START)) Mutating both our timestamp variables could be written as mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END)). Sometimes, such multiple-variable mutate statements can get quite long. A nice short version of this can be implemented using across(): hhdf %&gt;% mutate(across(starts_with(&quot;TIMESTAMP_&quot;), ymd_hm)) ## # A tibble: 192,864 × 65 ## TIMESTAMP_START TIMESTAMP_END TA_F SW_IN_F LW_IN_F VPD_F PA_F ## &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 2004-01-01 00:30:00 -2.22 0 304. 0.562 93.3 ## 2 2004-01-01 00:30:00 2004-01-01 01:00:00 -2.25 0 304. 0.56 93.3 ## 3 2004-01-01 01:00:00 2004-01-01 01:30:00 -2.28 0 281. 0.558 93.3 ## 4 2004-01-01 01:30:00 2004-01-01 02:00:00 -2.50 0 281. 0.565 93.3 ## 5 2004-01-01 02:00:00 2004-01-01 02:30:00 -2.72 0 281. 0.571 93.3 ## 6 2004-01-01 02:30:00 2004-01-01 03:00:00 -2.94 0 281. 0.577 93.3 ## 7 2004-01-01 03:00:00 2004-01-01 03:30:00 -3.17 0 281. 0.584 93.2 ## 8 2004-01-01 03:30:00 2004-01-01 04:00:00 -3.39 0 281. 0.59 93.2 ## 9 2004-01-01 04:00:00 2004-01-01 04:30:00 -3.61 0 264. 0.596 93.2 ## 10 2004-01-01 04:30:00 2004-01-01 05:00:00 -3.59 0 264. 0.606 93.2 ## # … with 192,854 more rows, and 58 more variables: P_F &lt;dbl&gt;, WS_F &lt;dbl&gt;, ## # CO2_F_MDS &lt;dbl&gt;, PPFD_IN &lt;dbl&gt;, GPP_NT_VUT_REF &lt;dbl&gt;, SWC_F_MDS_1 &lt;dbl&gt;, ## # SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;, SWC_F_MDS_4 &lt;dbl&gt;, ## # SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;, ## # SWC_F_MDS_4_QC &lt;dbl&gt;, NEE_VUT_REF_QC &lt;dbl&gt;, WS &lt;dbl&gt;, WD &lt;dbl&gt;, ## # USTAR &lt;dbl&gt;, RH &lt;dbl&gt;, TA_F_MDS_QC &lt;dbl&gt;, TA_F_QC &lt;dbl&gt;, ## # SW_IN_F_MDS_QC &lt;dbl&gt;, SW_IN_F_QC &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, … Our data frame now has the TIMESTAMP columns in the format ‘year-month-day hour:minute’ so ‘2004-01-01 00:00:00’ rather than as before ‘200401010000’. The seconds are added as well but there are ‘00’ throughout the data set as they were not recorded in the first place. Splitting the date into year, month and day, will simplify the analysis if we need to filter by a specific month, or date. We will encounter more ways to use mutate later in this tutorial. But a complete reference to mutate() is available here. 2.2.5 Selecting, cleaning and gap-filling Selecting variables refers to the process of deciding which variables to include and which to exclude. Variables to exclude would be ones that are irrelevant to the downstream analysis or onces that are redundant as the same or similar information is already covered by another variable. Data cleaning or cleansing is defined as the process by which inaccurate, incomplete or poor quality data is identified and removed or replaced. Gap-filling is a process that fill gaps in the data that might occur through temporary loses of measuring instrument connections or coarse data. There are numerous methods used for gap filling such as extrapolation, using proxies or regression models. For many applications, we want to filter the data so that the values of particular variables satisfy certain conditions. For example, if we have a good reason for excluding certain data points, we should do so. The function used for such tasks is filter(). As a first argument, it takes the data frame to which the filtering is applied. Remember that when using pipes (%&gt;%), the first argument is not spelled out, but is taken from what is coming from the left of %&gt;%. The second and subsequent arguments are the expressions that specify the criterion for filtering. The following operators relate to values to each other and evaluate to either TRUE or FALSE: &gt; greater than &gt;= greater or equal than &lt; smaller than &lt;= smaller or equal than !=: not equal ==: equal Multiple filtering criteria can be combined with logical (boolean) operators: &amp;: logical and |: logical or ! logical not Here, we want to check whether a variable takes any of a larger set of values. For example, if we wanted to check whether a date, given by the month of the year, is in meteorological spring, we could write something like: filter(df, month == 3 | month == 4 | month == 5) When writing code there are often several different ways to write commands that will output the same result. The just mentioned command is a little complicated. A much more powerful syntax to implement the same is: filter(df, month %in% c(3, 4, 5)) The %in% operator takes each element of what is on its left-hand-side and evaluates whether it is equal to any element of what is on its right-hand-side. Now, we can apply those different functions to our dataset. We will begin by filtering some GPP data based on its corresponding data quality information. This type of information is crucial as it allows us, e.g., to avoid using “poor” quality data for training a machine learning algorithm. In our dataset, the quality control flag for GPP_NT_VUT_REF is provided by NEE_VUT_REF_QC. Its corresponding codes are: 0 = measured 1 = good quality gap-filled 2 = medium 3 = poor To take only actually measured or good quality gap-filled GPP data (0 and 1), we can do: hhdf %&gt;% filter(NEE_VUT_REF_QC == 0 | NEE_VUT_REF_QC == 1) Or, as previously explained, we can also write: hhdf %&gt;% filter(NEE_VUT_REF_QC %in% c(0,1)) Note that filter() completely removes rows (note the information about the number of rows printed above). In some cases this is undesired and it is preferred to replace bad quality values with NA. NA is a widely used term to mark missing data and stands for “not available”, “not applicable” or “no answer”. It is important to note that specifying a value as missing is information. Dropping an entire row leads to the loss of this information. In R, NA is a “code” that lets R understand that the value is missing. Almost all operations on vectors where at least one value is NA also return NA. For example: mean(c(1, 2, NA)) ## [1] NA To remove all missing values before evaluating the function, the common argument to set in the respective function call is na.rm. By default, it is usually set to FALSE, but we can do: mean(c(1, 2, NA), na.rm = TRUE) ## [1] 1.5 For cases where we do not want to drop entire rows when applying filter(), but just replace certain values with NA, we can use mutate() instead and apply the function ifelse(). ifelse() takes a logical expression that evaluates to either TRUE or FALSE as the first argument, and returns the second argument if the expression evaluates to TRUE or the third argument if it evaluates to FALSE. In our case, we can do this by: hhdf %&gt;% mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA)) Some values in our data frame are -9999. When reading the documentation of this specific dataset, we learn that this is the code for missing data. We can replace such values in any column (except the columns starting with \"TIMESTAMP_\") with NA using the function na_if(). hhdf %&gt;% na_if(-9999) %&gt;% dim() ## [1] 192864 65 If our data contains NA values and we choose to drop the entire row should there be an NA in it, you can use another useful function drop_na(): hhdf %&gt;% na_if(-9999) %&gt;% drop_na() %&gt;% dim() ## [1] 0 65 In this case, however, we can see that every row contained an NA, resulting in all the rows being dropped, leaving only the column names. When using functions regarding the handling of NAs, be sure to know what the functions really do when applying them. 2.2.6 Functions Functions are a set of instructions encapsulated within curly brackets ({}) that generate a desired outcome. Functions contain three main elements: - They start with a name to describe their purpose, - then they need arguments, which are a list of the objects being input, - and lastly following the curly opening bracket function(x){... the code making up the ‘body’ of the function. They become increasingly important the more experienced one gets at coding. Using functions minimises the amount of code being re-written, decreases accidental errors when retyping code and are key to keeping a clean workspace. Functions have their own workspace, which means variables within the function are only ‘live’ or used when the function is running but are not saved to the global environment unless they are part of the output of the function. A good moment to think about using a function is when sections of code are being repeated again and again. Whenever possible, we should combine multiple processing steps that naturally belong together. Specifically, when the same sequence of steps must be applied to multiple datasets that have the same structure (variable names, etc.). We can combine the set of operations presented above into a single function. Once such a function is created, we can apply it to the data in one go, instead of repeating the successive steps. We will now write our first function and implement the data cleaning steps we described above. The function consists of multiple sequences of code as it contains the different steps presented above and applies them sequentially. clean_fluxnet_hh &lt;- function(df){ df &lt;- df %&gt;% ## select only the variables we are interested in select( starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), CO2_F_MDS, PPFD_IN, GPP_NT_VUT_REF, starts_with(&quot;SWC_F_MDS&quot;), NEE_VUT_REF_QC, WS, WD, USTAR, RH, ends_with(&quot;QC&quot;), -contains(&quot;JSB&quot;), NIGHT ) %&gt;% ## convert to nice time object mutate_at(vars(starts_with(&quot;TIMESTAMP_&quot;)), ymd_hm) %&gt;% ## set poor quality data to NA for multiple variables mutate( GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA), TA_F = ifelse(TA_F_QC %in% c(0,1,2), TA_F, NA), SW_IN_F = ifelse(SW_IN_F_QC %in% c(0,1,2), SW_IN_F, NA), LW_IN_F = ifelse(LW_IN_F_QC %in% c(0,1,2), LW_IN_F, NA), # relaxing filter criterion VPD_F = ifelse(VPD_F_QC %in% c(0,1,2), VPD_F, NA), PA_F = ifelse(PA_F_QC %in% c(0,1,2), PA_F, NA), # relaxing filter criterion P_F = ifelse(P_F_QC %in% c(0,1,2), P_F, NA), # relaxing filter criterion WS_F = ifelse(WS_F_QC %in% c(0,1,2), WS_F, NA), CO2_F_MDS = ifelse(CO2_F_MDS_QC %in% c(0,1,2), CO2_F_MDS, NA), SWC_F_MDS_1 = ifelse(SWC_F_MDS_1_QC %in% c(0,1,2), SWC_F_MDS_1, NA), SWC_F_MDS_2 = ifelse(SWC_F_MDS_2_QC %in% c(0,1,2), SWC_F_MDS_2, NA), SWC_F_MDS_3 = ifelse(SWC_F_MDS_3_QC %in% c(0,1,2), SWC_F_MDS_3, NA), SWC_F_MDS_4 = ifelse(SWC_F_MDS_4_QC %in% c(0,1,2), SWC_F_MDS_4, NA) ) %&gt;% ## set all -9999 to NA na_if(-9999) %&gt;% ## drop QC variables (no longer needed), except NEE_VUT_REF_QC select(-ends_with(&quot;_QC&quot;), NEE_VUT_REF_QC) return(df) } The code chunk above contains our function, with each step that will be applied to the data. We begin by giving the function a name (clean_fluxnet_hh) and defining the argument of the function. The argument of a function is simply what will be input into the function or in other words the object the function will apply the steps to. Here, the argunment is a data frame (df). Then within the {}, each step is then defined. The first set of steps selects the variables we want to include, the next part uses the function mutate() to change the date and time into a better format and then changes the poor quality data for several variables to NA. For the meteorological covariates, the filtering criteria are relaxed here, compared to the filtering criterium for GPP (*_F_QC %in% c(0,1,2) vs. NEE_VUT_REF_QC %in% c(0,1)). This is chosen to avoid excessive data loss due to missing covariates. In the modelling examples of later chapter, GPP is the target variable. That is, we’re formulating models that predict GPP from its covariates. It’s generally advisable to avoid relying on gap-filled target data for model training (fitting), hence the stricter criterium for GPP. In a next step, the missing data value of -9999 used for the data set is changed to NA, to make the further analysis easier and so functions in R to remove or omit NAs will work on our data. Lastly, no longer needed variables are removed. Before, the closing }, we tell the function what we want the output of the function to be. Here, we want the cleaned data frame to be the output, hence return(df). At this point, the function has not yet been applied to any data, it has only defined it and saved it under the name we gave it: clean_fluxnet_hh(). This is known as the function definition as it contains the instuctions of what the function should do when it is applied to the data. Next, we can apply it to our object, the data frame hhdf: ## apply our cleaning function hhdf &lt;- hhdf %&gt;% clean_fluxnet_hh() Above, the function we created called clean_fluxnet_hh() was applied to the data frame ‘hhdf’. It works within the tidyverse and applies all the functions specified above to our data line by line. The function takes a data frame as its first (and here only) argument, and it returns a data frame as its output. That is why we can write it in combination with the pipe operator as we did above. This was a very condensed introduction to functions. You will find more information here. 2.2.7 Data overview After we have done the data cleaning by imputing NA for bad quality data, we should check again, how much data we are now left with and how big the data gaps for different variables are. Knowing this is particularly important when using the data later in combination with machine learning algorithms that cannot deal with missing data. In such cases, rows where at least one value is missing (NA), have to be discarded entirely. This may not be desirable if it reduces the number of rows too drastically. We can calculate the percentage of missing data for each column with the following code: hhdf %&gt;% summarise_all(funs(100*sum(is.na(.))/length(.))) %&gt;% t() ## [,1] ## TIMESTAMP_START 0.000000 ## TIMESTAMP_END 0.000000 ## TA_F 0.000000 ## SW_IN_F 0.000000 ## LW_IN_F 0.000000 ## VPD_F 0.000000 ## PA_F 0.000000 ## P_F 0.000000 ## WS_F 0.000000 ## CO2_F_MDS 2.230069 ## PPFD_IN 7.143894 ## GPP_NT_VUT_REF 8.675543 ## SWC_F_MDS_1 12.091940 ## SWC_F_MDS_2 8.418886 ## SWC_F_MDS_3 8.418886 ## SWC_F_MDS_4 22.360316 ## WS 5.004563 ## WD 4.434213 ## USTAR 21.273021 ## RH 7.565435 ## NIGHT 0.000000 ## NEE_VUT_REF_QC 0.000000 We can see, that &gt;20% of all values for SWC_F_MDS_4 and USTAR are missing, which can be problematic for further analyses. We can remove those variables from the data frame. hhdf &lt;- hhdf %&gt;% select(-USTAR, -SWC_F_MDS_4) We can also visualise the fraction of missing data after applying our data cleaning step, using the vis_miss from the visdat package (Tierney 2017). Since applying this function on such a large dataframe can be time consuming, we will apply it on a randomly selected subset. For this, we randomly select 5’000 entries from hhdf to get a feeling for how much of which data is missing. To randomly select 5’000 entries, we use the function sample_n(). This function samples n rows randomly from the provided data frame hhdf. library(visdat) vis_miss( sample_n(hhdf, 5000), cluster = FALSE, warn_large_data = FALSE ) In the figure above we can see each column from the data frame and the percentage of data that is missing (NA) for that column in brackets after the column name. This is a visual output of the information we got earlier for the percentage of missing data per column, after which the two columns (SWC_F_MDS_4 and USTAR) were removed. The black bars indicate observations where data is missig. The two ‘TIMESTAMP’ columns are completely grey since they contain no missing data. The black and grey boxes below the figure show us the total percentage of missing data for the whole data frame combined. In total 5% of our data is missing, when only the columns with less than 20% missing data are included. Since 95% of our data contains values this is a reasonable amount to continue our analysis with. Let’s save this as a CSV file for later use. write_csv(hhdf, file = &quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-3_CLEAN.csv&quot;) Getting an overview of the quantity of missing data in a data set and each variable is an important step. It can help us decide which variables should be included for further analyses an may even highlight issues encountered during data collection. We showed you two ways to check the amount of missing data, in a table or visually. The cutoff for including variables for this data set was if over 20% data of the data for that variable was missing. Depending on the research question filtering may need to be more or less stringent. There are no general rules and it is advisable to explore sensitivity of your final results to the choice of such thresholds. 2.2.8 Data visualisation I Plotting and visualizing data is an integral part of data processing. We previously created a simple x-y line plot, using base-R, in Chapter 1. Here, we will be working with the same half-hourly time series data from the eddy covariance flux measurements for the years 2004-2014. A natural first visualisation step is therefore to plot our variables against time, for example, GPP_NT_VUT_REF versus TIMESTAMP_START: plot(hhdf$TIMESTAMP_START, hhdf$GPP_NT_VUT_REF, type = &quot;l&quot;) ggplot2 offers a powerful and (at least after an initial brain-effort) intuitive syntax for building data visualisations in a versatile, elegant, and efficient way. It defines a complete “grammar of graphics” (thus the name ggplot), which allows you to consistently apply the same syntax for different purposes. We create the same line plot as done above with geom_line(): library(ggplot2) ggplot(data = hhdf, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() This is a dense plot and we cannot distinguish patterns because variations in GPP happen at time scales that are too narrow for displaying 14 years in one plot. GPP varies throughout a day just as much as it varies throughout a season. To see this, we can focus on a narrower time span and make the plot easier to read: hhdf %&gt;% slice(24000:25000) %&gt;% ggplot(aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line(color = &quot;tomato&quot;) + labs(title = &quot;Gross primary productivity&quot;, subtitle = &quot;Site: CH-Lae&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() We can observe a variation in the measured values fluctuating between high and low values during one day. Hence, we see that we have GPP variations at the sub-daily (diurnal) time scale, as well as at the seasonal time scale. This is very typical for environmental time series data. Sometimes, we even observe a long-term trend on top of the daily signal. Dealing with such multiple scales of variations and “hidden dimensions” is something you will have to deal with a lot as an “Environmental Systems Data Scientist”. Above, we have first selected rows of the data frame with the dplyr function slice() and then piped its output (which is again a data frame) into ggplot(), which takes the data frame as the first argument. The second type of visualisation that allows one to quickly understand the data better and that often comes early in the exploratory data analysis phase is a histogram. A histogram is a figure that displays the distribution of numerical data. In a histogram data points are combined into specified ranges to display the frequency of points or number of points in each specified range group, also referred to as ‘bins’. It shows the count of how many points of a certain variable (here, GPP_NT_VUT_REF) fall into a discrete set of bins. When normalising (scaling) the “bars” of the histogram to unity, we get a density histogram. Histograms can be created with ggplot2 using the geom_histogram() function. In the example below, values of the variable of interest (GPP_NT_VUT_REF) are plotted along the x-axis (as is common for histograms). To specify the y-axis position of the upper end of the histogram bar as the density, use y = ..density.. in the aes() call. To show counts, use y = ..count... hhdf %&gt;% ggplot(aes(x = GPP_NT_VUT_REF, y = ..density..)) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &quot;red&quot;) The histogram shows the highest probability density for values between -2 and 2 GPP_NT_VUT_REF. The red line also highlights this as there is a steep drop in values above anf below the bin with the highest probability density. For this histogram, we added two visualisation layers above. First just the histogram (the dark grey bars), and second the continuous density plot as a red line. Both share the same aesthetics specification with aes(). Find a complete reference to ggplot here (Hadley Wickham 2016). If you are looking for a more complete tutorial on data visualisation which includes exercises and contains information on how to colour specific points, change the shape of certain data points, add different sized points depending on a factor from the data or to add several plots with facet_wrap() go to this link. For those who want an in-depth, detailed explanation of many different data visualisation options or how to “make visualizations that accurately reflect the data, tell a story, and look professional” go to the following guide book (C. Wilke 2019). 2.2.9 Aggregating Aggregating in data science refers to the process of bringing together different data or parts of the data to form a more summarised version. This can be data from multiple sources or just a compilation of the data already present. In R this can be done within a data frame or between data frames. Here, we will group our data by a specific variable (date) and summarise the variable GPP_NT_VUT_REF. All data frames have two dimensions, rows and columns. Our data frame is organised along half-hourly time steps in rows. These time steps belong to different days, months, and years, although these “dimensions” are not reflected by the structure of the data frame and we do not have columns that indicate the day, month or year of each half-hourly time step. This would actually be redundant information since the date-time objects of columns TIMESTAMP_* contain this information. The tidyverse makes it very easy to work with such “hidden dimensions” of a data frame. Let’s say we want to calculate the mean of half-hourly GPP across each day That is, to aggregate our half-hourly data to daily data by taking a sum. You see, there are two pieces of information needed for an aggregation step: The factor (or “hidden dimension”) that groups a vector of values for collapsing it into a single value, and the function used for collapsing values. This function should take a vector as an argument and return a single value as an output. These two steps are implemented by the dplyr functions group_by() and summarise() and the nice and intuitive code that solves our problem of aggregating to daily values by averaging (mean) looks like this: ddf &lt;- hhdf %&gt;% mutate(date = as_date(TIMESTAMP_START)) %&gt;% # converts the ymd_hm-formatted date-time object to a date-only object (ymd) group_by(date) %&gt;% summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE)) The code chunk above has aggregated the data frame hhdf into a new data frame ddf with two columns. The first column is the date, which was mutated from the year-month-date hour:minute to just a date format, so from “2004-01-01 00:00:00” it has been transformed to “2004-01-01”. Then the rows were grouped by date and summarised in a second column to a mean GPP_NT_VUT_REF per date. We now have just one GPP value per day. More info on how to group values using summarise functions here, or a summary on the inputs the function group_by() and summarise() take. Using filter(), we can now plot daily mean GPP for all days in the year 2007. ddf %&gt;% filter(year(date)==2007) %&gt;% # same functions as above can be applied to &#39;date&#39; ggplot(aes(date, GPP_NT_VUT_REF)) + geom_line() + geom_point() + # we can overlay multiple plot layers labs(title = &quot;Gross primary productivity&quot;, subtitle = &quot;Site: CH-Lae&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) We observe high outlying values sometime in May. Since we do not know the reason we have these high values, a next step would be to investigate them further. To learn more, it would help to know how many of the half-hourly data points in each (aggregated) day are based on “problematic” data and how many are missing (NA). To get this information, we can, again, use two aggregation functions group_by() and summarise(), now with multiple functions to summarise different variables. ddf &lt;- hhdf %&gt;% mutate(date = as_date(TIMESTAMP_START)) %&gt;% # converts time object to a date object group_by(date) %&gt;% summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE), n_datapoints = n(), # counts the number of observations per day n_measured = sum(NEE_VUT_REF_QC == 0), # counts the number of actually measured data (excluding gap-filled and poor quality data) PPFD_IN = mean(PPFD_IN, na.rm = TRUE), # we will use this later .groups = &#39;drop&#39; ) %&gt;% mutate(f_measured = n_measured / n_datapoints) # calculate the fraction of measured values over total observations ddf ## # A tibble: 4,018 × 6 ## date GPP_NT_VUT_REF n_datapoints n_measured PPFD_IN f_measured ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 NaN 48 0 NaN 0 ## 2 2004-01-02 NaN 48 0 NaN 0 ## 3 2004-01-03 NaN 48 0 NaN 0 ## 4 2004-01-04 NaN 48 0 NaN 0 ## 5 2004-01-05 NaN 48 0 NaN 0 ## 6 2004-01-06 NaN 48 0 NaN 0 ## 7 2004-01-07 NaN 48 0 NaN 0 ## 8 2004-01-08 NaN 48 0 NaN 0 ## 9 2004-01-09 NaN 48 0 NaN 0 ## 10 2004-01-10 NaN 48 0 NaN 0 ## # … with 4,008 more rows In this subsection, we reduced the full data frame ( hhdf) to a new data frame ( ddf) with six columns showing information such as the number of observartions per day (n_datapoints), the number of measured data points per day (n_measured), mean GPP for each date and the fraction of the total observations that were measured values (f_measured). 2.2.10 Data visualisation II After completing the aggregation above, we now have a new “hidden dimension” in our data frame: Each GPP measurement is located not only along a time axis, but also along a “data quality axis”, measured by the fraction of actually measured (not gap-filled) half-hourly data points per day (f_measured). In the next part of the tutorial, we will show you how to make the most of such “hidden dimensions” by adding them as an additional layer to our data plots using extra colours or regressions. We will proceed by using this additional axis to enhance our plot and visualising it the extra dimension in the data. We achieve this by colouring our points according to f_measured. In other words, we “map” f_measured to the color axis, similar to how we “mapped” time and GPP to the x and y axes before. When adding such an additional mapping to visualisation dimensions (“aesthetics”), we have to specify it using aes(). This only affects the points and the color of points, while the lines and points and their position in x-y space is shared. Hence, we write aes(x = date, y = GPP_NT_VUT_REF) in the ggplot() function call (indicating that all subsequent additions of geom_ layers share this x-y mapping); while aes(color = f_measured) is specified only in the geom_point() layer. ddf %&gt;% filter(year(date)==2007) %&gt;% # same functions as above can be applied to &#39;date&#39; ggplot(aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + geom_point(aes(color = f_measured)) + # we can overlay multiple plot layers! labs(title = &quot;Gross primary productivity&quot;, subtitle = &quot;Site: CH-Lae&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_color_viridis_c(direction = -1) # &quot;viridis&quot; continuous color scale in inverse direction We observe that the points with particularly low GPP during summer months are predominantly based on gap-filled half-hourly data. This is an insight we would never have gotten by just looking at the naked values in our data frames. Data visualisations are essential for guiding analyses and processing throughout all steps. Having learned this, we now have a justification for applying further data filtering criteria. In our intial research question, we want to know not only about variations in GPP, but also what controls it. We want to know the environmental factors that determine the variations in GPP. The environmental factors that influence GPP are known as the covariates of GPP. In a machine learning context, we call them “predictors” or “features”. To answer this question, we will have to turn to modelling. Here, we refer to modelling in the wider sense of predicting observed variations in a target variable based on empirical relationships with a set of predictors. Often, you will start delving into your research question with some a priori understanding of the system from which you have observational data. Such an understanding may be informed by previous observations and their interpretations, or by theory. In middle school already we learnt that photosynthesis requires sunlight and it shouldn’t come as a surprise that the more sunlight there is in a day, the higher the GPP. Such a presumed positive (maybe even monotonically increasing) relationship is also consistent with the apparent agreement between the scales of variation in GPP and the scales of variation in incoming solar radiation (dark night, bright day; dark winter, bright summer). In our dataset, PPFD_IN is the incoming photosynthetic photon flux density, measured in mol photons (that come in the right wavelength to be used for photosynthesis). We can plot this relationship to vizualise how it correlates with GPP using the daily data. ddf %&gt;% ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF)) + geom_point() + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + ylim(-10, 25) We observe a clear trend of increasing GPP with increasing PPFD, and it looks largely linear. Data collected in the field often has a substantial amount of scatter. Since we previously added a column to the data frame containing the data quality, the next step is to see if the data quality explains some of the scatter in the data. To investiagte this, we can “map” the data quality dimension onto the color aesthetic of the plot. ddf %&gt;% ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = f_measured)) + geom_point() + scale_color_viridis_c(direction = -1) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + ylim(-10, 25) We can notice that the colours showing the data quality or f_measured are mixed across the range of the values for GPP and PPFD. The high GPP values, we previously found to be associated with low fractions of underlying measured data in the time series plot, are not explained by simultaneously high PPFD. In the plot, we see a core cluster of points in the centre forming a positive trend with fewer points outside this denser ‘core’ area. These outlying points are lighter and do not fit the linear relationship as well. Despite scattered data points, there is a positive linear relationship between GPP and PPFD. To find the best fit of this linear relationship, meaning the straight line that best fits our data points, we will move on to modelling using a univariate linear regression. The section below, will serve as a brief introduction, since later chapters will go into more detail on modelling and machine learning. We start by making a simple linear model using the function lm() and adding in our desired variables: linmod &lt;- lm(GPP_NT_VUT_REF ~ PPFD_IN, data = ddf) We can also directly plot the fitted linear regression line over the scatter plot using geom_smooth(method = \"lm\"). Rather than first making a linear model and then plotting it onto the data. ddf %&gt;% ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + ylim(-10, 25) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) In red in the plot above is the linear regression fitted directly to our data using geom_smooth(), without needing an additional step of first making a linear model and then plotting it over the data. Based on our previous finding that the data quality is associated with GPP values, which is reflected in their relationship with PPFD, we can fit separate linear regression models for data where f_measured is greater than versus less than 0.5. For this, we can create a new variable with mutate(more_measured = as.factor(f_measured &gt; 0.5))). The new variable more_measured contains binary information as the data is either greater or less than 0.5. By adding new factors we can add another previously “hidden” dimension to our data. And because it is a categorical variable and not a continuous one, R treats it as a factor. We can plot this new variable more_measured onto the color aesthetic as we did before. Since we specify this aesthetic below in the ggplot() function call, all subsequent visualisation layers will respect it, also geom_smooth(). ddf %&gt;% mutate(more_measured = as.factor(f_measured &gt; 0.5)) %&gt;% ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = more_measured)) + geom_point(alpha = 0.2) + # set opacity to 20% to avoid underscernible overplotting geom_smooth(method = &quot;lm&quot;) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + ylim(-10, 25) We observe a slight difference in the slopes of the respective linear models. Note also that in the above ggplot() call, we specified the aesthetics as aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = more_measured). This triggers all subsequent additions of visualisation layers (here: geom_piont() and geom_smooth) to use the same aesthetics for plotting. The distinction by the same colors is applied both to the points and to the smoothing lines. Scatter plots can appear overcrowded. In this example, particularly in the low PPFD range, many points are plotted over each other, which may hide some information. To avoid obscuring important details in the plot, we may want to visualise the density of points. We want to plot how many points fall within bins of a certain range values in GPP and PPFD, this creates grid cells in the GPP-PPFD-space. We can create such a raster plot that measures the density using stat_density_2d(): ddf %&gt;% ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF)) + stat_density_2d( geom = &quot;raster&quot;, #the geometric object to display the data (in this case: rectangles) aes(fill = after_stat(density)), #using `density`, a variable calculated by the stat contour = FALSE ) + scale_fill_viridis_c() + ylim(-5, 15) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) In the figure, we can observe a much higher density of points between PPFD values 0 and 200 and between GPP values of -2 to 5. The density gets weaker as as PPFD increases but GPP stays within the same values. We can also observe a second cluster, albeit at a lower density. This starts at a PPFD value of around 200 and at a GPP value over 5. This second cluster has a steeper slope creating an interesting “separation” of points, where the increase in GPP with increasing PPFD is steeper for some than for others. Again, we can use our a priori understanding of the system to formulate hypotheses and test them by finding the appropriate visualisation type. Here, one hypothesis could be that the slope is steeper in some months than in others. This is actually more than just a vague guess. Since the relationship between incoming light and ecosystem photosynthesis is strongly affected by how much of this light is actually absorbed by leaves, and because the amount of green foliage varies strongly throughout a year (this is data from a mixed forest), the slope of the regression between PPFD and GPP should change between months. To test this hypothesis, we will visualize the slopes of each month by fitting separate linear regression models. Note that each month encompasses data from multiple years, so clear trends will only become clear if there is a commonality between the same months of different years. ddf %&gt;% mutate(month = as.factor(month(date))) %&gt;% ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = month)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ylim(-5, 15) + labs(x = expression(paste(&quot;PPFD (&quot;, mu, &quot;mol m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) The plot verifies that the relationships indeed differ slightly between months. In spring months (3 = March, 4 = April), light levels can be already quite high, but GPP remains much lower than in summer months. A reason for this may be that GPP is not influenced by PPFD alone. There are other environmental variables, such as temperature or moisture levels, etc., that influence GPP and may not yet be at the levels reached during the summer months. 2.2.11 Functional programming I The daily data is given for a set of eddy covariance measurement sites. Data for each site is given in a separate file. When dealing with such a setup, we will likely encounter situations where we have to apply the same sequence of data wrangling steps (or functions) to multiple instances of the same object class. For example, we will have to apply the same data cleaning steps or fitting a regression model to each site’s data. In this example, the object class is a data frame, and the “multiple instances” are the data frames for each site. This correspnds to functional programming. We can also combine each site’s data frame into a single large one, e.g., by “stacking” them along the time dimension (rows). In this case, we create a new “hidden dimension” - the site identity. In this subsection, you will learn how to keep an overview and code efficiently while dealing with such large data frames - always using the tidyverse in R. The purrr package of tidyverse offers the functionalities for functional programming. It makes use of lists and applies (or “maps”) a function to each element of the list. Let’s start by creating a list of paths that point to the files with daily data. They are all located in the directory \"./data\" and share a certain string of characters in their file names \"_FLUXNET2015_FULLSET_DD_\". vec_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;_FLUXNET2015_FULLSET_DD_&quot;, full.names = TRUE) print(vec_files[1:5]) ## [1] &quot;./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv&quot; ## [2] &quot;./data/FLX_BE-Lon_FLUXNET2015_FULLSET_DD_2004-2014_1-3.csv&quot; ## [3] &quot;./data/FLX_BE-Vie_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv&quot; ## [4] &quot;./data/FLX_CH-Cha_FLUXNET2015_FULLSET_DD_2005-2014_2-3.csv&quot; ## [5] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; vec_files is now a vector of 35 files for 35 sites. In simple base-R, we could read them in at once using a simple for loop. The following creates a list of data frames that are generated by read_csv() with the argument ifil iteratively changing, taking values of elements in vec_files. list_df &lt;- list() for (ifil in vec_files){ list_df[[ifil]] &lt;- read_csv(ifil) } In the tidyverse, the above loop can be written on one line, using the function map() from the purrr package, as: list_df &lt;- purrr::map(as.list(vec_files), ~read_csv(.)) Note that map() applies the function read_csv() to elements of a list. Hence, we first have to convert the vector vec_files to a list. The list is always the first argument within the function. Note two new symbols (~ and .) in the command. The ~ always goes before the function that is applied, or mapped, to elements of the list. The . indicates where the elements of the list would go if spelled out (e.g., read_csv(.) would here be read_csv(\"./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv\") for the first iteration). The output of map() is again a list. There are many variants of the function map() that each have a specific use. A complete reference for all purrr functions is available here. A useful and more extensive tutorial on purrr is available here. The above map() call does not return a named list as our for loop created. But we can give each element of the returned list of data frames different names by: names(list_df) &lt;- vec_files # this makes it a named list We will apply a similar cleaning function to this data set as we did earlier for half-hourly data. Unfortunately, we cannot reuse the same code because not all variables that are given in the half-hourly data are available also in the daily data and because the quality control flag is defined differently. We can define the daily data cleaning function: ## function definition clean_fluxnet_dd &lt;- function(df){ df %&gt;% ## select only the variables we are interested in select(starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), CO2_F_MDS, PPFD_IN, GPP_NT_VUT_REF, NEE_VUT_REF_QC, USTAR, ends_with(&quot;QC&quot;), -contains(&quot;JSB&quot;) ) %&gt;% ## convert to a nice date object mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %&gt;% ## not setting heavily gap-filled data to zero ## set all -9999 to NA na_if(-9999) %&gt;% ## drop QC variables (no longer needed), except NEE_VUT_REF_QC select(-ends_with(&quot;_QC&quot;), NEE_VUT_REF_QC) } The cleaning function is very similar to the one we used earlier in this tutorial. The main objectives it fulfills are to select only the columns relevant for the further analysis using select() and the column names, then to convert the column TIMESTAMP into a more manageable format, then we convert -9999 into NA and drop variables that are now no longer needed. Once, we have constructed our function, we will apply this ‘cleaning’ function to each site’s data frame as follows: list_df &lt;- purrr::map(list_df, ~clean_fluxnet_dd(.)) Have different data frames as elements of a list may be impractical. Upon closer examination, the data frames read in here all have similar shapes, meaning they share the same columns. They only differ by the number of rows, and the data values they contain. This suggests that we can “stack” each data frame along its rows. This can be done using bind_rows() and we can automatically create a new column \"siteid\" in the stacked data frame that takes the name of the corresponding list element. ddf_allsites &lt;- bind_rows(list_df, .id = &quot;siteid&quot;) head(ddf_allsites) ## # A tibble: 6 × 14 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F CO2_F_MDS ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ./data/FLX… 1996-01-01 2.85 11.9 327. 0.262 100. 0.882 1.64 NA ## 2 ./data/FLX… 1996-01-02 0.716 29.6 290. 0.273 101. 0.093 1.28 NA ## 3 ./data/FLX… 1996-01-03 1.01 15.0 318. 0.272 102. 0.41 1.23 NA ## 4 ./data/FLX… 1996-01-04 1.59 22.4 299. 0.624 102. 0.113 3.47 NA ## 5 ./data/FLX… 1996-01-05 2.02 31.5 276. 1.90 100. 0.359 3.25 NA ## 6 ./data/FLX… 1996-01-06 1.61 10.5 308. 1.36 99.7 0.478 3.38 NA ## # … with 4 more variables: PPFD_IN &lt;dbl&gt;, GPP_NT_VUT_REF &lt;dbl&gt;, USTAR &lt;dbl&gt;, ## # NEE_VUT_REF_QC &lt;dbl&gt; This creates one single large data frame containing all sites’ data (&gt;167’000 rows) and adds a column named \"siteid\" that is automatically created by using the names of the list elements of list_df. As above for the half-hourly data, we will check the fraction of missing data for each variable. ddf_allsites %&gt;% summarise_all(funs(100*sum(is.na(.))/length(.))) %&gt;% t() ## [,1] ## siteid 0.000000 ## TIMESTAMP 0.000000 ## TA_F 0.000000 ## SW_IN_F 0.000000 ## LW_IN_F 0.000000 ## VPD_F 0.000000 ## PA_F 0.000000 ## P_F 0.000000 ## WS_F 0.000000 ## CO2_F_MDS 2.710296 ## PPFD_IN 22.947253 ## GPP_NT_VUT_REF 2.400650 ## USTAR 21.993209 ## NEE_VUT_REF_QC 1.527306 From the table above we can see that the first nine columns listed have no missing data, and a few (CO2_F_MDS, GPP_NT_VUT_REF, NEE_VUT_REF_QC) have less than 3% missing data. For PPFD_IN and USTAR, however, over 20% of the data in those columns in missing (NA). We will also visualise these data gaps to check that our filtering criteria are not too strong. vis_miss(ddf_allsites, cluster = FALSE, warn_large_data = FALSE ) We see that overall, there is 3.7% missing data. Specifically, PPFD_IN (the photosynthetic photon flux density) is often missing. In our dataset, we also have information about shorwave radiation (SW_IN_F), which scales largely linearly with PPFD_IN. For modelling below, we can therefore use SW_IN_F instead. 2.2.12 Strings In R, strings refer to any set of characters between two double or single quotes: - \"This is a string in R\" - 'This is also a string in R' Knowing how to create and manipulate strings can be useful if you need to manipulate a sample name or extract some information from a string. Below, we will show you how to extract a specific part of a string to create a new column. The column siteid currently contains strings specifying the full paths of the files that were read in earlier. Since the file path is not information we need, we would like to extract the site name from these strings. The file names follow a clear pattern, which highlights why naming files wisely can often make life a lot simpler. ddf_allsites$siteid %&gt;% head() ## [1] &quot;./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv&quot; ## [2] &quot;./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv&quot; ## [3] &quot;./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv&quot; ## [4] &quot;./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv&quot; ## [5] &quot;./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv&quot; ## [6] &quot;./data/FLX_BE-Bra_FLUXNET2015_FULLSET_DD_1996-2014_2-3.csv&quot; The paths each start with the subdirectory where they are located (\"./data/\"), then \"FLX_\", followed by the site name (the first three entries of the table containing data from all sites are for the site \"BE-Bra\"), and then some more specifications, including the years that respective files’ data cover. The most effective way to extract the site name from all these strings is achieved using the package stringr. stringr contains a whole host of functions for working with strings, such as: - detecting matches (str_detect(),str_count(), etc), - subsetting strings (str_sub(),str_extract(), etc), - managing the length of strings (str_length(),str_trim(), etc), - mutating or odering strings (str_replace(), str_to_lower(), str_sort(), etc). Here, we would like to extract the six characters, starting at position 12. vec_sites &lt;- str_sub(vec_files, start = 12, end = 17) head(vec_sites) ## [1] &quot;BE-Bra&quot; &quot;BE-Lon&quot; &quot;BE-Vie&quot; &quot;CH-Cha&quot; &quot;CH-Dav&quot; &quot;CH-Fru&quot; Next, we overwrite the values of column \"siteid\" with just these six characters. ddf_allsites &lt;- ddf_allsites %&gt;% mutate(siteid = str_sub(siteid, start = 12, end = 17)) head(ddf_allsites) ## # A tibble: 6 × 14 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F CO2_F_MDS ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BE-Bra 1996-01-01 2.85 11.9 327. 0.262 100. 0.882 1.64 NA ## 2 BE-Bra 1996-01-02 0.716 29.6 290. 0.273 101. 0.093 1.28 NA ## 3 BE-Bra 1996-01-03 1.01 15.0 318. 0.272 102. 0.41 1.23 NA ## 4 BE-Bra 1996-01-04 1.59 22.4 299. 0.624 102. 0.113 3.47 NA ## 5 BE-Bra 1996-01-05 2.02 31.5 276. 1.90 100. 0.359 3.25 NA ## 6 BE-Bra 1996-01-06 1.61 10.5 308. 1.36 99.7 0.478 3.38 NA ## # … with 4 more variables: PPFD_IN &lt;dbl&gt;, GPP_NT_VUT_REF &lt;dbl&gt;, USTAR &lt;dbl&gt;, ## # NEE_VUT_REF_QC &lt;dbl&gt; Knowing how to manipulate strings or find patterns amongst regular expressions can be very useful, when filtering data or adjusting sample names. Here, we saw how to extract the characters from the string needed for the siteid using str_sub(). The stringr package (Hadley Wickham 2019b) offers a set of very handy tools to work with strings and regular expressions, see this cheat sheet to see how to find for specific patterns in a string and many other useful tools for manipulating strings or go here for more general introduction with examples. 2.2.13 Combining relational data Metadata is additional information on the data that will make understanding ad working with the data easier. Metadata can take many forms from descriptive to statistical or structural. Metadata is a way to explain important elements of the data to other people who did not collect the data or previously work on it. In many situations, we want to combine information from multiple data frames into a single one. In our case, we are interested in knowing more about the sites for which we have time series data. We are interested in meta-information about the sites, for example, the vegetation type, geographical location, elevation, etc. In such cases, where information about common sets of units (here sites) is distributed across multiple data objects, we are referring to relational data. These cannot be not completely independent data but must contain a common element that links them. In our case, this is the site identity or site name. Specifically, this means that the same labeling of site identities has to be available from all relational data objects. It may seem counterintuitive to keep an extra data in a separate data object but in some cases this can simplify the various data objects. Think for example if we wanted to have a column containing the elevation of a site. Each site has a fixed elevation, but if we were to add the elevation column to the data set containing time series data for say temperature, then the elevation for each respective site would be listed hundreds or even thousands of times for each series entry. This would inflate the memory of the object considerably without actually adding information. In this case, having a separate data frame containing information that stays the same for a site irrespective of factors such as seasons or days that can change, makes sense. This data frame would have as many rows as there are sites and include attibutes such as full site name, country, elevation, latitude, longitude, year the site started recording data and even the land cover. Therefore it helps to keep such differently structured data objects separate and combine them only during the analysis step. A comprehensive collection of FLUXNET site meta information is freely available from Falge et al.. We will now read in this file (\"fluxnet_site_info_all.csv\"). df_sites &lt;- read_csv(&quot;./data/fluxnet_site_info_all.csv&quot;) head(df_sites) ## # A tibble: 6 × 21 ## siteid fluxnetid keyid sitename countryid land_unit status latitude longitude ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 AR-Lac ar.la… La Cigu… Argentina South Am… Active -29.3 -61.0 ## 2 12 AT-Lan at.la… Langenf… Austria Europe Inact… 47.1 11.0 ## 3 13 AT-Leu at.le… Leutasch Austria Europe Inact… 47.4 11.2 ## 4 14 AT-Neu at.ne… Neustif… Austria Europe Active 47.1 11.3 ## 5 15 AT-Sch at.sc… Schlitt… Austria Europe Inact… 47.4 11.2 ## 6 17 AU-Cas au.ca… Burdeki… Australia Australi… Active -19.6 147. ## # … with 12 more variables: year_began &lt;chr&gt;, network1 &lt;chr&gt;, network2 &lt;chr&gt;, ## # network3 &lt;chr&gt;, koeppen_climate &lt;chr&gt;, gtopo30_elevation &lt;chr&gt;, ## # igbp_land_use &lt;chr&gt;, umd_land_cover &lt;chr&gt;, lai_fpar &lt;chr&gt;, ## # npp_land_cover &lt;chr&gt;, plant_functional_type &lt;chr&gt;, network1_id &lt;chr&gt; The file contains information on the the names of all the sites globally (844 rows for 844 sites), their coordinates and elevation, the countries the sites are in, whether they are still active the land cover, and much more. There are many more sites than we have data for in mainland Europe (35 sites). The ‘key’ variable that the two data frames have in common is the standard site ID name that is used for FLUXNET sites. In the sites table (df_sites), the column identifiying the site is called fluxnetid. In the temporal dataset ddf_allsites, the site column is called siteid. To combine (“join”) the two data frames, the joining “key” ID has to be named the same way (here \"siteid\") and should contain the same set of different values (site names in our case). Hence, we have to rename the respective column in one of our data frames in order to join them. df_sites &lt;- df_sites %&gt;% select(-siteid) # remove this variable first because it doesn&#39;t contain the name we want ddf_allsites_joined &lt;- df_sites %&gt;% rename(siteid = fluxnetid) %&gt;% right_join(ddf_allsites, by = &quot;siteid&quot;) %&gt;% ## perform some variable renaming for our own taste rename(lon = longitude, lat = latitude, elv = gtopo30_elevation ) To understand the differences between the data frames before and after the join, we check the number of columns in the two data frames provided as arguments and in the resulting data frame. ncol(df_sites) + ncol(ddf_allsites) - 1 ## [1] 33 ncol(ddf_allsites_joined) ## [1] 33 The two data frames df_sites and ddf_allsites (minus the column they are joined by) have the equivalent number of columns as the joined data frame ddf_allsites_joined. As you can see below, the number of columns differ between the joined and not joined data frames. Since ddf_allsites_joined contains all the columns present in ddf_allsites, we do see identical column names from column [21] onward in the list of ddf_allsites_joined’s columns, as ddf_allsites was the second data frame in the joining argument. names(ddf_allsites) ## [1] &quot;siteid&quot; &quot;TIMESTAMP&quot; &quot;TA_F&quot; &quot;SW_IN_F&quot; ## [5] &quot;LW_IN_F&quot; &quot;VPD_F&quot; &quot;PA_F&quot; &quot;P_F&quot; ## [9] &quot;WS_F&quot; &quot;CO2_F_MDS&quot; &quot;PPFD_IN&quot; &quot;GPP_NT_VUT_REF&quot; ## [13] &quot;USTAR&quot; &quot;NEE_VUT_REF_QC&quot; names(ddf_allsites_joined) ## [1] &quot;siteid&quot; &quot;keyid&quot; &quot;sitename&quot; ## [4] &quot;countryid&quot; &quot;land_unit&quot; &quot;status&quot; ## [7] &quot;lat&quot; &quot;lon&quot; &quot;year_began&quot; ## [10] &quot;network1&quot; &quot;network2&quot; &quot;network3&quot; ## [13] &quot;koeppen_climate&quot; &quot;elv&quot; &quot;igbp_land_use&quot; ## [16] &quot;umd_land_cover&quot; &quot;lai_fpar&quot; &quot;npp_land_cover&quot; ## [19] &quot;plant_functional_type&quot; &quot;network1_id&quot; &quot;TIMESTAMP&quot; ## [22] &quot;TA_F&quot; &quot;SW_IN_F&quot; &quot;LW_IN_F&quot; ## [25] &quot;VPD_F&quot; &quot;PA_F&quot; &quot;P_F&quot; ## [28] &quot;WS_F&quot; &quot;CO2_F_MDS&quot; &quot;PPFD_IN&quot; ## [31] &quot;GPP_NT_VUT_REF&quot; &quot;USTAR&quot; &quot;NEE_VUT_REF_QC&quot; Here, we applied the function right_join(). This can be understood as joining the data frame given by the first argument to right_join() (here, what is being piped from the left side of the pipe) onto the data frame given by the second argument (ddf_allsites). The output of right_join() has the same number or rows as the data frame on the “right” (the second argument) and is, as for all tidyverse functions, a data frame. There is also a left_join() that creates a new data frame with the number of rows corresponding to the data frame on the “left” (the first argument). You can find more on relational data and joining data frames, e.g., here, or here. 2.2.14 Key points from the tutorial The aim of this tutorial was to introduce the main processes considered as data wrangling. These include data exploration, data cleansing and filtering, data transformation, adding ‘hidden’ dimensions and data visualisation. This is an iterative process and certain steps may be repeated multiple times, as more information is gathered on the data. In this tutorial, we introduced: the use of select() and the functions that go within it (starts_with(), ends_with, contains(), matches(), etc.) to keep only the relevant variable in a data frame. how the package lubridate can help convert time data into a useful date-time format and apply this to the whole data set using mutate(). methods to replace values such as -9999 that signify missing values with NA so we can use common functions to exclude or omit the missing data. the power of using functions and their three main components: name, arguments that go within function() and the body of the function within curly brackets {}. ways to quantify how much missing data each variable contains and how to use this as a selection criterion for which variables should be excluded from further analysis. the diversity in plotting options for different data usingggplot2(). We made line graphs, histograms, scatter plots with and without regression lines and even a density plot. Finding ways to add ‘hidden’ data dimensions is a way to visualise even more information. using summarise() and group_by() to aggregate variables by the mean over a certain time period (e.g. date, month, year, season, etc.). the many functions in the package stringr to manipulate strings. how to combine the information in two data frame that contain a common column using right_join() or left_join(). 2.3 Exercise Detecting unusual values or outliers in datasets is important, as they increase the variablility of data and can distort statistical tests performed on the data. The decision of which or whether outliers should be removed is not always an easy one, since outliers can also provide valuable information on the dataset. Outliers could come from errors or problems in the sampling procedure or be errors in measurements or arise during dat entry into a database (e.g. typing a wrong value or putting the decimal point in the wrong place) or even be a by-product of natural variation in the data. The aim of this exercise is to get familiarised with detecting and removing outliers. Below a pseudo code is provided to help you complete the exercise. A pseudo code is plain language description of what will be done by an algorithm or function. It plans out each step that will be completed. It can be helpful for data scientists to create a pseudo code before starting any analyses to ensure they complete all the desired steps and do not miss out anything important. Outlier removal: Based on the half-hourly dataset for site CH-Lae, aggregated to daily means, identify outliers in GPP_NT_VUT_REF with respect to the linear relationship between GPP_NT_VUT_REF and PPFD_IN. To do so, first fit a linear regression model using lm(). This function returns a list of objects, one of which is residuals. Determine outliers as the “outlying” points in the distribution of residuals. You may use the base-R function boxplot.stats() and set the argument coef accordingly to our customised threshold definition. *** Remove outliers by setting values in the data frame (aggregated daily data frame for CH-Lae) to NA. *** Create a scatterplot of all daily data (GPP vs. PPFD) and highlight outliers that are removed by step 2. *** Visualising diurnal and seasonal cycles: Using the half-hourly dataset for site CH-Lae, visualise how GPP (GPP_NT_VUT_REF) varies on two time scales: diurnal (within-day at hourly time scale) and seasonal. To implement this, follow the following steps: Summarise half-hourly data for each data across multiple years to get a mean seasonality with a mean diurnal cycle for each day of the year. You will use functions from the lubridate package (e.g., yday()). To deal with date-time objects, use the lubridate package. Enter ?day to get more hints. Create a raster plot (geom_raster()), mapping the hour of the day to the x-axis, the day of the year to the y-axis, and the magnitude of GPP_NT_VUT_REF to color (fill). Make this figure ready for publication by adding nice labels and choosing a good color scale. You can use the pseudo code below as a guidance or come up with your own solution ## 1. Outlier Removal library(tidyverse) library(lubridate) # not part of the automatic load of tidyverse # read half-hourly csv hhdf &lt;- ... %&gt;% # interpret -9999 as missing value ... %&gt;% # interpret timestamp variable as a date-time object mutate(across(starts_with(...), ymd_hm)) # aggregate to daily ddf &lt;- hhdf %&gt;% # create a date-only object ... %&gt;% # aggregate ... # fit linear regression model linmod &lt;- ... # get box plot statistics with for determining &quot;outlying&quot; points out_boxplot &lt;- boxplot.stats(...) # record the row numbers of outlying points based on the output list element &#39;out&#39; # row numbers are the names of elements in out_boxplot$out, provided as strings. # convert them to integers. idx_outlying &lt;- names(...) %&gt;% as.integer() ## 2. Remove Outliers # In base-R, this could be done as: ... # In tidyverse style: ddf &lt;- ddf %&gt;% mutate(rownumber = row_number()) %&gt;% # could also do: mutate(rownumber = 1:nrow(.)) ... ## 3. Create scatterplot ddf %&gt;% ggplot(..., aes()) + geom_...(...) ## 4. Visualising cycles ## a. Half-hourly dataset hhdf_meanseason &lt;- hhdf %&gt;% mutate(hour_day = ..., day_year = ...) %&gt;% ... ## b. Raster plot hhdf_meanseason %&gt;% ggplot(...) + ... ## c. Make raster plot publishable - This is up to your judgement. ... References "],["ch-03.html", "Chapter 3 Data variety 3.1 Introduction 3.2 Tutorial 3.3 Exercise", " Chapter 3 Data variety 3.1 Introduction 3.1.1 Overview In this chapter, we will look at data variety in environmental sciences. We will start with the data from one eddy covariance measurement site in Switzerland encountered in Chapter 1, and complement it with remote sensing data. Specifically, we will download remote sensing data that measures vegetation greenness data, quantified by the normalised difference vegetation index, NDVI. We are interested in remote sensing data, because we expect ecosystem photosynthesis (gross primary production), measured at the eddy covariance tower, to covary with NDVI. Such covariation can provide powerful information for modelling. For example, we can train a machine learning model with the observed covariation between measured GPP and NDVI at eddy covariance sites and use that model to predict GPP from data obtained in space (NDVI is available for the whole globe, while GPP can only be measured locally.) You will be doing this modelling in exercise 13. Here, we will focus on working with the spatial aspects of the data (since remote sensing data is inherently spatial) and introduce the tool set to work with geo-spatial data. To get familiar with spatial data types, we will use a variety of data from freely accessible sites. We can obtain abiotic and biotic conditions at the tower locations. We will be spatially plotting the eddy towers you have already encountered in the first two chapters and extracting values for these locations from other data. By doing this, we can show that the climate at these sites spans a gradient temperature and landcover. 3.1.2 Learning objectives After this learning unit, you will be able to … Explain the possible sources of data in environmental sciences. Understand the range of operations applied to data from basic to complex. Read various types of data in R and apply basic operations. Understand the structure of spatial data, including rasters and shapefiles. Apply a range of operations to prepare data for analyses. 3.1.3 Key points from the lecture Data operations range in complexity, here they are listed from simplest to extremely complex: Statistical operations Similarity metrics Ordinations Clustering Classifications Regressions Neural network deep learning Examples of methods for the computing differences between observations associated with a set of features: Euclidean distance is used for the simple quantification of variation. PCA is a valuable tool to explore variation and reduce dimension. Hamming distance helps measure the difference between strings. Jaccard index describes the similarity between two or more binary data sets. Machine learning is loosely defined as any learning done by a computer. It can be divided into two main categories… Unsupervised learning: without explanatory data, which uses clustering to obtain a categorical output Supervised learning: with explanatory data, which uses either classification also resulting in a categorical output, or regression when a numerical output is required Data errors are divided into errors of accuracy and precision. If data is inaccurate, it strays from the true value. If data is imprecise, individual data points are variable under the same conditions. Systematic errors and inaccurate data is much harder to correct. Errors can arise throughout the data science workflow. Inherent error refers to the error present in the source documents and data. Operational error is created after data collection. Data can be cleaned from errors by looking for: Non-uniform data where the data is not uniformly or normally distributed. Missing data where there is no value for the observed variable, often denoted as NA. Duplicated data where a dataset contains multiple records of identical data. Outliers which are data points that are significantly different to other data points or lie abnormally far from most other data points. Measurement errors known as the discrepancy between the measured value and its true value. Quantifying uncertainty is extremely important, as undetectable errors will persist even after data cleaning. Examples of uncertainty quantification metrics include: Confidence interval (CI) is a range of values that the chosen parameter of the data variable will fall within with a certain probability or degree of certainty. Prediction interval is the estimated interval within which observations will occur based on the previously observed data and with a certain degree of confidence. Error propagation is used for datasets containing different uncertainties to determine the overall uncertainty. For example, if the variables in the dataset come from measurements, these values will have a level of uncertainty due to limitations in the measuring process, such as the precision of the instrument. This uncertainty or errors will add up across the data set and analysis due to the variables being combined within a function. Often this error or uncertainty is expressed in terms of the standard deviation σ. 3.2 Tutorial 3.2.1 Overview In this practical, we will be downloading and getting to know spatial data. There are three types of spatial data: points: Spatial points are data points consist of x and y coordinates representing a specific geographic location. shapefiles: Shapefiles are a format for storing geospatial data in the form of points, lines or polygons. They contain geographic locations and attribute information on the features stored within them. rasters: Rasters are gridded data stored in pixels or cells. Each grid cell represents a section of the geographic area the raster covers and contains a value of the variable of interest. This practical will start with a little introduction on downloading remote sensing data for our tower sites and seeing how this information can be used for analyses. Then we will get into the nitty-gritty aspects of the aforementioned spatial data types. 3.2.2 MODIS remote download We will be starting off this tutorial with an example on using remote sensing data. We will focus on a fluxnet site already presented in the first practical: CH-Lae. The Lägern site is located on a mountain in the Swiss Plateau NW of Zürich and is surrounded by managed mixed deciduous mountain forests. The forest is highly diverse and dominated by beech. In the following figures, you can get an impression of the tower and its surroundings. Figure 3.1: Images of Fluxnet sites and measurement station at the Lägern site. As always, we start by loading tidyverseand our fluxnet site data. library(tidyverse) df_sites &lt;- read_csv(&quot;./data/fluxnet_site_info_reduced.csv&quot;) Since we will only be working with the Lägern site for this section, we can extract its latitude and longitude for further use. lon_lae &lt;- df_sites %&gt;% filter(site == &quot;CH-Lae&quot;) %&gt;% pull(lon) lat_lae &lt;- df_sites %&gt;% filter(site == &quot;CH-Lae&quot;) %&gt;% pull(lat) In this first analysis, we want to complement temporal data measured by the Fluxnet tower with remote sensing data. Remote sensing is the detection or monitoring of physical characteristics of the Earth’s surface by measuring reflected and emitted radiation at a distance. Data is collected in the form of images by special cameras typically from aircrafts or satellites. With this method, a large amount of global data at large spatial scales gets easily accessible and therefore represents a useful method for the monitoring of ecosystems. However, this remotely acquired data requires validation from ground measurements, which can be done for example using the eddy towers from the previous practicals. One of these satellite remote sensing instruments is MODIS (Moderate Resolution Imaging Spectroradiometer). Terra MODIS and Aqua MODIS are viewing the entire Earth’s surface every 1 to 2 days, acquiring data in 36 spectral bands, or groups of wavelengths. We have access to tools that make it more straightforward to download remote sensing data from satellites. It provides Atmosphere, Ocean, Cryosphere and Land data series. The MODISTools package in R provides a programmatic interface to the MODIS Land Products Subsets web services allows for easy downloads of “MODIS” time series (of single pixels or small regions of interest) directly to your R workspace or your computer. Using the mt_...() functions of the MODISTools, we can check which products are available in MODIS, how the product we are searching for is called, and what temporal and spatial resolutions are available. t_products() lists all available products from MODIS with their temporal and spatial resolution. Products are parent categories of variables measured by the satellites, such as vegetation indices. mt_bands() requires the entry of a product chosen from “products” and list the available data variables that are represented by the different actually measured wavelengths. Finally, mt_dates() lists all dates of which data from the chosen product and band are available. We will make use of this by loading NDVI data around our towers directly from MODIS. library(MODISTools) products &lt;- mt_products() %&gt;% as_tibble() bands &lt;- mt_bands(product = &quot;MOD13Q1&quot;) %&gt;% as_tibble() dates &lt;- mt_dates(product = &quot;MOD13Q1&quot;, lat = lat_lae, lon = lon_lae) %&gt;% as_tibble() Next, we will get the NDVI data for 6 years for a square of 2km x 2km around the tower site. We can use the function mt_subset() for this. The parameters of this function download a subset of data within the chosen product (here ‘MOD13Q1’). We specify the location as latitude and longitude, and the band we chose, which is at 250m spatial resolution and 16-day temporal resolution. The time period can be chosen as a subset of “dates” and must be provided in the form of start and end date. We are looking at the beginning of 2009 to the end of 2014. km_lr and km_ab define the kilometers to the left and right of the location and kilometers above and below, respectively. Since these values are being rounded to the nearest integer, we choose this minimum of 1, corresponding to 2 km^2. Sitename is used in writing data to file, internal gives the command whether the data should be returned as an internal structure. The progress setting defines whether the download progress should be shown or not. df_ndvi &lt;- mt_subset(product = &quot;MOD13Q1&quot;, # the chosen product lat = lat_lae, # desired lat/lon lon = lon_lae, band = &quot;250m_16_days_NDVI&quot;, # chosen band defining spatial and temporal scale start = &quot;2009-01-01&quot;, # start date: 1st Jan 2009 end = &quot;2014-12-19&quot;, # end date: 19th Dec 2014 km_lr = 1, # kilometers left &amp; right of the chosen location (lat/lon above) km_ab = 1, # kilometers above and below the location site_name = &quot;CH-Lae&quot;, # the site name we want to give the data internal = TRUE, progress = FALSE ) %&gt;% as_tibble() head(df_ndvi) %&gt;% select(2:7) # Only displaying 6 of 21 features ## # A tibble: 6 × 6 ## yllcorner cellsize nrows ncols band units ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5278290.12 231.656358264 9 9 250m_16_days_NDVI NDVI ratio - No units ## 2 5278290.12 231.656358264 9 9 250m_16_days_NDVI NDVI ratio - No units ## 3 5278290.12 231.656358264 9 9 250m_16_days_NDVI NDVI ratio - No units ## 4 5278290.12 231.656358264 9 9 250m_16_days_NDVI NDVI ratio - No units ## 5 5278290.12 231.656358264 9 9 250m_16_days_NDVI NDVI ratio - No units ## 6 5278290.12 231.656358264 9 9 250m_16_days_NDVI NDVI ratio - No units This data frame appears complicated, but we can transform the data into a useful spatial format. For that it needs to be converted to a raster. This is a key spatial data type and will be described in more detail in the section Raster below. The MODISTools packages provides a handy function, called mt_to_raster(), for converting such data frames produced by mt_subset() into a raster. By plotting the data we can verify that we loaded a square of NDVI data around our tower for every 16-day time step. We plot four images in the winter season. plot(raster_ndvi[[1:4]], zlim=c(-0.1, 0.95)) We can derive a variety of statisctics and for instance calculate the minimum and maximum of these rasters. min_winter &lt;- min(minValue(raster_ndvi[[1:4]])) max_winter &lt;- max(maxValue(raster_ndvi[[1:4]])) cat(&quot; Minimum Winter: &quot;, min_winter, &quot;\\n&quot;, &quot;Maximum Winter: &quot;, max_winter) ## Minimum Winter: -0.06791774 ## Maximum Winter: 0.6837171 The same can be done for the summer season. plot(raster_ndvi[[13:16]], zlim=c(-0.1, 0.95)) We again compute minimum and maximum. min_summer &lt;- min(minValue(raster_ndvi[[13:16]])) max_summer &lt;- max(maxValue(raster_ndvi[[13:16]])) cat(&quot; Minimum Summer: &quot;, min_summer, &quot;\\n&quot;, &quot;Maximum Summer: &quot;, max_summer) ## Minimum Summer: 0.569611 ## Maximum Summer: 0.9034679 From the two plots, we can clearly distinguish summer and winter time steps due to the differences in NDVI. Summer NDVI is higher since our tower site is located in the Northern hemisphere where leaves get lost during winter for many vegetation types. The range in winter is also larger which might be due to conifers that do not lose their leaves during this season. We are also interested in the mean across all (spatial) pixels for each date. For this we can collapse the pixel dimension into a single mean value for each date. This is done using group_by() in combination with summarise(), which were explained in Chapter 2. library(lubridate) # lubridate is loaded to work with dates ## first determine the scaling factor that is to be applied to the NDVI values. This has practical reasons: reduces disk space scale_factor &lt;- bands %&gt;% filter(band == &quot;250m_16_days_NDVI&quot;) %&gt;% pull(scale_factor) %&gt;% as.numeric() df_ndvi_spatialmean &lt;- df_ndvi %&gt;% mutate(calendar_date = ymd(calendar_date)) %&gt;% # make the dates into comprehensible values not X.yyyy.mm.dd group_by(calendar_date) %&gt;% # group the data by day summarise(mean = mean(value), min = min(value), max = max(value)) %&gt;% # calculate mean, min and max across pixels mutate(mean = mean * scale_factor, min = min * scale_factor, max = max * scale_factor) # apply scale_factor (see ) We can now plot our NDVI time series across our years (2009 to 2014) using the mean NDVI values from the rasters calculated. df_ndvi_spatialmean %&gt;% ggplot(aes(x = calendar_date)) + geom_ribbon(aes(ymin = min, ymax = max), fill = &quot;grey70&quot;) + geom_line(aes(y = mean)) After having aggregated the data to the mean NDVI across all 81 pixels for each date, we have reduced the dimensions of the data frame to one unique dimension. It now has a structure that can easily be combined with the time series data from the eddy covariance tower (each site is a point in space). # this can now be combined to the data frame with flux data that also has dates along rows ddf_ch_lae &lt;- read_csv(&quot;./data/ddf_ch_lae.csv&quot;) # now we can combine the two data frames along dates. ddf_ch_lae_ndvi &lt;- ddf_ch_lae %&gt;% dplyr::rename(date = TIMESTAMP) %&gt;% dplyr::mutate(year = year(date)) %&gt;% dplyr::filter(year %in% 2009:2014) %&gt;% # NDVI data was downloaded only for these years left_join(df_ndvi_spatialmean %&gt;% rename(date = calendar_date), by = &quot;date&quot;) %&gt;% dplyr::select(-year) NDVI data is provided every 16 days, but the eddy towers provide continuous measurements. In order to predict values at other time steps, we can fit a cubic smoothing spline to our daily values using smooth.spline(). In a nutshell, this is a way to fit a curve between two data points using cubic functions. Cubic smoothing splines embody a curve fitting technique that blends the ideas of cubic splines and curvature minimization to create an effective data modeling tool for noisy data. Traditional interpolating cubic splines represent the tabulated data as a piece-wise continuous curve which passes through each value in the data table. The curve spanning each data interval is represented by a cubic polynomial, with the requirement that the endpoints of adjacent cubic polynomials match in location and in their first and second derivatives. For more, have a look at this blogpost. ddf_ch_lae_ndvi &lt;- ddf_ch_lae_ndvi %&gt;% mutate(date_dec = decimal_date(date)) # adds a column with the date converted into a decimal of its year df_nona &lt;- ddf_ch_lae_ndvi %&gt;% drop_na() # removes any rows containing NAs out_spline &lt;- smooth.spline(df_nona$date_dec, df_nona$mean, spar=0.1 ) # fits a cubic smoothing spline to df vals_spline &lt;- predict(out_spline, ddf_ch_lae_ndvi$date_dec )$y # applies the cubic smoothing spline to our data ddf_ch_lae_ndvi &lt;- ddf_ch_lae_ndvi %&gt;% mutate(ndvi_splined = vals_spline) # adds the spline values for our data to ddf_ch_lae_ndvi Now, we can plot this relationship. ddf_ch_lae_ndvi %&gt;% ggplot() + geom_point(aes(x = date, y = mean)) + geom_line(aes(x = date, y = ndvi_splined), color = &quot;red&quot;) Note how the red line is smoothed and closely follows the black data points. To see how well this data correlates with measurements from the fluxnet tower, we look at the relationship of GPP (gross primary productivity) vs. NDVI (Normalized Difference Vegetation Index) * PPFD (photosynthetic photon flux density). ddf_ch_lae_ndvi &lt;- ddf_ch_lae_ndvi %&gt;% mutate(ppfd_abs = ndvi_splined * PPFD_IN) In order to investigate whether NDVI is associated with GPP and PPFD, we can fit a linear model. linmod1 &lt;- lm(GPP_NT_VUT_REF ~ PPFD_IN, data = ddf_ch_lae_ndvi) summary(linmod1) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ PPFD_IN, data = ddf_ch_lae_ndvi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.3749 -2.0413 -0.0137 1.9913 14.8576 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.9348433 0.1097112 8.521 &lt;2e-16 *** ## PPFD_IN 0.0148270 0.0003038 48.797 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.06 on 2173 degrees of freedom ## (16 observations deleted due to missingness) ## Multiple R-squared: 0.5229, Adjusted R-squared: 0.5226 ## F-statistic: 2381 on 1 and 2173 DF, p-value: &lt; 2.2e-16 The summary contains the following information: Call: this is the formula R uses to fit the data. Residuals: residuals are essentially the difference between the actual observed response values and the response values that the model predicts. Coefficients: the coefficients are the two unknown constants that represent the intercept and slope terms in the linear model. Besides the estimates of the two unknown constants (-5.078 and 15.412), the standard error of the estimates and measures of statistical significance (t value and Pr(&gt;|t|)) are given. The coefficient standard error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. The coefficient t-value is a measure of how many standard deviations our coefficient estimate is away from zero. The larger the absolute value of t, the smaller gets the probability of observing any value larger than |t| by chance, which is expressed by Pr(&gt;|t|). The signif. codes or p-values (stars behind the coefficients) are associated to each estimate and categorize the probabilities Pr(&gt;|t|) by significance thresholds. Residual standard error: the Residual Standard Error is the average amount that the response will deviate from the true regression line. It is a measure of the quality of a linear regression fit. Multiple R-squared : the R^2 quantifies what proportion of the variance found in the response variable can be explained by the predictor variable. It is a measure of how well the model is fitting the actual data. F-statistic: F-statistic is a good indicator of whether there is a relationship between our predictor and the response variables. The further the F-statistics from 1 the better it is. However it depends on the number of data points and the number of predictors how much larger the F-statistics has to be in order to ascertain a relationship between the predictor and the response variables. linmod2 &lt;- lm(GPP_NT_VUT_REF ~ ppfd_abs, data = ddf_ch_lae_ndvi) summary(linmod2) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ ppfd_abs, data = ddf_ch_lae_ndvi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.9202 -1.8307 -0.0829 1.7236 14.4273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.3395678 0.0903180 14.83 &lt;2e-16 *** ## ppfd_abs 0.0180868 0.0003148 57.45 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.792 on 2173 degrees of freedom ## (16 observations deleted due to missingness) ## Multiple R-squared: 0.603, Adjusted R-squared: 0.6028 ## F-statistic: 3301 on 1 and 2173 DF, p-value: &lt; 2.2e-16 R2 increased from 0.5229 to 0.603 when including considering NDVI * PPFD, instead of just PPFD. Hence, it also matters whether leaves are actually out and green for modelling the relationship between GPP and PPFD. NDVI * PPFD approximates the amount of absorbed incoming light, not just the incoming light, and is therefore a physiologically more relevant quantity. 3.2.3 Points on the globe Documenting the geographic coordinates of data allows us to assign a precise location to that data observation, which can be important for further analysis when, for example, comparing location. Coordinates come with a latitude (defining the position N or S of the equator) and longitude (defining the position E or W of the meridian). With the coordinates, we can plot data points, remembering that the latitude is equivalent to the y-axis and longitude to the x-axis. In this section, we look at the position of the Fluxnet sites. The Fluxnet network measures land-atmosphere exchanges of greenhouse gases and energy for sites across the globe using the eddy covariance technique. High-frequency measurements of vertical wind velocity and a scalar mixing ratio (CO2, H2O, temperature, etc.) provides estimates of the net exchange of the scalar. Eddy covariance is currently the standard method to measure fluxes of trace gases between ecosystems and the atmosphere (sources: Fluxnet, Nature Article by Pastorello et al. (2020) on Fluxnet Dataset. At the beginning of this tutorial we loaded the dataset containing information on sites in Europe. We treat site locations as a geographical position. So we will extract the name, longitude and latitude of our sites. head(df_sites) ## # A tibble: 6 × 3 ## site lon lat ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BE-Bra 4.52 51.3 ## 2 BE-Lon 4.75 50.6 ## 3 BE-Vie 6.00 50.3 ## 4 CH-Cha 8.41 47.2 ## 5 CH-Fru 8.54 47.1 ## 6 CH-Lae 8.36 47.5 Our dataset ‘df_sites’ contains two sites in Greenland and Siberia. We’ll focus on a smaller spatial domain, restricted to “mainland” Europe. Therefore, we exclude sites RU-Cok and DK-ZaH. We will also remove DE-Obe since this was removed in previous chapters. df_sites &lt;- df_sites %&gt;% filter(!(site %in% c(&quot;RU-Cok&quot;, &quot;DK-ZaH&quot;, &quot;DE-Obe&quot;))) Now we can plot our data points without any further adjustments. ggplot() + geom_point(data = df_sites, aes(x = lon, y = lat), color = &quot;red&quot;) + labs(title = &quot;Selected FLUXNET sites&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;)+ theme_bw() We can make an educated guess where the points are but this plot does not tell us much by itself without a map as a reference. To visualize the location of our points we plot them on a map. We use the packages sf (Pebesma 2018) and SpData (R-SpData?) to get the world map and crop it to Europe. library(sf) library(spData) ggplot() + geom_sf(data = world) + geom_point(data = df_sites, aes(x = lon, y = lat), color = &quot;red&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + coord_sf(xlim = c(-30,40), ylim = c(35,80), expand = FALSE) + theme_bw() We can carry on using the data points as they are (for plotting or analysis) but for more detailed spatial analysis we will transform them into a SpatialPoints format. SpatialPoints consist of a matrix with n rows and 2 columns, one for each coordinate (latitude &amp; longitude). n is the number of points in the data. These points also have a so-called ‘projection string’ or ‘crs’ indicating the coordinate reference system in which coordinates of points are expressed. There are different ways (formulas) to project the earth (an ellipsoid) onto a 2-dimensional map. We can only perform calculations, if the same method (projection, coordinate reference system (crs)) is used. Having points in different coordinate reference systems can cause data to look skewed. It is essential to know which coordinate reference system your data is in. The most commonly used one is WGS84 (EPSG: 4326). There are some useful resources to convert help you find the correct spatial reference. To transform points into SpatialPoints, we use the function SpatialPoints in the R package sp (R. S. Bivand, Pebesma, and Gómez-Rubio 2013; Pebesma and Bivand 2005). library(sp) sp_sites &lt;- SpatialPoints(dplyr::select(df_sites, -site)) # remove character column Below you can see how the R-base call plot(), plots the SpatialPoints directly. plot(sp_sites, col = &quot;red&quot;, pch = 16) If, however, we want to make the point’s locations visually meaningful, we have to provide a map to plot as a background. To load a raster file in R, we will use the package rgdal (R. Bivand, Keitt, and Rowlingson 2021). The shapefile of Europe we will load, will also be used in the next section called Shapefiles. library(rgdal) europe_shape &lt;- readOGR(dsn=&quot;./data/shapefiles&quot;, layer=&quot;europe_map&quot;) # imports the shapefile of the European map ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/pascalschneider/Polybox/Shared/Data Science Lecture Planning - shared folder/4 Datasets/shapefiles&quot;, layer: &quot;europe_map&quot; ## with 53 features ## It has 94 fields plot(europe_shape, col=&quot;grey&quot;) # plots the European map as the background plot(sp_sites, col = &quot;red&quot;, add = TRUE, pch = 16) # plots the points (here tower locations) on to the map Now, we can see the tower locations as red circles plotted on a grey European map with country boarders, which puts the data points in a meaningful and comprehendable visual context. 3.2.4 Shapefiles Above we looked at points on the globe and transformed our data to SpatialPoints. This was already the first example of what forms a shapefile can come in. Shapefiles store geographic information, including location and any additional information, in shape objects. Shape objects in R are defined by SpatialPoints, SpatialLines, and SpatialPolygons classes of the sp package . The corresponding SpatialPointsDataFrame, SpatialLinesDataFrame, and SpatialPolygonsDataFrame classes allow storing shape objects together with a data frame. The number of rows in the data frame corresponds to the number of points (number of rows in coordinate matrix), lines (size of the list of Lines), or polygons (size of the list of Polygons). This allows us to associate a vector of variables (a row of the data frame) to each shape object. In the next step, we load a shapefile containing the countries of the world provided by Natural Earth, as a reference with which we can contextualize our spatial data. Since we only are looking at towers in Europe, we have cropped the raster to Europe for you. library(rgdal) europe_shape &lt;- readOGR(dsn=&quot;./data/shapefiles&quot;, layer=&quot;europe_map&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/pascalschneider/Polybox/Shared/Data Science Lecture Planning - shared folder/4 Datasets/shapefiles&quot;, layer: &quot;europe_map&quot; ## with 53 features ## It has 94 fields We can look at what class europe_shape is: class(europe_shape) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; Above we mentioned that SpatialPolygonsDataFrame contains both a list of SpatialPolygons and a with information on each polygon. Here is how we can access the information in the . We will start by displaying the header of some of the 94 features for now. europe_shape@data %&gt;% select(c(NAME, SOVEREIGNT, SUBREGION, POP_EST))%&gt;% head() ## NAME SOVEREIGNT SUBREGION POP_EST ## 0 Vatican Vatican Southern Europe 1000 ## 1 Jersey United Kingdom Northern Europe 98840 ## 2 Guernsey United Kingdom Northern Europe 66502 ## 3 Isle of Man United Kingdom Northern Europe 88815 ## 4 United Kingdom United Kingdom Northern Europe 64769452 ## 5 Ukraine Ukraine Eastern Europe 44033874 Now, we want to plot our towers on this map of Europe. First, we need to extract the projection from europe_shape using proj4string() and add it to our SpatialPoints. Thereby ensuring they are in the same reference system to plot them together. geo.proj &lt;- sp::proj4string(europe_shape) pts &lt;- sp::SpatialPoints(sp_sites, proj4string = sp::CRS(geo.proj)) By using the SpatialPoints, now in the correct coordinate reference system, and the data from europe_shape, we can bind them to our tower sites in the data frame df_sites. The function overfrom the package sp, retrieves the indexes or attributes from the specified spatial object (europe shape) at the spatial locations of a specified object (pts). df_sites_country_info &lt;- sp::over(pts, europe_shape) %&gt;% # retrieves info from &#39;europe_shape&#39; at the &#39;pts&#39; locations as_tibble() %&gt;% # makes a tibble bind_cols(df_sites, .) # bind the information gathered to the df_sites head(df_sites_country_info) %&gt;% select(1:6) ## # A tibble: 6 × 6 ## site lon lat featurecla scalerank LABELRANK ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 BE-Bra 4.52 51.3 Admin-0 country 1 2 ## 2 BE-Lon 4.75 50.6 Admin-0 country 1 2 ## 3 BE-Vie 6.00 50.3 Admin-0 country 1 2 ## 4 CH-Cha 8.41 47.2 Admin-0 country 1 4 ## 5 CH-Fru 8.54 47.1 Admin-0 country 1 4 ## 6 CH-Lae 8.36 47.5 Admin-0 country 1 4 In the header above, is the new data frame df_sites_country_info consisting of three columns (‘site’, ‘lat’, ‘lon’) from df_sites and then 91 columns containing information from europe_shape at the spatial locations of pts (the tower sites). For the next part, we will be using a European map with country boarders. For this, we will make an object called shp_df using the column SOVEREIGNT, which we saw when looking at europe_shape@data. The package broom (Robinson, Hayes, and Couch 2021) converts objects from R into ‘tidy tibbles’, this will allow us to preserves only the data needed for further analysis and plotting. shp_df &lt;- broom::tidy(europe_shape, region = &quot;SOVEREIGNT&quot;) # transforms the information in &#39;SOVREIGNT&#39; into a tidy df head(shp_df) ## # A tibble: 6 × 7 ## long lat order hole piece group id ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; ## 1 19.3 41.9 1 FALSE 1 Albania.1 Albania ## 2 19.3 41.9 2 FALSE 1 Albania.1 Albania ## 3 19.4 42.0 3 FALSE 1 Albania.1 Albania ## 4 19.4 42.0 4 FALSE 1 Albania.1 Albania ## 5 19.4 42.1 5 FALSE 1 Albania.1 Albania ## 6 19.3 42.1 6 FALSE 1 Albania.1 Albania The data frame above is the tidied version of the information in the SOVEREIGNT column of europe_shape. It contains the coordinates, country (sovereignty) and order of each of the polygons from the SpatialPolygonsDataFrame. Plotting the information in this data frame looks as follows: ggplot() + geom_polygon(data = shp_df, aes(x = long, y = lat, group = group, fill = id), colour = &quot;black&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) As we saw in the earlier plot of the tower locations only some countries in Europe contain towers. Therefore, we want to crop our map to only include those countries. We first create a vector which is lists the country each towers is located in. We obtain this vector from the data frame df_site_country_info. Then, we filter out only those countries with a tower in them. We can plot the countries and tower locations as a result. # character vector of the country each tower is located in from column &#39;SOVREIGNT&#39; countries_with_site &lt;- df_sites_country_info %&gt;% pull(SOVEREIGNT) %&gt;% as.character() # resulting character vector (without NAs) countries_with_site %&gt;% na.omit() ## [1] &quot;Belgium&quot; &quot;Belgium&quot; &quot;Belgium&quot; &quot;Switzerland&quot; &quot;Switzerland&quot; ## [6] &quot;Switzerland&quot; &quot;Switzerland&quot; &quot;Switzerland&quot; &quot;Switzerland&quot; &quot;Germany&quot; ## [11] &quot;Germany&quot; &quot;Germany&quot; &quot;Germany&quot; &quot;Denmark&quot; &quot;Finland&quot; ## [16] &quot;Finland&quot; &quot;France&quot; &quot;France&quot; &quot;France&quot; &quot;France&quot; ## [21] &quot;Italy&quot; &quot;Italy&quot; &quot;Italy&quot; &quot;Italy&quot; &quot;Italy&quot; ## [26] &quot;Italy&quot; &quot;Italy&quot; &quot;Italy&quot; &quot;Italy&quot; &quot;Netherlands&quot; ## [31] &quot;Czechia&quot; ## attr(,&quot;na.action&quot;) ## [1] 32 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; # country with towers filtered shp_df_sub &lt;- shp_df %&gt;% filter(id %in% countries_with_site) # plot result ggplot() + geom_polygon(data = shp_df_sub, aes(x = long, y = lat, group = group, fill = id), colour = &quot;black&quot;) + geom_point(data = df_sites, aes(x = lon, y = lat)) + theme_bw() Checkpoint You just learnt how to plot europe_shape coloured by country (“SOVREIGNT”), now plot it coloured by region in Europe. Start by finding the column that gives you this information in europe_shape@data. Solution # take a look at the first 6 column names names(europe_shape@data)[1:10] ## [1] &quot;featurecla&quot; &quot;scalerank&quot; &quot;LABELRANK&quot; &quot;SOVEREIGNT&quot; &quot;SOV_A3&quot; ## [6] &quot;ADM0_DIF&quot; &quot;LEVEL&quot; &quot;TYPE&quot; &quot;ADMIN&quot; &quot;ADM0_A3&quot; # make a df of the data needed from europe_shape shp_df_region &lt;- broom::tidy(europe_shape, region = &quot;SUBREGION&quot;) head(shp_df_region) ## # A tibble: 6 × 7 ## long lat order hole piece group id ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; ## 1 28 42.0 1 FALSE 1 Eastern Europe.1 Eastern Europe ## 2 27.9 42.0 2 FALSE 1 Eastern Europe.1 Eastern Europe ## 3 27.8 42.0 3 FALSE 1 Eastern Europe.1 Eastern Europe ## 4 27.8 42.0 4 FALSE 1 Eastern Europe.1 Eastern Europe ## 5 27.7 42.0 5 FALSE 1 Eastern Europe.1 Eastern Europe ## 6 27.7 42.0 6 FALSE 1 Eastern Europe.1 Eastern Europe # plot europe coloured by region ggplot() + geom_polygon(data = shp_df_region, aes(x = long, y = lat, group = group, fill = id), colour = &quot;black&quot;) + geom_point(data = df_sites, aes(x = lon, y = lat)) + theme_bw() We now have a map showing the tower’s locations as black circles within the different regions in Europe. 3.2.5 Rasters In this section we will learn how to use rasters. A raster is a matrix or grid of cells each of which contains information in the form of a value. A raster object consists primarily of: A grid of cells; A coordinate reference system (CRS) for the grid and its cells so that we know the location to which the grid refers; A variable of interest for which each cell in the grid has a value, and; Other information relating to the CRS, projection, resolution, etc. We will load and check some rasters and get familiar with functions to analyse them. Since our goal ultimate goal is to predict productivity, we will consider different factors that might explain productivity, for example, landcover and temperature. To start off we’ll look at some landcover data. With this data, we can investigate the surroundings of the Fluxnet towers. This data gives us the different classes of physical coverage of the Earth’s surface, such as forests, grasslands, croplands, lakes, etc. Landcover types affect fluxes measured by the tower through the vegetation that characterizes them, or by influencing the radiation budget through albedo, or water availability. Although we have some information from the overview file of the towers, we can still check whether we can confirm this information. We will load landcover data from GlobCover provided by the European space agency (ESA). This project provides landcover maps observations of surface reflectance from the 300m MERIS sensor on board the ENVISAT satellite mission as their input. # library(raster) -&gt; we have already done this but remember you need this to load rasters! raster_landcover &lt;- raster(&quot;./data/Globcover_EU.tif&quot;) This landcover data consists of different categories, these GlobCover categories are based on the Land Cover Classification System (LCCS) which was developed by the Food and Agricultural Organization of the United Nations (FAO) to provide a consistent framework for the classification of mapping land cover. They include various types of forest, shrub- and grasslands, cropland and artificial surfaces. Before we look at the data, we first reduce the spatial extent that the raster covers. We crop it to a rectangle around the site location of CH-Lae (+/- 2 degrees in longitudinal direction, +/- 1 degree in latitudinal direction). bounding_box &lt;- extent(lon_lae-2, lon_lae+2, lat_lae-1, lat_lae+1) raster_landcover_crop &lt;- crop(raster_landcover, bounding_box) We can plot our raster as the region around our tower. To indicate the position of our tower, we add a red dot. plot(raster_landcover_crop, legend=FALSE, xlab=&quot;longitude&quot;, ylab=&quot;latitude&quot;) points(lon_lae, lat_lae, pch = 16, col = &quot;red&quot;) If we want to conduct analysis using two different rasters, we have to make sure they have the same resolution and are aligned on the same grid. We can read a second raster and compare the grids. We load spatio-temporal temperature rasters available from CHELSA to complement our flux tower measurements for further analysis. The temperature raster data from CHELSA provides free high-resolution climate data (temperature and precipitation) for the past and the future. The past data we use here are downscaled model output temperature and precipitation estimates of the ERA-Interim climatic reanalysis (forecast models and data assimilation systems reanalysing archived observations). While the temperature algorithm is based on statistical downscaling of atmospheric temperatures, the precipitation algorithm incorporates orographic predictors including wind fields, valley exposition, and boundary layer height, with subsequent bias correction. The data we use consists of a monthly temperature and precipitation time series over the area of Europe from 2006 to 2012 in January. To compare this new raster with the previous one we need to crop it to the same region as the tower. Cropping rasters can be done using the R package raster (Hijmans 2021). library(raster) raster_chelsa &lt;- raster(&quot;./data/Chelsa_t_mean_2006-2012.tif&quot;) raster_chelsa_crop &lt;- crop(raster_chelsa, bounding_box) We now plot our raster. pal &lt;- colorRampPalette(c(&quot;purple&quot;,&quot;blue&quot;,&quot;cyan&quot;,&quot;green&quot;,&quot;yellow&quot;,&quot;red&quot;)) plot(raster_chelsa_crop, col = pal(20), xlab=&quot;longitude&quot;, ylab=&quot;latitude&quot;) points(lon_lae, lat_lae, pch = 16, col = &quot;red&quot;) We see that the temperature units are not in a format that is straightforward to read. To change the unit from Kelvin x10 to degree Celsius, we have to divide the data by 10 and subtract 273.15. raster_chelsa_crop &lt;- raster_chelsa_crop/10-273.15 We plot our raster again, with this temperature transformation. pal &lt;- colorRampPalette(c(&quot;purple&quot;,&quot;blue&quot;,&quot;cyan&quot;,&quot;green&quot;,&quot;yellow&quot;,&quot;red&quot;)) plot(raster_chelsa_crop, col = pal(20), xlab=&quot;longitude&quot;, ylab=&quot;latitude&quot;) points(lon_lae, lat_lae, pch = 16, col = &quot;red&quot;) We have seen some of the basic functions in the package raster. Now we will load one more package to work with raster files. We use the package rasterVis (Perpiñán and Hijmans 2021). This package contains methods for enhanced visualization and interaction with raster data. It implements visualization methods for quantitative data and categorical data, both for univariate and multivariate rasters. It also provides methods to display spatiotemporal rasters, and vector fields. To enhance the visualisation of our raster data, we will use the colour palettes from the package RColorBrewer (Neuwirth 2014) library(rasterVis) library(RColorBrewer) mapTheme &lt;- rasterTheme(region = rev(brewer.pal(8,&quot;RdYlBu&quot;))) plt &lt;- levelplot(raster_chelsa_crop, margin=FALSE, par.settings = mapTheme) plt Checkpoint Create your own small raster with 100 grid cells. Give it values, a projection (e.g. ‘+proj=utm +zone=48 +datum=WGS84’) and plot it. If you chose the example projection provided, add europe_shape to your plot. Solution # create a raster # short compact version myraster &lt;- raster(ncol=10, nrow=10) # OR # longer version myraster &lt;- raster() ncol(myraster) &lt;- 10 nrow(myraster) &lt;- 10 # add values to the raster, this can be done in many ways here are two examples: values(myraster) &lt;- 1:ncell(myraster) # OR values(myraster) &lt;- runif(ncell(myraster)) # add a projection projection(myraster) &lt;- &quot;+proj=utm +zone=48 +datum=WGS84&quot; # plot it plot(myraster) # Since the raster has a projection we can visualise Europe on this new raster. plot(europe_shape, add=TRUE) 3.2.5.1 Aggregating A raster grid is uniquely defined by an origin, a point that one of the intersections of grid lines, and its resolution, the magnitude of each cell-side. In the R package raster, the origin is defined as the point closest to (0,0) that is still an intersection of grid lines. The functions origin() and res() from the R package raster return the origin and resolution of a raster object. As an example, we will see what the resolution and origin of the rasters we loaded before are. cat(&quot; Resolution Raster Landover: &quot;, res(raster_landcover), &quot;\\n&quot;, &quot;Resolution Raster Chelsa: &quot;, res(raster_chelsa)) ## Resolution Raster Landover: 0.002777778 0.002777778 ## Resolution Raster Chelsa: 0.008333333 0.008333333 cat(&quot; Origin Raster Landover: &quot;, origin(raster_landcover), &quot;\\n&quot;, &quot;Origin Raster Chelsa: &quot;, origin(raster_chelsa)) ## Origin Raster Landover: 0.001388889 -0.001388889 ## Origin Raster Chelsa: -0.000139609 -0.000139249 Landcover has a much higher resolution. We can re-grid the Landcover raster to match the grid of the Chelsa raster. Aggregating means resampling an input raster to a coarser resolution based on a specified aggregation strategy. We now determine the aggregation factor by dividing the resolution of the landcover raster by that of the Chelsa raster. We do it both for the longitude and latitude to make sure they match. res_landcover &lt;- res(raster_landcover) res_chelsa &lt;- res(raster_chelsa) factor_agg_lon &lt;- res_chelsa[1] / res_landcover[1] factor_agg_lat &lt;- res_chelsa[2] / res_landcover[2] cat(&quot; Longitudinal Aggregation Factor: &quot;, factor_agg_lon, &quot;\\n&quot;, &quot;Latitudinal Aggregation Factor: &quot;, factor_agg_lon) ## Longitudinal Aggregation Factor: 3 ## Latitudinal Aggregation Factor: 3 We observed that the landcover raster has a higher resolution than the temperature raster, we have deduced it is 3 times higher. So we need to transform the landcover raster to make sure it has the same grid (same origin and resolution) as the temperature raster. This will allow us to combine them in a potential analysis. We achieve this using aggregate() from the R package raster to obtain a sum of edges raster at 1000m x 1000m resolution. The aggregate function takes three main arguments. * x : the raster object to aggregate, * fact : aggregation factor, i.e. the number of current cells that will make up the side of one of the new cells, and, * fun : the function to apply to summarize the raster values of the fact^2 (in this case 3ˆ2=9) cells. We aggregate the landcover raster at 1km using the aggregation factor calculated above. raster_landcover_crop_agg &lt;- aggregate(raster_landcover_crop, fact = factor_agg_lon, fun = modal) We check the resolutions and origin of the two rasters and plot our the new landcover one. cat(&quot; Resolution Landcover: &quot;, res(raster_landcover_crop_agg), &quot;\\n&quot;, &quot;Resolution Chelsa: &quot;, res(raster_chelsa), &quot;\\n&quot;, &quot;Origin Landcover: &quot;, origin(raster_landcover_crop_agg), &quot;\\n&quot;, &quot;Origin Chelsa: &quot;, origin(raster_chelsa), &quot;\\n&quot;) ## Resolution Landcover: 0.008333333 0.008333333 ## Resolution Chelsa: 0.008333333 0.008333333 ## Origin Landcover: -0.001388889 -0.004166667 ## Origin Chelsa: -0.000139609 -0.000139249 plot(raster_landcover_crop_agg, legend=FALSE, xlab=&quot;longitude&quot;, ylab=&quot;latitude&quot;) We can see that the resolutions are now the same but the origin still differs. To get matching origins we must do another step: align the rasters. 3.2.5.2 Aligning Aligning rasters is done using the function resample(). There are two main resampling methods: Nearest Neighbour or Bilinear Interpolation, which method is used depends upon the input data and its use after the operation is performed. Nearest Neighbour is best used for categorical data like land-use classification or slope classification. The values that go into the grid stay exactly the same, a 2 comes out as a 2, and 99 comes out as 99. The value of the output cell is determined by the nearest cell center on the input grid. Nearest Neighbor can be used on continuous data but the results can be blocky. Bilinear Interpolation uses a weighted average of the four nearest cell centers. The closer an input cell center is to the output cell center, the higher the influence of its value is on the output cell value. This means that the output value could be different than the nearest input, but is always within the same range of values as the input. Since the values can change, Bilinear is not recommended for categorical data. Instead, it should be used for continuous data like elevation and raw slope values. raster_landcover_crop_resampl &lt;- resample(raster_landcover_crop, raster_chelsa_crop, method=&quot;ngb&quot;) plot(raster_landcover_crop_resampl, legend=FALSE, xlab=&quot;longitude&quot;, ylab=&quot;latitude&quot;) Now, we can check whether the two rasters are aligned. cat(&quot; Resolution Landcover: &quot;, res(raster_landcover_crop_resampl), &quot;\\n&quot;, &quot;Resolution Chelsa: &quot;, res(raster_chelsa), &quot;\\n&quot;, &quot;Origin Landcover: &quot;, origin(raster_landcover_crop_resampl), &quot;\\n&quot;, &quot;Origin Chelsa: &quot;, origin(raster_chelsa), &quot;\\n&quot;) ## Resolution Landcover: 0.008333333 0.008333333 ## Resolution Chelsa: 0.008333333 0.008333333 ## Origin Landcover: -0.000139609 -0.000139249 ## Origin Chelsa: -0.000139609 -0.000139249 As we can see in the output of the code above, both the resolution and origin are identical. This means our rasters are now aligned. Remember, the resolution is the size of the cells or pixels in a raster and the origin is the coordinates of the point point closest to (0,0) that you could get if you moved towards that point(0,0) in steps of the x and y resolution of the rasters being analysed. 3.2.5.3 Point extraction In this section, we will extract data from our rasters or spatial data. We will be extracting the values for the locations of our towers. The function to extract values is aptly named extract(). First, we specify the raster from which the values should be extracted and then the exact sites at which the values should be extracted. Here, we combine all this into a . df_sites_temp &lt;- read_csv(&quot;./data/fluxnet_site_info_reduced.csv&quot;) df_sites_temp &lt;- extract(raster_chelsa, sp_sites, sp = TRUE) %&gt;% as_tibble() %&gt;% right_join(df_sites_temp, by = c(&quot;lon&quot;, &quot;lat&quot;)) %&gt;% dplyr::rename(temp_chelsa = Chelsa_t_mean_2006.2012) In the lectures, we have seen that we must always be aware of modelled data and be aware of its limitations. Therefore, we do not just want to use CHELSA temperature data. It is important to consider different models and compare them. Hence, we will look at another climate model: WorldClim. The WorldClim data website is a database containing global weather and climate data at a high spatial resolution. The data can be downloaded for historic (“near current”) and future climate and weather conditions. We will be using the historical monthly weather data. Due to the size of the rasters downloaded from WorldClim, we have done this already and made a raster of the mean annual temperature for the years 2006 to 2012. Through this we lose some details and information but for our analysis it suffices. Next, we load and plot the provided rasters showing the mean annual temperatures for the WorldClim data for the years 2006-2012. WorldClim is a set of global climate layers (gridded climate data) with a spatial resolution between 1-340 km^2. We want to extract values from WorldClim at the same tower sites, as we have used previously, and add them to the df_sites data frame. This will give us a tidy with the site, latitude and longitude, temperatures for Chelsa and WorldClim at each tower location. raster_worldclim &lt;- raster(&quot;./data/WC_mean_t_2006-2012.tif&quot;) df_sites_temp &lt;- extract(raster_worldclim, sp_sites, sp = TRUE) %&gt;% as_tibble() %&gt;% right_join(df_sites_temp, by = c(&quot;lon&quot;, &quot;lat&quot;)) %&gt;% dplyr::rename(temp_wc = WC_mean_t_2006.2012) head(df_sites_temp) ## # A tibble: 6 × 5 ## temp_wc lon lat temp_chelsa site ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -0.633 4.52 51.3 2750 BE-Bra ## 2 -1.45 4.75 50.6 2741 BE-Lon ## 3 -3.75 6.00 50.3 2716 BE-Vie ## 4 -4.76 8.41 47.2 2730 CH-Cha ## 5 -4.97 8.54 47.1 2699 CH-Fru ## 6 -5.38 8.36 47.5 2710 CH-Lae We obtain the monthly mean temperature of January 2006 from FLUXNET data. library(lubridate) load(&quot;./data/ddf_allsites_nested_joined.RData&quot;) df_sites_temp &lt;- ddf_allsites_nested_joined %&gt;% dplyr::select(site = siteid, data) %&gt;% unnest(data) %&gt;% dplyr::select(site, TIMESTAMP, TA_F) %&gt;% mutate(month = month(TIMESTAMP), year = year(TIMESTAMP)) %&gt;% filter(month == 1, year == 2006) %&gt;% group_by(site) %&gt;% summarise(temp_fluxnet = mean(TA_F)) %&gt;% # add temp data extracted from spatial files (chelsa and worldclim) left_join(df_sites_temp, dplyr::select(site, temp_wc, temp_chelsa), by = &quot;site&quot;) We compare the climate models by showing the correlations with the tower’s data. The dotted line shows what the linear model would look like if the values of our model and the tower data overlapped. The red line shows the actual linear model of the relationship between the climate model and the tower data. The closer the red line to the dotted line the better the model is at predicting the temperatures measured by the towers. df_sites_temp$temp_chelsa &lt;- df_sites_temp$temp_chelsa/10 -273.15 # we remove the sites we removed before df_sites_temp &lt;- df_sites_temp %&gt;% filter(!(site %in% c(&quot;RU-Cok&quot;, &quot;DK-ZaH&quot;))) df_sites_temp %&gt;% ggplot(aes(x = temp_fluxnet, y = temp_chelsa), xlim=c(-10,5)) + geom_point() + geom_abline(intercept=0, slope=1, linetype=&quot;dotted&quot;) + geom_smooth(method=&#39;lm&#39;, color=&quot;red&quot;, size=0.5, se=FALSE) + theme_classic() We compute the mean squared error (MSE). The MSE calculates the average of the squares of errors of all the data points from a fitted line or model. Put simply this is the difference between the data points (the actual data) and the estimated points given by the line or model. The values are squared to get positive values even if the difference is negative. Smaller values reflect less variation of the data. In a model you want to minimise the MSE, as this reflects that the model’s predicted or estimated points are closer to your actual data. The R package Metrics (Hamner and Frasco 2018), has a function mse() to calculate the mean squared error. library(Metrics) mse(df_sites_temp$temp_fluxnet, df_sites_temp$temp_chelsa) ## [1] 2.314917 df_sites_temp %&gt;% ggplot(aes(x = temp_fluxnet, y = temp_wc), xlim=c(-10,5)) + geom_point() + geom_abline(intercept=0, slope=1, linetype=&quot;dotted&quot;) + geom_smooth(method=&#39;lm&#39;, color=&quot;red&quot;, size=0.5, se=FALSE) + theme_classic() We compute the mean squared error. mse(df_sites_temp$temp_fluxnet, df_sites_temp$temp_wc) ## [1] 10.43716 We see that CHELSA correlates better, since the mean squared error is smaller, with the temperature values at the towers. We also see that WorldClim compared to CHELSA tends to predict colder temperatures. It is important to remember that CHELSA and WorldClim are both ‘modelled data’ and as such come with errors and uncertainties. When using environmental layers for analyses, the data source should be well researched and possible uncertainty or errors carefully considered to understand the weaknesses and limitations of the data. Th main differences between CHELSA and WorldClim are down to the input data of their models. As mentioned earlier CHELSA predicts its variable on down-scaled ERA-Interim data (global atmospheric reanalysis) and implements bias correction analyses. WorldClim’s predicted variables are based on point data from weather station, which is then interpolated based on elevation, longitude and latitude. Due to the dependence on data from weather stations, the accuracy of WorldClim is more dependent on station density. For a more detailed comparison of the two climate data and why such models must be used with care, refer to the Bobrowski, Weidinger, and Schickhoff (2021) paper here. Checkpoint Above, we compared the CHELSA temperatures with the tower temperatures. Now we want to visually see which of the two data had the higher mean for each tower location. To achieve this, attempt to plot the map of Europe and add the tower locations as points. Then colour the points so that for a higher CHESLA mean they are one colour and a higher tower temperature another. Solution # There are more way to do this but we used the function ifelse(). # It takes 3 components: a condition, what to do if the condition applies and what to do if it doesn&#39;t apply. # So here if CHELSA temperatures are higher we want the point to be purple otherwise orange: cols_chelsa_flux &lt;- ifelse(df_sites_temp$temp_chelsa &gt; df_sites_temp$temp_fluxnet,&quot;purple&quot;,&quot;orange&quot;) # We could also do different shapes if we wanted... pch_chelsa_flux &lt;- ifelse(df_sites_temp$temp_chelsa &gt; df_sites_temp$temp_fluxnet, 19 ,17) ## Plot the result using base R: # plot(europe_shape) # points(df_sites_temp$lon,df_sites_temp$lat, pch=pch_diff, col = col_checkpoint) ## Plot the result using ggplot: ggplot() + geom_polygon(data = europe_shape, aes(x = long, y = lat, group = group), fill=NA, colour = &quot;black&quot;) + geom_point(data = df_sites_temp, aes(x = lon, y = lat), color = cols_chelsa_flux, shape = pch_chelsa_flux, size=2) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + theme_classic()+ coord_quickmap() From the resulting map of Europe, we can deduce that for any points (here representing the towers) that are purple (circles) the mean temperature of CHELSA is higher than the measured temperature. Any orange points (triangles) the towers measured a higher temperature than CHELSA predicted. There seem to be more purple circles suggests CHELSA predicts higher temperatures than are measured by the fluxnet towers. 3.2.5.4 Correlations with GPP We already had a look at GPP in Chapter 2. In this section we want to correlate CHELSA temperature and landcover with GPP. In models we often take multiple variables and correlate them to another variable (here GPP). By seeing if they correlate well we can see if any of the variables make good so-called predictors. Below we will explore how well landcover and temperature correlate with GPP. We start off by extracting the mean GPP at the tower sites. To do this we group our data by the siteid (or tower) and the year. This way we can get the GPP across a year and then an average across all years for each site. We will also consider the median to see if this changes the correlation. To compare GPP and landcover, we will use both the landcover from the raster used above and the landcover given in the fluxnet data for each tower site. There is a column in the the fluxnet data set called umd_land_cover, which refers to the University of Maryland landcover classification scheme. GPP_ann &lt;- ddf_allsites_nested_joined %&gt;% unnest(data) %&gt;% mutate(year = year(TIMESTAMP)) %&gt;% group_by(siteid, year) %&gt;% summarise(gpp_ann = sum(GPP_NT_VUT_REF)) %&gt;% ungroup() %&gt;% group_by(siteid) %&gt;% summarise(gpp_meanann = mean(gpp_ann), gpp_medianann = median(gpp_ann)) %&gt;% dplyr::select(siteid, gpp_meanann, gpp_medianann) GPP_ann &lt;- cbind(GPP_ann, &#39;umd_land_cover&#39; = ddf_allsites_nested_joined$umd_land_cover) # for the correlations we remove the sites we don&#39;t have in the other datasets GPP_ann &lt;- GPP_ann %&gt;% filter(!siteid %in% c(&quot;DE-Obe&quot;,&quot;DK-ZaH&quot;, &quot;RU-Cok&quot;)) We then extract landcover data from the GlobCover raster at the tower sites. df_sites_landcover &lt;- extract(raster_landcover, sp_sites, sp = TRUE) df_sites_landcover &lt;- as.tibble(df_sites_landcover) To make the correlations, we create a data frame with all the extracted temperature, landcover and GPP data from the tower sites. landcover &lt;- df_sites_landcover$Globcover_EU chelsa &lt;- df_sites_temp$temp_chelsa df_corr_ann &lt;- cbind(landcover, chelsa, GPP_ann) df_corr_ann &lt;- as.data.frame(df_corr_ann) head(df_corr_ann) ## landcover chelsa siteid gpp_meanann gpp_medianann umd_land_cover ## 1 50 1.85 BE-Bra NA NA Mixed Forests ## 2 20 0.95 BE-Lon 1348.667 1538.198 Croplands ## 3 90 -1.55 BE-Vie 1792.497 1757.801 Mixed Forests ## 4 120 -0.15 CH-Cha 2478.429 2635.465 Grasslands ## 5 140 -6.95 CH-Dav 1166.543 1176.724 Grasslands ## 6 50 -3.25 CH-Fru 2106.319 2015.295 Mixed Forests We will start by correlating the CHELSA temperatures with GPP. We do this by building a simple linear model using the function lm(). We do this for both the mean and the median. # linear model of temperature and mean annual GPP: reg_mean_1 &lt;- lm(gpp_meanann~chelsa, data = df_corr_ann) summary(reg_mean_1) ## ## Call: ## lm(formula = gpp_meanann ~ chelsa, data = df_corr_ann) ## ## Residuals: ## Min 1Q Median 3Q Max ## -854.69 -279.10 29.79 260.51 843.33 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1637.47 77.85 21.03 &lt;2e-16 *** ## chelsa 15.81 15.50 1.02 0.317 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 407.4 on 26 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.0385, Adjusted R-squared: 0.001519 ## F-statistic: 1.041 on 1 and 26 DF, p-value: 0.317 # linear model of temperature and meadian annual GPP: reg_median_1 &lt;- lm(gpp_medianann~chelsa, data = df_corr_ann) summary(reg_median_1) ## ## Call: ## lm(formula = gpp_medianann ~ chelsa, data = df_corr_ann) ## ## Residuals: ## Min 1Q Median 3Q Max ## -835.60 -283.48 36.19 233.05 1008.69 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1629.43 78.63 20.72 &lt;2e-16 *** ## chelsa 17.68 15.65 1.13 0.269 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 411.5 on 26 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.04679, Adjusted R-squared: 0.01013 ## F-statistic: 1.276 on 1 and 26 DF, p-value: 0.2689 To visualise the correlation we plot our temperature data with the two linear models. p_mean &lt;- df_corr_ann %&gt;% ggplot(aes(x = chelsa, y = gpp_meanann)) + geom_point() + geom_smooth(method=&#39;lm&#39;, color=&quot;red&quot;, size=0.5, se=FALSE) + xlab(&quot;Chelsa temperature&quot;) + ylab(&quot;GPP mean [µmolCO2 m-2 s-1]&quot;) + theme_classic() p_mean p_median &lt;- df_corr_ann %&gt;% ggplot(aes(x = chelsa, y = gpp_medianann)) + geom_point() + geom_smooth(method=&#39;lm&#39;, color=&quot;blue&quot;, size=0.5, se=FALSE) + xlab(&quot;Chelsa temperature&quot;) + ylab(&quot;GPP median [µmolCO2 m-2 s-1]&quot;) + theme_classic() p_median We can see a slight positive correlation between the mean and median annual GPP and the CHELSA temperatures. To see if this varies across the whole year, we will also investigate the correlation between mean seasonal GPP and CHELSA temperatures. The same steps apply as for considering mean annual GPP, except that the data is grouped into seasons according to the equinoxes. WS &lt;- as.Date(&quot;2012-12-21&quot;, format = &quot;%Y-%m-%d&quot;) # Winter Solstice SE &lt;- as.Date(&quot;2012-3-21&quot;, format = &quot;%Y-%m-%d&quot;) # Spring Equinox SS &lt;- as.Date(&quot;2012-6-20&quot;, format = &quot;%Y-%m-%d&quot;) # Summer Solstice AE &lt;- as.Date(&quot;2012-9-23&quot;, format = &quot;%Y-%m-%d&quot;) # Autumn Equinox GPP_seasonal &lt;- ddf_allsites_nested_joined %&gt;% unnest(data) %&gt;% mutate(site = siteid) %&gt;% group_by(site) %&gt;% mutate(season = case_when(TIMESTAMP &gt;= WS | TIMESTAMP &lt; SE ~ &quot;Winter&quot;, TIMESTAMP &gt;= SE &amp; TIMESTAMP &lt; SS ~ &quot;Spring&quot;, TIMESTAMP &gt;= SS &amp; TIMESTAMP &lt; AE ~ &quot;Summer&quot;, TIMESTAMP &gt;= AE &amp; TIMESTAMP &lt; WS ~ &quot;Autumn&quot;)) %&gt;% group_by(site, season) %&gt;% summarise(gpp_season = mean(GPP_NT_VUT_REF)) %&gt;% ungroup() # we remove the towers we don&#39;t have in the other datasets for the correlations GPP_seasonal &lt;- GPP_seasonal %&gt;% filter(!site %in% c(&quot;DE-Obe&quot;,&quot;DK-ZaH&quot;, &quot;RU-Cok&quot;)) # here we aren&#39;t looking t landcover so we only add the CHELSA temperatures to the correlations data frame. chelsa_season &lt;- df_sites_temp[,c(1,6)] df_corr_season &lt;- merge(GPP_seasonal, chelsa_season, by = &quot;site&quot;) df_corr_season &lt;- as.data.frame(df_corr_season) head(df_corr_season) ## site season gpp_season temp_chelsa ## 1 BE-Bra Autumn 2.4531735 1.85 ## 2 BE-Bra Spring 6.6628702 1.85 ## 3 BE-Bra Summer 9.6589652 1.85 ## 4 BE-Bra Winter NA 1.85 ## 5 BE-Lon Autumn 1.3152037 0.95 ## 6 BE-Lon Spring 0.9780171 0.95 Once, the data frame for the seasonal correlations is ready, we order the plots by season and plot the results. # re-order by season as they should appear in plots order_by_season &lt;- c(&quot;Spring&quot;, &quot;Summer&quot;,&quot;Autumn&quot;, &quot;Winter&quot;) df_corr_season &lt;- arrange(transform(df_corr_season, season=factor(season, levels=order_by_season)), season) p_mean_season &lt;- df_corr_season %&gt;% ggplot(aes(x = temp_chelsa, y = gpp_season)) + geom_point() + geom_smooth(method=&#39;lm&#39;, color=&quot;red&quot;, size=0.5, se=FALSE) + xlab(&quot;Chelsa temperature&quot;) + ylab(&quot;GPP mean [µmolCO2 m-2 s-1]&quot;) + facet_wrap(vars(season)) + theme_classic() p_mean_season From the graph above we can see that the correlation between GPP and temperature differs among the seasons. Summer temperatures show a negative correlation with GPP. Spring, Autumn and Winter temperatures, on the other hand, are slighlty positively correlated with GPP. Spring and Summer seem to have more variability in GPP at different temperatures. Back to landcover and GPP. The landcover column from the GlobCover raster contains numbers, even though it is actually categorical data. Each number represents a specific landcover type, so we can add a column with the corresponding the landcover category name. df_corr_ann &lt;- df_corr_ann %&gt;% mutate(landcover_cat = case_when(landcover == 14 ~ &quot;cropland&quot;, landcover == 20 ~ &quot;mosaic cropland&quot;, landcover == 50 ~ &quot;deciduous forest&quot;, landcover == 70 ~ &quot;evergreen forest&quot;, landcover == 90 ~ &quot;needleleaved forest&quot;, landcover == 110 ~ &quot;mosaic forest&quot;, landcover == 120 ~ &quot;mosaic forest/shrub/grass&quot;, landcover == 130 ~ &quot;mosaic shrubland&quot;, landcover == 140 ~ &quot;mosaic grassland&quot;, landcover == 150 ~ &quot;sparse vegetation&quot;)) %&gt;% droplevels() Again, we can plot it to visualise the correlation. However, since landcover is categorical data we will make a boxplot instead. A boxplot is a figure that gives a graphical representation of the variability or spread of the data, by providing information beyond the central tendency of the data. A boxplot will show the median (the mid-pont of the data or 50th percentile, shown as a bold line in the middle of the box), the interquartile range (the box shows the range of the data with the edges being the 25th percentile and 75th percentile limits), whiskers (the lines coming out of the box) and any outliers as points beyond the whiskers. We make a boxplot for mean GPP and the GlobCover landcover raster, as well as mean GPP and the provided fluxnet tower site landcover for comparison. ggplot(data = df_corr_ann, aes(x = landcover_cat, y = gpp_meanann, group = as.factor(landcover_cat)))+ geom_boxplot() + labs(x = &quot;Landcover Category (raster)&quot;, y = &quot;GPP mean [µmolCO2 m-2 s-1]&quot;) + theme_classic() + theme(axis.text.x = element_text(angle = 90, size = 10)) ggplot(data = df_corr_ann, aes(x = umd_land_cover, y = gpp_meanann, group = as.factor(umd_land_cover)))+ geom_boxplot() + labs(x = &quot;Landcover Category (tower - umd)&quot;, y = &quot;GPP mean [µmolCO2 m-2 s-1]&quot;) + theme_classic() + theme(axis.text.x = element_text(angle = 90, size = 10)) We don’t see a particularly strong correlation with the two plotted landcover categories, as the mean GPP varies depeding on the landcover type. Thus we need to consider other predictors and more complex models to find better predictors of GPP. 3.2.6 Key points from the tutorial Integrating remote sensing data: We extracted NDVI in the area around our eddy flux towers from the MODISTools package and converted it into a raster format. Then collapse NDVI measured within an area around the eddy flux towers to calculate the mean across years. Merge this temporal data with our eddy flux data. Apply a linear regression to test whether the addition of NDVI data to our model of GPP and PPFD_IN improves its accuracy. Points: The eddy flux tower sites were plotted as points on a map of Europe. Extract coordinates of the tower sites from the metadata. Transform the points into SpatialPoints with the sp package and plot them on a map of Europe. Shapefiles: We added country and border information to our tower site plots. Extract the information about the spatial polygons in the europe_shape shapefile and apply one coordinate reference system. Add the SpatialPoints matrix and shapefile data europe_shape to the tower site data frame. Filter out the countries with towers and visualize only the countries containing a tower. Rasters: We investigated whether the variables landcover and temperature can explain GPP. Load landcover data from Globcover temperature data from CHELSA in the form of rasters and plot them around our tower sites with the raster and rasterVis packages. Apply the same resolution (aggregate) and origin (align) to the climate and landcover rasters. Compare modelled climate data from CHELSA and WorldClim to our fluxnet data by extracting the temperatures at the tower sites and calculating the MSE (mean square error). Finally, landcover and CHELSA modelled temperatures are plugged into a linear regression to see if they are significantly correlated with GPP. 3.3 Exercise Part 1: Plotting Elevation differences In this first part of the exercise, we consider elevation data from the ETOPO dataset. ETOPO is a 1 arc-minute global relief model of the Earth’s surface that integrates land topography and ocean bathymetry. Since it is modelled date there is a certain level of uncertainty associated with it. In this exercise, you will look at the differences between the elevations from the tower data, which is directly measured at the site, and the values you will extract from this modelled ETOPO data. Note: think about which packages you need to load to complete each of the tasks. Load the ‘dataset_ex3.csv’ file containing the elevations of the towers. Load both the ‘ETOPO.tif’ raster and the ‘europe_shape’ shapefile. Plot the shape of Europe over the raster. (Since we used the shapefile in the tutorial too, it isn’t located in the same folder as the data for this exercise. You’ll have to use the correct pathway to load it, if you are unsure about how to find this use the ‘Files’ tab on the right can help you figure it out.) Extract the elevations from the raster at the tower sites. Calculate the difference between the elevations of the towers and the one extracted from the ETOPO. Plot the difference in elevation calculated for each site on a map of Europe. Hint: there are several ways to do this, one could be to using ifelse(). Part 2: Temperature and Elevation Correlations Since we know temperature and altitude (or elevation) are correlated due to a decrease in air pressure at higher altitudes, we want to see this correlation in our data. Therefore, in this second part, we want to find out which of the elevations we gathered in the first part (ETOPO and tower elevations) correlates better with the temperatures measured at the towers. Since the temperatures and elevations for the tower were measured in the same place, we would expect those to be more correlated than the modelled ETOPO elevations and the tower temperatures. Correlate the ETOPO elevations at the towers sites with the temperatures measured at the towers. Correlate the elevations measured at the tower sites with the temperatures measured at the towers. What is the mean squared errors of the two models above. Which of the two correlates better? The ‘lapse rate’ is known as the rate of decrease of an atmospheric variable, in our case temperature, with increased elevation (also altitude). This lapse (gradual fall) in temperature with increasing altitude at a given time and place, is the slope of your models above. Find and compare the lapse rate of the two models. Hint: if you are stuck take a closer look at your model with summary(). Now, that you’ve made it to the end of the exercise, remember to ‘knit’ this document so you can upload the html to moodle. References "],["ch-04.html", "Chapter 4 Data Scraping 4.1 Introduction 4.2 Tutorial 4.3 Exercise", " Chapter 4 Data Scraping 4.1 Introduction This chapter covers data scraping in R and yes, this is not a typo. Scraping means to gather or to extract whereas scrapping means to get rid of something. The goal of the sections below is to gather and convert online data into a structured format that can be easily accessed and modified in R. As a case study, we will scrape data from a website containing various information about fish species, called FishBase. We will first learn how to access the website and how to extract useful useful information from plain text. Then, we will look at how to access a table that is embedded in the website and how to generate a new table in R which holds all information we want to extract. Next up is cleaning up the scraped data for visualization and analysis. At the end we will conduct a case study where we create a model to make predictions about fish species richness. 4.1.1 Key points from the lecture Let us first have a brief review of the main concept of the lecture. The web is the largest source of information and often free to access. In some cases the information is already presented in a nice format and can be easily copied into an excel spreadsheet. However, this is often not the case and data-sets are only presented in a format that is not easy to download or modify. Manually copying data from different sub pages and sections is a tedious, slow and error prone approach. We therefore need a more automated and efficient technique to access such data. Web scraping is a popular technique to extract information directly from a website by accessing its underlying HTML code. Scraping allows us to gather this unstructured data from many websites and put it all together in a ready-to-use data format. This structured data can then be further used as training, validation or test data sets for our machine learning algorithms. Important Concepts of Websites HTTP: The Hypertext Transfer Protocol is a widely used protocol for information systems. Hypertext documents include hyperlinks that can be clicked to access other resources. Put simply, HTTP is the foundation of how information, i.e., data, is communicated in the world wide web. HTML: Hypertext Markup Language is the coding language in which most websites are written. This includes elements like formatting or structuring and is often combined using CSS or JavaScript. HTML is organized using tags, which are surrounded by &lt; &gt;. Each HTML document is made of elements that are specified using tags. HTML elements and HTML tags are often confused. Tags are used to open and close the object, whereas elements include both tags and its content. Let’s consider an example with the &lt;h1&gt; tag: &lt;h1&gt; Title of the document &lt;/h1&gt; is an element, and &lt;h1&gt;, &lt;/h1&gt; - are the tags enclosing it. The &lt;&gt; symbol is used to open a tag or an element and &lt;/&gt; is used to close it. HTML documents have a hierarchical or tree like structure with different types of nodes as described in 4.1. The rectangular boxes are referred to as nodes. The Text node is also called as a child node of the Element node and also a leaf node as it only contains text and no links to further nodes. Both of the Element nodes that are attached to the Root Element node are called sibling nodes of the Root Element node. When we do web scraping we go through such a hierarchical structure to get the content (here text in the Test node). Keep in mind that every HTML document can vary with respect to its structure. This example is to just to provide you some knowledge about HTML document and its structure. Figure 4.1: Visualization of the structure of an HTML document. XML: The Extensible Markup Language has some similarities to HTML but the format is generally easier to read for machines. While HTML has a number of pre-defined tags, one can create (“extend”) new tags as needed . This allows to define and control the meaning of the elements contained in a document or text. API: An Application Programming Interface is a set of procedures and communication protocols that provide access to the data of an application, operating system or other services. Both APIs and web scraping are used to retrieve data from websites, but their methodology differs substantially. APIs give us direct access to the data we would want, but they are limited to the corresponding website. As a result, we might find us in a scenario where there might not be an API to access the data we want. In these scenarios, web scrapping would allow us to access the data as long as it is available on a website. Hence APIs are very source/website specific and we can only do what is already implemented but in a clean fashion, while scrapping is more flexible and can be applied (nearly) everywhere but we have to handle all the formatting, inconsistencies, extraction, etc. by yourself. Next, we will demonstrate the principles of web scraping using a simple case study. We will extract fish occurrence data from an online database and perform some basic correlations with the obtained data. 4.2 Tutorial 4.2.1 R-Packages and Functions We will now load all packages necessary to perform web scraping on FishBase, namely RCurl (Temple Lang 2021a) and XML (Temple Lang 2021b). RCurl provides functions to allow us to compose general HTTP requests and provides convenient functions to fetch URLs via get and post requests and process the results returned by the web server. XML give us approaches for both reading (get request) and creating (post request) XML and HTML documents, both locally and on the web via HTTP. Later in this tutorial, we will create some spatial visualization and load the packages raster, sf and rgdal to do so. Before we proceed further to accessing FishBase, we will have a quick look at the useful functions lapply() and sapply(). ## Check for missing packages list_pkgs &lt;- c(&quot;RCurl&quot;, &quot;XML&quot;, &quot;raster&quot;, &quot;rgdal&quot;, &quot;rfishbase&quot;, &quot;tidyverse&quot;, &quot;sf&quot;) new_pkgs &lt;- list_pkgs[!(list_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) ## Load packages lib_vec &lt;- c(&quot;RCurl&quot;, &quot;XML&quot;, &quot;raster&quot;, &quot;rgdal&quot;, &quot;rfishbase&quot;, &quot;tidyverse&quot;, &quot;sf&quot;) sapply(lib_vec, library, character.only = TRUE) # See below for how sapply() works help(package = &quot;XML&quot;) # Use this code line in the RStudio console to learn more about loaded packages Sidenote: For data scraping in R one can also use ‘rjson’ to convert R objects into JSON objects and vice-versa. Another option we will use later are APIs. To get information about the loaded packages we can type the following command in the console: lapply() lapply() allows us to apply the same function to a vector of objects. The respective arguments are X (the vector or object we give as input) and FUN (the function which is applied to each element of X). The output of lapply() is a list of the same length as X where each element of X is the result of applying FUN to the corresponding element of X. X is a vector (atomic, list or data frame) or an expression object. The l in lapply() thus stands for list. A simple example is to change the string value of a matrix to lower case with the R-function tolower() function. First, we set up a vector with string objects that we want to apply the function on. Then we create a pipe that feeds this vector into lapply() which applies tolower. The output is a list of the original two string objects but now written in lower case. fish &lt;- c(&quot;FAMILY&quot;,&quot;SPECIES&quot;) fish %&gt;% lapply(tolower) # Note that tolower() must not be written with brackets ## [[1]] ## [1] &quot;family&quot; ## ## [[2]] ## [1] &quot;species&quot; sapply() sapply() takes a list, vector or data frame as input and gives a vector or matrix as output. This is very useful if we have to use another function which only accepts a vector as input but does not accept a list. Note that we can apply the same function using lapply() or sapply but their outputs are not the same. As an example we can have a look at the cars data where the speed and respective stopping distance are saved. We can apply the min() function to both features of the data set to get their minimal values. If we are using lapply(), we get a list that is accessible using $. With sapply() we get a named vector which can be seen at the headings speed and dist in the output below (those headings are of course the same as the variable names in the outputted list of lapply()). (df &lt;- head(cars)) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 df %&gt;% sapply(min) ## speed dist ## 4 2 df %&gt;% lapply(min) ## $speed ## [1] 4 ## ## $dist ## [1] 2 4.2.2 The FishBase website FishBase is a global species database of fish species. It provides data of various fish species, including information on taxonomy, geographical distribution, biometrics and morphology, behaviour and habitats, ecology and population dynamics as well as reproductive, metabolic and genetic data. Different tools, such as trophic pyramids, identification keys, biogeographical modelling and fishery statistics can be accessed on the website. Furthermore, direct species level links to information in other databases such as LarvalBase, GenBank, the IUCN Red List and the Catalog of Fishes exist. As of November 2018, FishBase included descriptions of 34,000 species and subspecies. Figure 4.2: Screenshot of the FishBase webpage for the species Coregonus lavaretus, a member of the family Salmonidae. As can be read in the distribution paragraph, it is widespread in freshwater systems from central and northwest Europe to Siberia. As shown in Figure 4.2, the website contains lots of information for all the different species and this information is stored in various different data types. A few examples of these data types are: Numbers in different formats and units (temperature ranges, latitudinal distribution) Text blocks (description of the distribution) Tables (fecundity, larvae information) Pictures and videos (of the species, embedded into HTML code) For a better understanding of the following R code, you are strongly encouraged to have a look at the FishBase website in your browser. In the next sections you will learn how to deal with these different complex data types. A careful approach is required when extracting and downloading such information. Understanding the structure of the website is an important first step and it will save you coding time. So, always identify the relevant information you want to extract first and then write a suitable extraction code which fits the needs of your machine learning algorithm. 4.2.3 Accessing FishBase We are now ready to extract data from the Coregonus lavaretus website on FishBase. For this we first create a string object which holds the name of the fish profile we want to access. Using a variable to do this, adds flexibility in functions as will be shown below. If we want to get data about other species we can simply assign a new value to the object xwith the desired species name. x &lt;- &quot;Coregonus-lavaretus&quot; x ## [1] &quot;Coregonus-lavaretus&quot; Next, we use the function paste() to create an object holding the URL to the fish profile. Attaching strings like this is called concatenate and will be used quite often in later tutorials, so remember this wording. To create the URL we do not have to add any separation between the arguments, so we use sep = \"\". Using the flexibility of using x allows us here to add any species name directly into paste() instead of deleting and adding another Latin name every time. url &lt;- paste(&quot;http://www.fishbase.de/summary/&quot;, x ,&quot;.html&quot;,sep=&quot;&quot;) url ## [1] &quot;http://www.fishbase.de/summary/Coregonus-lavaretus.html&quot; For url we could also directly use http://www.fishbase.de/summary/Coregonus-lavaretus but then we would loose flexibility if we want to look for information about other species. Checkpoint Try to do the same for the URL of other species by changing the code above. Solution # Change the object x_end to desired species name x_ex&lt;- &quot;Salvelinus alpinus&quot; # Concatenate the URK url_ex &lt;- paste(&quot;http://www.fishbase.de/summary/&quot;,x_ex,&quot;.html&quot;,sep=&quot;&quot;) To access the URL and its HTML documents, we will use various functions from the XML package. First, we want to get the URL content using getURLContent() which takes a URL as input argument. Then, we use htmlParse() to read the HTML document from the URL content into an R object. To get help on functions we can always use help(\"htmlParse\"). fishbase &lt;- url %&gt;% getURLContent(followlocation = TRUE) %&gt;% htmlParse() The HTML document saved in fishbase is quite long to show it entirely but a snapshot is displayed in Figure 4.3. Figure 4.3: Extract from the HTML document saved in the variable fishbase. Somewhere in this gigantic object we can find the relevant information we are looking for. For example, Figure 4.4 below shows the extract where the maximal lenght of the fish is documented. Check out the online profile to double check whether the value is correct. Figure 4.4: Extract of fishbase documenting maximal length of Coregonus lavaretus. Highlighted in red are the opening and closing tags of the element where the information on the maximal length is stored. Knowing these tags, we can identify where our information is stored and how to extract it. In this example, we need to target either the span or \\div tags. In order to extract the information inside these two tags, we use the function getNodeSet(). This function finds XML nodes that match a particular criterion. span is used for a small chunk of HTML inside a line whereas div is used to group larger chunks of code. fishbase_div &lt;- fishbase %&gt;% getNodeSet(&quot;//div &quot;) fishbase_span &lt;- fishbase %&gt;% getNodeSet(&quot;//span &quot;) In the next two Figures 4.5 and 4.6 we can see the differences of what is stored in variable fishbase and fishbase_div. The difference to fishbase_span is similar. We see that instead of having this one long unstructured extract in Figure 4.5, we have now a list of objects that we easily access. In Figure 4.6, the list can be identified by the [[22]] in the top left corner which marks the position of the object within the list. In short, getNodeSet() identifies all the sections for a given tag (i.e., nodes), separates them and gathers them in a list. Figure 4.5: fishbase before the command getNodeSet() Figure 4.6: fishbase after the command getNodeSet() 4.2.4 Scraping Numbers Next, we now want to extract the maximal body length of Coregonus lavaretus out of the list of nodes we created in the previous section. For this, we will use the function xmlValue() which allows us to access the text in the nodes by converting them into strings. To proceed further, we briefly have a look at regular expressions. A regular expression is basically a sequence of characters (think of a certain word or a number) that can be searched for in a string (which is nothing else but a sequence of characters). The regexec() (for regular expression) function allows us to search for a pattern within a string. The output of regexec() gives you different information. For now it enough to know that it returns a list where the first object holds the position of the pattern in the string or simply a -1 if the pattern is not found. In our case, we are going to look for the pattern “Max length” because after this pattern, the value we are looking for is saved (see Figure 4.7). Figure 4.7: In blue highlighted is the regular expression on the FishBase website which we are looking for. The code to identify the position where the regular expression Max Length is located is rather complex, so here is a step-by-step breakdown. You can run each part of the pipe below one-by-one to better understand what is happening. pos &lt;- fishbase_span %&gt;% lapply(xmlValue) %&gt;% # 1. Convert all list objects in fishbase_span into strings regexec(pattern =&quot; Max length&quot;) %&gt;% # 2. Search strings for pattern which.max() # 3. Return position in list where highest number is found # (just one way to identify the node with relevant information) pos ## [1] 44 Now we know that our regular expression Max length can be found in node number 42. To get access to the text in this node we simply access fishbase_span via the list notation [[ ]] and turn the object into a string and save it as fish_length. As you can see in the output, the information on max length has been found correctly. fish_length &lt;- fishbase_span[[pos]] %&gt;% xmlValue() fish_length ## [1] &quot;\\r\\n\\t\\t\\t\\t\\tMaturity: Lm27.1, range 40 - ? cm Max length : 73.0 cm TL male/unsexed; (Ref. 40637); max. published weight: 10.0 kg (Ref. 35388)\\t\\t\\t\\t&quot; We now use the function substr(), which allows to extract a section of a string that lies between two characters. Think of this as extracting a number interval in a series of numbers. Unfortuntaly, identifying this start and end has to be done somewhat by hand. In Figure 4.7 you can see that, starting from the ‘M’ in Max Length, there are 13 characters until the value begins with a 7 and 16 characters until it ends with a 0. Keep in mind that every blank space is also counted as character. Knowing this, we can first use regexec() to get the character position where Max Length starts and just add 13, respectively 16 to the value. Two things to note here: (i) Keep in mind that every blank space is also counted. (ii) Note that the output of regexec() is a list where the first object is the character position where the pattern begins in the string. Thus, we need to access this position number by using [[1]][1]. Another way instead of using regexec() is just by typing in some numbers into substr() and find the relevant characters by trial and error. start_M &lt;- fish_length %&gt;% regexec(pattern= &quot;Max length&quot;) max_length &lt;- fish_length %&gt;% substr(start_M[[1]][1] + 13, start_M[[1]][1] + 16) max_length ## [1] &quot;73.0&quot; At last, we have to convert the value of the maximal length (which is a string of characters) into a number using the function as.double(). This converts the value into a machine readable format which makes things easier later. We see from the output below that we correctly exctracted a maximal length of Coregonus lavaretus of 73 cm. Now, we can start thinking about looping this approach to extract the maximal length of various species and collect them in a nicely formatted data frame. To demonstrate fexibility in data reading: This entire approach can also be used to for example to identify any numeric value that is followed by any one or enclosed by any two characters. max_length &lt;- as.double(max_length) max_length ## [1] 73 4.2.5 Scraping Text Snippetrs International Union for Conservation of Nature Another interesting information on FishBase is the IUCN (International Union for Conservation of Nature) status of each species. The IUCN Red List of threatened Species has evolved to become the world’s most comprehensive information source on the global extinction risk status of animal, fungus and plant species. The IUCN Red List is a critical indicator of world’s biodiversity and has been established in 1994. It contains explicit criteria and categories to classify the conservation status of individual species on the basis of their probability of extinction. After a species is evaluated by the IUCN, it is placed into one of eight categories based on its current conservation status as shown in the figure. Far more than a list of species and their status, it is a powerful tool to inform and catalyze action for biodiversity conservation and policy change, critical to protecting the natural resources we depend on for survival. The IUCN also provides information about the range, population size, habitat and ecology, use and/or trade, threats, and conservation actions that will help inform necessary conservation decisions. Figure 4.8: Structure of IUCN categories. Extracting IUCN Status Now that we learned how to extract values, we can move on to extracting text snippets! In this part, we are going to get the IUCN Status of Coregonus lavaretus. In contrary to above, we are going to use the function which() (not which.max()) to find the position of the elements we are looking for. We use regexec() to search for matches of our pattern within each element of a character vector. In this case, the pattern is simply IUCN. As for the maximal length, we can look up this pattern on the website we are scraping from. IUCN_pos &lt;- which( # which() gives us the numbers of nodes where pattern was found fishbase_div %&gt;% # pipe from above to find pattern put into which() lapply(xmlValue) %&gt;% regexec(pattern = &quot;IUCN&quot;) # end of pipe &gt; 0 # check for which() to only give positive numbers for nodes ) # we do this because if pattern is not found, regexec returns a -1 IUCN_pos # pattern can be found in nodes 5, 14 and 24 ## [1] 5 14 24 If the pattern cannot be found in fishbase_div, the variable w_IUCN will be empty (we use fishbase_div here because the pattern was not found in fishbase_span). To avoid complications later, we better do a quick check and set the variable to NA if it is empty and else use xmlValue() to get the value at the last node where the pattern was found. if(length(IUCN_pos)==0){ # if which() from above returned 0 (no node with &gt;0) IUCN_stat = &quot;NA&quot; # set status to NA } else { # else IUCN_xml &lt;- fishbase_div[[ # access fishbase_div nodes with [[ IUCN_pos[length(IUCN_pos)]]] %&gt;% # access last node where pattern was found, length(IUCN_pos) = 24 xmlValue() # feed node into xmlValue() to return readable string } IUCN_xml ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tIUCN Red List Status (Ref. 124695)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Vulnerable (VU) (D2); Date assessed: 01 January 2008\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; The IUCN_xml looks a little confusing and we only need a tiny part of it, namely VU. We can see that in this node VU has a unique character pattern which does not appear twice: The closing bracket )is right after U, an alphabetical character. We can use this pattern to extract VU! Here, the str_extract() function from tidyverse comes in very handy. We can directly specify the pattern we are looking for and extract the substring from the original string. Note that for generalization, we need to specify [[:alpha:]] for any alphabetical character +[)] for the closing bracket. However, if we do so, we still extract the bracket as well. To get rid of it, we can simply remove it by replacing it with “nothing” using str_replace(). Instead of specifying the closing bracket, we use [:punct:] which generalizes punctuations like . , : ; ? ! etc. As we see in the output below, the IUCN status VU has been correctly extracted - nice! Plus, we don’t need to convert it and can use it as string later on. IUCN_sta &lt;- IUCN_xml %&gt;% str_extract(pattern = &quot;[:alpha:]+[)]&quot;) %&gt;% str_replace(pattern = &quot;[:punct:]&quot;, replacement = &quot;&quot;) IUCN_sta ## [1] &quot;VU&quot; 4.2.6 Scraping Tables The next step is to read a table from a website. Here, we are going to get information on the eggs of Coregonus lavaretus. Have a look at its profile. In section Life cycle and mating behavior you can see that the table for information on eggs is a link and not a table directly. Foruntatley for us, we can use the function getHTMLLinks() to retrieve links within an HTML document or the collection of names of external files referenced in an HTML document. In Figure 4.9, an extract is shown of the list with more than 100 links found on the profile page. We see that either we could directly access link number 100 or search for the substring FishEggInfoSummary to look for other links. In fact, we will find two links with this substring (see Figure 4.10. For this reason, we quickly check if the links are identical of if they lead to different pages. We can do this by using the logic operator ==. The link comparison gives TRUE as output meaning that both links are completely identical and it does not matter which one we use. Let us save the first one in egg_link. egg_link &lt;- fishbase %&gt;% getHTMLLinks() %&gt;% str_subset(pattern = &quot;FishEggInfoSummary&quot;) egg_link[1] == egg_link[2] ## [1] TRUE egg_link &lt;- egg_link[1] Figure 4.9: Extract of the list of links found on the FishBase profile of Coregonus lavaretus. Figure 4.10: List of links that have the substring FishEggInfoSummary. Checkpoint Call the variable egg_link, you will notice the link begins with ‘..’ (see above 4.10). Try to remove these dots so that the link begins with ‘/Reproduction/..’. As a hint, you can use the function str_replace() as was done above. You will need this code later, so make sure that you run it. # your code Solution egg_link &lt;- egg_link %&gt;% str_replace(&quot;..&quot;, &quot;&quot; ) egg_link ## [1] &quot;/Reproduction/FishEggInfoSummary.php?ID=232&amp;GenusName=Coregonus&amp;SpeciesName=lavaretus&amp;fc=76&amp;StockCode=246&quot; As you can see, this is no proper URL yet. Thus, we first need to create a working URL to access its content. Similarly to what we did previously, we do this using the function getURLContent(). Since we know that this link leads to a table, we can pipe the URL content into readHTMLTable(). The output here is a list and we have access the first object as done below. egg_tab &lt;- paste(&quot;http://www.fishbase.de/&quot;, egg_link, sep=&quot;&quot;) %&gt;% getURLContent(followlocation=TRUE, .encoding=&quot;CE_UTF8&quot;) %&gt;% readHTMLTable() egg_tab &lt;- egg_tab[[1]] # To save the list object as a data frame egg_tab ## Main Ref. ## 1 Place of Development ## 2 Shape of Egg ## 3 Attributes ## 4 Color of Eggs ## 5 Color of Oil Globule ## 6 Additional Characters ## 7 Get Information on Scirus Now we can extract information from this table by using the function which(). We want to find the position in this table, where Shape of Egg is defined. To identify the right row, we look in the first column for the respective string and extract the value stored in the second column of this row. To automatize, we add a check to see whether any information was extracted at all from the table. If not (as it is the case here), we just set egg_shape to NA. egg_shape &lt;- egg_tab[ # Accessing rows of egg_tab (remember: df[rows, cols]) which(egg_tab[, 1] == &quot;Shape of Egg&quot;), # Get number of row where &quot;Shape of Egg&quot; appears in the first column 2] # Access the second column egg_shape ## [1] &quot;&quot; if(egg_shape == &quot;&quot;) {egg_shape = &quot;NA&quot;} egg_shape ## [1] &quot;NA&quot; Now we can put all the information into a data frame for the entry Coregonus lavaretus by using the function tibble(). Having done this entire data scraping for one species, we can start to automatize the process for multiple species and simply append the data to our data frame. species_df &lt;- tibble(Species = &quot;Coregonus-lavaretus&quot;, Length = max_length, IUCN = IUCN_sta, Egg = egg_shape) species_df ## # A tibble: 1 × 4 ## Species Length IUCN Egg ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Coregonus-lavaretus 73 VU NA Checkpoint Try to create and add a new variable egg_color to the data frame and feed it with the respective information from our egg_table (follow the same process used for ‘egg_shape’). # your code Solution egg_color = egg_tab[which(egg_tab[, 1] == &quot;Color of Eggs&quot;), 2] # extract information if(egg_color == &quot;&quot;){egg_color = &quot;NA&quot;} # set missing information to NA species_df &lt;- tibble(species_df, Color = egg_color) # Add new variable to data frane species_df ## # A tibble: 1 × 5 ## Species Length IUCN Egg Color ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Coregonus-lavaretus 73 VU NA NA 4.2.7 The Fishbase Package Using an API As we saw in the previous sections, web scraping can be tedious. In this subsection, we want to get data using an API which is a much easier way. We use the R package rfishbase (Boettiger, Temple Lang, and Wainwright 2012) to get data from https://www.fishbase.de. This package makes it very easy to look up for information on the most well-known fish species. It simplifies the data extraction process but also has some limits as we will see below. You ca have a look at the functions built into rfishbase by typing help(package = \"rfishbase\") into your RStudio console or have a look at the RDocumentation. Most functions have straight forward names and have a sring as input which holds the Latin name of the wanted fish species. Let us get some information on Coregonus lavaretus with this new package! We see that the super short code species(\"Coregonus lavaretus\")provides us with 101 variables with entries for our fish species - think about the hand-written web scraping code this would require! To directly asses the maximal length of the species, we can directly use the $ notation. As you might recall, this is the same length as we got above. CL &lt;- species(&quot;Coregonus avaretus&quot;) CL$Length ## [1] NA In the next step we are interested to get information about the diet of Coregonus lavaretus by using the function diet_items(). It allows us to access the table on the food items of the chosen species. To extract information on food, we can save the diet items in a table and use the tidyverse notation plus the respective variables FoodI, FoodII: food &lt;- diet_items(&quot;Coregonus lavaretus&quot;) %&gt;% as_tibble food %&gt;% dplyr::select(FoodI, FoodII) %&gt;% head() # We&#39;re using dplyr:: here to avoid package conflicts ## # A tibble: 6 × 2 ## FoodI FoodII ## &lt;chr&gt; &lt;chr&gt; ## 1 zoobenthos mollusks ## 2 nekton finfish ## 3 zoobenthos benth. crust. ## 4 zoobenthos worms ## 5 zoobenthos benth. crust. ## 6 zoobenthos benth. crust. Checkpoint Now your next task is to get information on predators (hint: the function is exactly named like that). # your code Solution # use function &#39;predators()&#39; for Coregonus lavaretus pre &lt;- predators(&quot;Coregonus lavaretus&quot;) pre %&gt;% dplyr::select(PredatorName) %&gt;% head() ## # A tibble: 6 × 1 ## PredatorName ## &lt;chr&gt; ## 1 Coregonus peled ## 2 Esox lucius ## 3 Sander lucioperca ## 4 Lampetra fluviatilis ## 5 Phocoena phocoena ## 6 Salmo trutta trutta Limits of APIs One of the major limitations of an API is that we can only use already implemented functions. For example, in our case, we cannot get the IUCN Status of a given species because it is not built into rfishbase. In order to obtain the IUCN Status we must use another API, namely the package rredlist (Chamberlain 2020). Unfortunately, to have access to the rredlist API we need an authentication key which we only get for a small fee. As it is with other things in life, either you try to put in the work yourself or you can pay someone to do so (except for the amazing open source community). 4.2.8 Summary In this third section we learned how to extract data from a website. We saw that this can be done either with web scraping or using APIs. We saw that web scraping can be tedious, but we learned about some limitations of APIs. Additional Resources: http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/styled-4/styled-6/code-13/ https://ourcodingclub.github.io/tutorials/webscraping/ https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/ 4.2.9 Case Study Please note that the code below is written in base R and not tidyverse which makes the codes a bit more difficult to read. But do not worry about this and simply just enjoy the beauty of what R is capable of. Next, we look at another case study on species richness and red list species proportions. To proceed with the case study we need to prepare our dataset. First, we need to get a list of species for the dataset. In this subsection we will see how to get all the species in a given family. 4.2.9.1 Creating List of Species In this section, we want to get all the species of the family of Salmonidae. As we did above, for flexibility we will create an object x with the name of the family and use paste() to get the link. We then use getURLContent() to get the content of the link url and save it in con_sal. x &lt;- &quot;Salmonidae&quot; url &lt;- paste(&quot;http://www.fishbase.de/Nomenclature/FamilySearchList.php?Family=&quot;, x, sep=&quot;&quot;) con_sal &lt;- getURLContent(url, followlocation = TRUE) Next we create a dataframe using data.frame() and get a list of variables. We will extract the variables with the same number of rows and unique row names. The function readHTMLTable() (from earlier) helps to extract data from HTML tables in an HTML document. Then we can extract the species from the given family with as.character(). We use z[,1] to get the first column which is the column with the scientific names of species. Now we can print the first element of the column with the scientific names. df &lt;- data.frame(readHTMLTable(con_sal)) sp_per_family &lt;- as.character(df[,1]) sp_per_family[1] ## [1] &quot;Brachymystax lenok&quot; Using str_replace() function we can substitute the empty space between the Genus and the Species with a \"-\". Finally we can print the first element of sp_per_family again. sp_per_family &lt;- str_replace(sp_per_family, &quot; &quot;, &quot;-&quot;) sp_per_family[1] ## [1] &quot;Brachymystax-lenok&quot; 4.2.9.2 Extracting IUCN Status for all species In this section, we are going to get the IUCN Status for a given List of species using a ‘for loop’. We will extract the IUCN Status of all the species from the Netherlands. First, we are going to upload the dataset containing the list of the species and some other data that is going to be useful for the next sections. We will do that by using the function read_csv(). This data is taken from this nature article by cropping it to Western Europe. As we only need data related to Netherlands, we will use the function grep() and pass Netherlands as an argument. dataset &lt;- read_csv(&quot;./data/dataset2.csv&quot;) subset &lt;- dataset[grep(&quot;Netherlands&quot;, dataset$Country),] Let us first briefly discuss how to construct a ‘for loop’, since it’s been a while since you used it in previous chapters (also see Chapter Chapter 5 for examples). To get the IUCN Status of a list of species we always change the value of x (the species) and run the code, but if we have to do that for many species it will be very long and tedious. In this case ‘for loops’ come in handy. In a ‘for loop’ the variable x runs over the vector (here each species) and returns the IUCN Status. Before getting the IUCN status we will go over an easy example, such as printing the integers from 1 to 10 using a ‘for loop’. In this case, we iterate over the vector 1:10. for(j in 1:10) { print(j) # this prints the value of j for that given loop } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 Now we can make a ‘for loop’ in order to get the IUCN status of all the species in the above subset (Netherlands) of the initial dataset. We are going to iterate over the column of the dataset with the valid FishBase species names. The code lines inside the loop are exactly a copy-paste of what we had for the Coregonus lavaretus, but in this case, we have to look for other species. In the last line of the code below, we created a new column in the data frame subset in order to save the IUCN Status. The ‘i’ in subset$IUCN[i] is used to save the IUCN status of the species we are iterating over in the ‘for loop’. It will save the results of the species one by one. i &lt;- 0 for(x in subset$X6.Fishbase.Valid.Species.Name) { i &lt;- i + 1 url &lt;- paste(&quot;http://www.fishbase.de/summary/&quot;,x,&quot;.html&quot;,sep=&quot;&quot;) # we call the url fish_species &lt;- htmlParse(getURLContent(url, followlocation=TRUE)) # get the content fish_species_div &lt;-getNodeSet(fish_species, &quot;//div &quot;) # get the nodes with species w_IUCN &lt;- which(sapply(lapply(fish_species_div,xmlValue),function(x) # look for the IUCN pattern {regexec(pattern=&quot;IUCN&quot;, x)[[1]][1]})&gt;0) if(length(w_IUCN)==0){ # NA if no IUCN status IUCN_status=&quot;NA&quot; } else { # else access information d1_IUCN &lt;- xmlValue(fish_species_div[[w_IUCN[length(w_IUCN)]]]) IUCN &lt;- unlist(regmatches(d1_IUCN,gregexpr(pattern= &quot;[[:alpha:]]+)&quot;, d1_IUCN))) IUCN_status &lt;- sub(pattern=&quot;[[:punct:]]&quot;,replacement=&quot;&quot;,IUCN[1] ) } print(IUCN_status) subset$IUCN[i] &lt;- IUCN_status # make a new column in &#39;subset&#39; containing the IUCN status } ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; ## [1] &quot;VU&quot; ## [1] &quot;LC&quot; ## [1] &quot;LC&quot; We can see which IUCN statuses are present in Netherlands by using the unique() function. There are LC for least concern, VU for vulnerable and of course NA for unknown values. unique(subset$IUCN) ## [1] &quot;LC&quot; &quot;VU&quot; 4.2.9.3 Proportion of species in Netherlands We will now plot the proportion of species in each category for the Netherlands. So let us calculate the number of species in each category (from above we just have 2 outputs,* LC &amp; NT). In general in other countries, we also have other IUCN Status, for example, VU. In this section, we will focus only on the two listed statuses. Using the function length() we can obtain the number of species in each category. number_lc &lt;- length(which(subset$IUCN == &quot;LC&quot;)) number_vu &lt;- length(which(subset$IUCN == &quot;VU&quot;)) number_na &lt;- length(which(is.na(subset$IUCN))) # print the values for NT, VU and NA paste0(&quot;LC:&quot;, &quot; &quot;, number_lc) ## [1] &quot;LC: 8&quot; paste0(&quot;VU:&quot;, &quot; &quot;, number_vu) ## [1] &quot;VU: 1&quot; paste0(&quot;NA:&quot;, &quot; &quot;, number_na) ## [1] &quot;NA: 0&quot; We are ready to plot this as pie chart. slices &lt;- c(number_lc, number_vu, number_na) lbls &lt;- c(&quot;LC&quot;,&quot;NT&quot;, &quot;NA&quot;) pie(slices, labels = lbls, font.main = 1, main = &quot;Proportion of species per IUCN Status in Netherlands&quot;, col=c(&quot;red&quot;, &quot;yellow&quot;, &quot;grey&quot;)) 4.2.9.4 Cleaning data with IUCN Status In this part of the tutorial, we have to clean the data that we will use for the correlations. We have provided the dataset already with the IUCN status since the code needs a lot of time to run, but the procedure is exactly the same as we did in the previous sections. So, let us load the dataset, it’s called datasetIUCN. In the previous sections, we saw that for some species we did not have information on the IUCN status. In these cases, we set the IUCN status as NA. We can check for possible NA values by calling the unique() function. dataset_IUCN &lt;- read_csv(&quot;./data/datasetIUCN.csv&quot;) unique(dataset_IUCN$IUCN) ## [1] &quot;LC&quot; &quot;CR&quot; &quot;lc&quot; &quot;VU&quot; NA &quot;EX&quot; &quot;NT&quot; &quot;DD&quot; &quot;EN&quot; &quot;EW&quot; Checkpoint Now your task is to remove the row with the IUCN status as NA. You can use the function subset() to get the subset of the dataset with information about the IUCN status. # your code Solution # subset the data without NAs (!=NA; not equal to NA), so this effectively removes NAs dataset_IUCN_NA &lt;- subset(dataset_IUCN, dataset_IUCN$IUCN != &quot;NA&quot;) # check if it worked unique(dataset_IUCN_NA$IUCN) ## [1] &quot;LC&quot; &quot;CR&quot; &quot;lc&quot; &quot;VU&quot; &quot;EX&quot; &quot;NT&quot; &quot;DD&quot; &quot;EN&quot; &quot;EW&quot; Next, we will load the shapefile of glacial basins using the package rgdal and the function readOGR(). We fetch the data from different fish basins across Europe in the basin_shapefile from the data stored in the basins folder. In order to plot the basins on the map we will use fortify() function on the basin_shapefile. This function helps to convert a lines-and-points object into a data frame for ggplot. We will store this dataframe as fort_basin. basin_shapefile &lt;- readOGR(&quot;./data/basins&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/pascalschneider/Polybox/Shared/Data Science Lecture Planning - shared folder/4 Datasets/basins&quot;, layer: &quot;basins&quot; ## with 3119 features ## It has 9 fields fort_basin &lt;- fortify(basin_shapefile) head(fort_basin) ## long lat order hole piece id group ## 1 -43.00000 -22.55000 1 FALSE 1 0 0.1 ## 2 -43.03750 -22.69583 2 FALSE 1 0 0.1 ## 3 -43.05325 -22.69523 3 FALSE 1 0 0.1 ## 4 -43.05507 -22.68816 4 FALSE 1 0 0.1 ## 5 -43.06888 -22.68693 5 FALSE 1 0 0.1 ## 6 -43.07279 -22.68390 6 FALSE 1 0 0.1 We can visualise our basin_shapefile using ggplot(). ggplot() + geom_polygon(data = fort_basin, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) Figure 4.11: Plot of fort_basin. 4.2.9.5 Maps of species richness and Red List species proportions Preparing data We now want to map the species richness and Red List species proportions. Let us first calculate the species richness and proportion of Red List species per basin. Species richness is the number of species pro basin and the proportion of red list species is the ratio between the number of species in the red list and the number of species pro basin. We will iterate over the vector with the basin names and we will use the function nrow() to find the number of occurrences. for (x in as.character(basin_shapefile$BasinName)) { # we now restrict to dataset to the basin x dataset_basins &lt;- dataset_IUCN[dataset_IUCN$X1.Basin.Name==x,] # number of species in the given country x n1 &lt;- nrow(dataset_basins) # we now restrict to dataset to the country x and Red List dataset_c_IUCN &lt;- dataset_basins[grep(&quot;VU|EN|EX|EW|CR&quot;, dataset_basins$IUCN),] # number of species in the given country x with given IUCN Status n2 &lt;- nrow(dataset_c_IUCN) # compute the proportion basin_shapefile$proportion[basin_shapefile$BasinName==x] &lt;- n2/n1 basin_shapefile$richness[basin_shapefile$BasinName==x] &lt;- n1 } We can see the newly computed data for the columns ‘proportion’ and ‘richness’ with respect to each basin in the basin_shapefile. head(basin_shapefile@data) ## BasinName Country Ecoregion Endorheic Out_Longit Out_Latit Med_Longit ## 0 Cachoeirinha Brazil Neotropic &lt;NA&gt; -43.10344 -22.693658 -43.06899 ## 1 Comprido Brazil Neotropic &lt;NA&gt; -47.07734 -24.451571 -47.15300 ## 2 Arroyo.Walker Argentina Neotropic &lt;NA&gt; -62.29922 -40.628898 -62.59625 ## 3 Aconcagua Chile Neotropic &lt;NA&gt; -71.53604 -32.912822 -70.65645 ## 4 Amazon Brazil Neotropic &lt;NA&gt; -52.23409 -1.619426 -64.57286 ## 5 Andalien Chile Neotropic &lt;NA&gt; -73.09136 -36.664541 -72.81968 ## Med_Latit Surf_area proportion richness ## 0 -22.595111 228.8151 NaN 0 ## 1 -24.465831 204.7977 NaN 0 ## 2 -40.507344 969.1561 NaN 0 ## 3 -32.770645 7318.8768 NaN 0 ## 4 -6.714857 5888416.9156 NaN 0 ## 5 -36.824925 767.4691 NaN 0 Next we get the map of Europe. We will read the data in continent_shapefile and then will extract the continent ‘Europe’ map. If you call the variable europe you will see that the dataframe is empty. This is because it extracts only the map data which we can simply plot by passing the europe as an argument in plot() function. We will do it in the upcoming code cells. continents &lt;- readOGR(&#39;./data/continent_shapefile&#39;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/pascalschneider/Polybox/Shared/Data Science Lecture Planning - shared folder/4 Datasets/continent_shapefile&quot;, layer: &quot;continent&quot; ## with 8 features ## It has 1 fields europe &lt;- continents[continents$CONTINENT == &#39;Europe&#39;,] Warning message in readOGR(&quot;../data/continent_shapefile&quot;): “First layer europe_map read; multiple layers present in /work/04_data_scraping/data/continent_shapefile, check layers with ogrListLayers()” OGR data source with driver: ESRI Shapefile Source: &quot;/work/04_data_scraping/data/continent_shapefile&quot;, layer: &quot;europe_map&quot; with 53 features It has 94 fields Plotting data So now let us plot the proportions of Red List species. First, we will create a new column ‘proportion_colour’ and in this column, we will store the colours. Then we will break the proportions into 10 different parts by grouping the values in the proportion column into 10 using the cut() function. Then we get rid of the index vector using as.numeric() and get all the values as numeric values. We used the rev() function to reverse the colours, red colour indicates the species that are getting distinct and have very less proportion pro basin and yellow indicates the species with comparatively more proportion in the basin. Finally, we store these colours to the column proportion_colour. The colour indicates the proportion of Red List species occurring in the corresponding basin. We do the same for the species richness. basin_shapefile$proportion_colour &lt;- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$proportion, breaks = 10))] Now we create a new column in the fort_basin dataframe and map the values of ‘proportion_colour’ into the new color column. We are doing this to have the colour (species proportion divided into 10 parts) and lat-long values in one datframe which helps to plot the graph using ggplot(). Remember, above we extracted the continent ‘europe’ from the continent_shapefile, here we will use fortiy() on the europe dataset to plot it. In detail, we will plot the fort_europe and fort_basin data on the map. fort_basin$color &lt;- fort_basin$id # create a new column color for (i in as.numeric(unique(fort_basin$id))){ # map the values into color column by id fort_basin$color[fort_basin$id == i] &lt;- basin_shapefile@data$proportion_colour[i+1] } fort_europe &lt;- fortify(europe) ggplot(fort_basin, aes(x = long, y = lat, group = group)) + geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = &#39;white&#39;) + geom_polygon(fill = fort_basin$color, colour = &quot;black&quot;)+ xlim(-25, 28) Figure 4.12: Proportions of Red List species in Europe. From the Figure 4.12, we see that the proportion of Red List species is highest in South-Western Europe. The aim of the Red List is to inform decision-makers about potentially endangered species, i.e. species whose population size has been rapidly declining during the last decades or the species that only occur in small numbers at present. The proportion of species on the Red List of each region, therefore, gives an indication of the risk of species going extinct in a region and represents an important tool for conservation strategies. Repeat for mapping species richness We will repeat the above steps all together for plotting the species richness on map. # break the richness of the species into 10 parts and then we assign colors to each part basin_shapefile$richness_colour &lt;- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$richness, breaks = 10))] fort_basin$rich_color &lt;- fort_basin$id # create a new column rich_color # mapping the values from basin_shapefile to fort_basin for (i in as.numeric(unique(fort_basin$id))) { fort_basin$rich_color[fort_basin$id == i] &lt;- basin_shapefile@data$richness_colour[i+1] } # plot ggplot(fort_basin, aes(x = long, y = lat, group = group)) + geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = &#39;white&#39;) + geom_polygon(fill = fort_basin$rich_color, colour = &quot;black&quot;)+ xlim(-25, 28) Figure 4.13: Species richness in Europe. 4.2.9.6 Relation of basin size and species richness In the previous section, you observed the spatial patterns of fish diversity across Europe. In this section, we will try to explain these patterns. To achieve this, we will correlate the fish species richness of each basin to the surface area of the corresponding area to see how the species richness varies with respect to the surface area of the basin. We begin with plotting a simple scatterplot. We will log transform the data we have on surface area as it helps to make data conform to normality and also helps to deal with the outliers and skeweness in the data. Then we will plot this data against species richness. Since the data-deficient basins show zero observations in the dataframe, we will remove those first. Now we will create a new dataframe with the surface area and richness. We’ll rename the columns as ‘Basin_area’ and ‘Species_richness’ respectively. We make a simple scatterplot with a regression line to visualise the relationship. basin_shapefile &lt;- basin_shapefile[basin_shapefile$richness!=0,] bs_sr &lt;- tibble(basin_shapefile@data$Surf_area, basin_shapefile@data$richness ) names(bs_sr)[1]&lt;- &#39;Basin_area&#39; names(bs_sr)[2]&lt;- &#39;Species_richness&#39; ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) + geom_point() + geom_smooth(method=&#39;lm&#39;, color=&quot;red&quot;, size=0.5, se=FALSE)+ xlab(&quot;Basin Area&quot;) + ylab(&quot;Species Richness&quot;) + theme_classic() The plot shows a positive correlation between basin area and fish richness, meaning that we expect to see a higher richness in larger basins. This pattern is commonly observed in ecology and one explanation for this is that larger areas provide different habitat types (niches), which allows more species to co-exist. As a next step, we will create a simple model of this relationship. This allows us to make predictions on the richness of fish species in other basins based on the basin area. richness_model &lt;- lm(richness~log(Surf_area), data=basin_shapefile@data) #create a linear model We now want to calculate the confidence intervals of the model to have a better idea of the uncertainty of the model. We will then coerce the basin surface, the model fit and the confidence interval into a new data frame called model_df. #calculate the confidence intervals model_df &lt;- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval=&#39;confidence&#39;))) names(model_df)[1] &lt;- &#39;log_area&#39; head(model_df) ## log_area fit lwr upr ## 174 8.274449 28.39050 26.57731 30.20369 ## 291 9.960755 37.89576 35.13808 40.65343 ## 335 7.949240 26.55739 24.82776 28.28701 ## 340 5.953703 15.30907 13.07901 17.53914 ## 392 9.074369 32.89944 30.71907 35.07981 ## 393 5.538369 12.96794 10.47659 15.45930 The final step is to plot the model fit and prediction intervals over the data. To get nice lines in the plot, the model_df dataframe needs to be ordered by the basin area first. ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) + geom_point() + geom_line(aes(x = model_df$log_area, y= model_df$fit ), color = &quot;red&quot;)+ geom_line(aes(x = model_df$log_area, y= model_df$lwr ), color = &quot;red&quot;, linetype = &quot;dotted&quot;)+ geom_line(aes(x = model_df$log_area, y= model_df$upr ), color = &quot;red&quot;, linetype = &quot;dotted&quot;)+ xlab(&quot;Basin Area&quot;) + ylab(&quot;Species Richness&quot;) + theme_classic() Checkpoint You have now seen how you can calculate and plot the confidence interval of your data. Based on your model you can also create a so-called prediction interval which gives an estimate of the range in which the model will most likely predict the y-values (for a given x-value). In our case the prediction interval would indicate in which range the model would expect the fish richness to be for a given basin surface area. Do you think that these prediction intervals will be broader or narrower than the confidence interval? Try to write the code for calculating and plotting the prediction intervals by yourself. # your code Solution #prediction intervals # make a dataframe of area and richness model predictions model_df_prediction &lt;- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval=&#39;prediction&#39;))) names(model_df_prediction)[1] &lt;- &#39;log_area&#39; # order the basins by size model_df_prediction &lt;- model_df_prediction[order(model_df_prediction$log_area),] # plot the model plot(log(basin_shapefile$Surf_area), basin_shapefile$richness, pch=16, cex=1, col=rgb(0,0,0,0.6), xlab=&#39;Basin area&#39;, ylab=&#39;Basin species richness&#39;) lines(model_df_prediction$log_area, model_df_prediction$fit, col=&#39;skyblue3&#39;, lwd=2) lines(model_df_prediction$log_area, model_df_prediction$lwr, col=&#39;skyblue2&#39;, lwd=2, lty=3) lines(model_df_prediction$log_area, model_df_prediction$upr, col=&#39;skyblue2&#39;, lwd=2, lty=3) 4.3 Exercise Today’s exercise is about getting data from the web and extracting useful insights from it. This exercise is about getting the data from the web. Once you have finished the exercise please knit this document to create an .html file, export it and upload it to moodle. Get the data Following the steps described in the tutorial, get the maximal length of the species Salvelinus alpinus and the Salmo trutta using an API. After fetching the values create a table to store these values in one dataframe. Get the IUCN Status and eggs shape for Salvelinus alpinus and Salmo trutta using web scraping. Again follow the same steps you learned in the tutorial. In the end, add the new information to the dataframe. Get all the species in a family and the IUCN status Your next task is to get all the species in the family ‘Neoscopelidae’ and print the first 5 elements of the family. Extract the ‘Native Exotic Status’ of all the species from France. Then end get the unique values of the Native Exotic Status. For this task you will need to use the file ‘ex02_data.csv’. Now, use the information you got above to plot the proportion of species in each category for France. Extract the mean temperature for Coregonus lavaretus and Salvelinus alpinus using the function stocks() from rfishbase package and store the values in a new dataframe. References "],["ch-05.html", "Chapter 5 Catch-up 5.1 Loops in R 5.2 Functional programming using purr 5.3 String Manipulations 5.4 Web-scraping in a nut-shell 5.5 Tidyverse’s filter and select 5.6 Preparing data for ggplot() 5.7 Base R functions", " Chapter 5 Catch-up This chapter is a review of various applications and functions of the code we have covered so far. We’ll start as always by loading all the necessary packages: list_pkgs &lt;- c(&quot;tidyverse&quot;, &quot;datasets&quot;, &quot;stringr&quot;, &quot;XML&quot;, &quot;RCurl&quot;, &quot;ggplot2&quot;, &quot;rfishbase&quot;, &quot;ggridges&quot;) new_pkgs &lt;- list_pkgs[!(list_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) library(&quot;tidyverse&quot;) library(&quot;datasets&quot;) library(&quot;stringr&quot;) library(&quot;XML&quot;) library(&quot;RCurl&quot;) library(&quot;ggplot2&quot;) library(&quot;rfishbase&quot;) library(&quot;ggridges&quot;) 5.1 Loops in R 5.1.1 Some simple examples # simple example with a for loop ## convert all the names to uppercase letters names = c(&#39;Maria&#39;,&#39;Thomas&#39;,&#39;Andreas&#39;) #initialize uppercase names upper_names = rep(NA,length(names)) for (i in 1:length(names)){ #convert to uppercase upper_name = toupper(names[i]) #add it to upper_names upper_names[i] = upper_name } upper_names ## [1] &quot;MARIA&quot; &quot;THOMAS&quot; &quot;ANDREAS&quot; # equivalent operation with the lapply lapply(names,toupper) ## [[1]] ## [1] &quot;MARIA&quot; ## ## [[2]] ## [1] &quot;THOMAS&quot; ## ## [[3]] ## [1] &quot;ANDREAS&quot; # simple example with a while loop ## calculate the sum of the values values = c(2,4,6) #initialize index i &lt;- 1 #initialize sum sum &lt;- 0 while (i &lt;= 3){# for as long as i is smaller or equal to 3, execute the code below # add the next value to the sum sum = sum + values[i] #update the index i &lt;- i+1 } #print the sum cat(&quot;Sum = &quot;, sum) ## Sum = 12 For loops with breaks: ## Print all the numbers until the number 10 appers. Then stop the loop values = c(3,1,5,18,10,12,13) for (i in 1:length(values)){ #print value cat(values[i],&#39;\\n&#39;) #check the printed number if (values[i] == 10){ break # if the number is 10 stop the loop } } ## 3 ## 1 ## 5 ## 18 ## 10 # while true loop ## Print all the numbers until the number 10 appers. Then stop the loop values = c(3,1,5,18,10,12,13) #initialize index i &lt;- 1 while (TRUE){# It always gets inside and executes the code. The loop only stops when the break starement is encountered #print value cat(values[i],&#39;\\n&#39;) #check the printed number if (values[i] == 10){ break # if the number is 10 stop the loop } #otherwise update the index and continue i &lt;- i + 1 } ## 3 ## 1 ## 5 ## 18 ## 10 5.1.2 Nested loops values &lt;- matrix(c(1,2,3,4,5,6), nrow = 2 ,ncol = 3) values ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # nested loops ## find the sum of the values per column #initialize the sums of the columns to zero col_sum &lt;- rep(0,ncol(values)) #iterate over the columns for (j in 1:ncol(values)){ #iterate over the rows for each column for (i in 1:nrow(values)){ #calculate the sum of the jth column by adding the values of each row (of the jth column) col_sum[j] &lt;- col_sum[j] + values[i,j] } } col_sum ## [1] 3 7 11 5.1.3 Exercise Create a for loop program that estimates the sum of each row. However it has to take into account only the numbers that are greater than 2. If you get it right, you should come up with 3 for the first row and 15 for the second row. values &lt;- matrix(c(3,4,1,5,2,6),nrow=2,ncol=3) values ## [,1] [,2] [,3] ## [1,] 3 1 2 ## [2,] 4 5 6 5.2 Functional programming using purr purrr (Henry and Wickham 2020) is tidyverse’s version of the base R’s apply (lapply(), sapply(),..etc) functions for iterating over objects and lists. But purrr is a lot more powerful than that! While the main data-type used in dplyr is the data frame, for purrr it is lists. Let us look at a few map functions and how they can be used to replace apply functions / loops in R. For this we’ll use the iris data set from the R package datasets (R Core Team 2021). Then we look at nested data handling and a fairly realistic workflow. Splitting the data based on some column, fitting multiple models to each split, and extracting the R^2 for these models. Let us first load in the required libraries. library(&#39;tidyverse&#39;) library(&#39;datasets&#39;) data(iris) # Let&#39;s define a vector with values from 1 to 10 vector_1 &lt;- c(1:10) # Next we define a function to compute the square of a number square &lt;- function(.x){ return (.x^2) } # What happens if we apply our function to vector_1? Of course, it squares every element! vector_1 ## [1] 1 2 3 4 5 6 7 8 9 10 square(vector_1) ## [1] 1 4 9 16 25 36 49 64 81 100 The same can be done using map() functions, as there is nothing wrong with them, but the output format we get, can become a little ambiguous (recall sapply()), and the function syntax can become a little inconsistent. The input to a map() function is either: list, vector, or a dataframe. For lists and vectors the iteration is carried over the elements of the list/vector. For dataframes, the iteration is carried over the columns of the dataframe. Here’s a quick overview over the different functions with their outputs and object classes below as code. Note that .x stands for the input object and .f for the function that you want to apply: map(.x, .f) is the main mapping function and returns a list map_df(.x, .f) takes data frame as input and returns one map_dbl(.x, .f) returns a numeric (double) vector map_chr(.x, .f) returns a character vector map_lgl(.x, .f) returns a logical vector map(vector_1, square) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 9 ## ## [[4]] ## [1] 16 ## ## [[5]] ## [1] 25 ## ## [[6]] ## [1] 36 ## ## [[7]] ## [1] 49 ## ## [[8]] ## [1] 64 ## ## [[9]] ## [1] 81 ## ## [[10]] ## [1] 100 map_df(as.data.frame(vector_1), square) ## # A tibble: 10 × 1 ## vector_1 ## &lt;dbl&gt; ## 1 1 ## 2 4 ## 3 9 ## 4 16 ## 5 25 ## 6 36 ## 7 49 ## 8 64 ## 9 81 ## 10 100 map_dbl(vector_1, square) ## [1] 1 4 9 16 25 36 49 64 81 100 map_chr(vector_1, square) ## [1] &quot;1.000000&quot; &quot;4.000000&quot; &quot;9.000000&quot; &quot;16.000000&quot; &quot;25.000000&quot; ## [6] &quot;36.000000&quot; &quot;49.000000&quot; &quot;64.000000&quot; &quot;81.000000&quot; &quot;100.000000&quot; # map_lgl(vector_1, square) - useless because numbers except 1 and 0 cannot be turned into logic operators 5.2.1 Shortcuts in a purrr function We can also define inline functions using the ~ syntax. This means that you can directly define a function that has to be applied to your input object(s) without having to define it separately. Note that this allows you to combine multiple input variables as you whish. For example, if you want to multiply x and y, you would define the function as ~(.x * .y). In the list below, the multiplication can be swapped arbitrarily and is thus any operation is generally referred to by using operation. Also note that for multiple inputs, you need to adjust the map() function to map2() or pmap() Play around with the codes below to get a feeling for this formulation! Here are some examples explained: ~ operation . becomes function(x) x.: map(input, ~ (2 + . )) adds 2 to every element in your input ~ .x operation .y becomes function(x, y) .x .y.: map2(input1, input2, ~ (.x + .y )) means every input1 element is added to the respective input2 element. ~ ..1 operation ..2 operation ..etc becomes function(1, 2, etc): pmap(list(input1, input2, input3), ~ (..3 + ..1 - ..2)) for example adds the first element in input1 to and subtracts the first element in input2 from the first element in input 3. The same is done for the second element of each input and so on. In other words the elements of the list or vector you give as input are paired up depending on their position. Note that pmap() requires a list for multiple inputs. vector_2 &lt;- rep(5,10) # Vector holding 10 elements which are a 5 vector_3 &lt;- seq(1, 10, 1) # Vector holding elements from 1 to 10 with 1 as interval (same as vector_1) map_dbl(vector_1, ~ (.x^2)) # Does the same as above using our pre-defined square() function ## [1] 1 4 9 16 25 36 49 64 81 100 map_dbl(vector_1, ~ (.x + 1)) ## [1] 2 3 4 5 6 7 8 9 10 11 map2_dbl(vector_1, vector_2, ~(.x * .y)) ## [1] 5 10 15 20 25 30 35 40 45 50 pmap_dbl(list(vector_1, vector_2, vector_3), ~(..1*..2 + ..3)) ## [1] 6 12 18 24 30 36 42 48 54 60 pmap_dbl(list(vector_1, vector_2, 1), ~(..1*..2 + ..3)) # What if vector_3 is a 1? It simply takes 1 for every operation! ## [1] 6 11 16 21 26 31 36 41 46 51 5.2.2 Workflow: nested data, map and mutate Now let us consider workflow with the iris dataset from R. We want to fit a linear model to predict the the Sepal.length as a function of all the other features in the dataset. But we want a different linear model for each type of species. This is pretty realistic as one would have different species to have different “models” for their sepal length. After loading the data, let us first group all the rows in our dataframe by the Species. Then we can use the nest() function, which gives us a nested dataframe for each unique entry in the grouping column. The code below walks you through every step to access the R2 values of different models and then in the end combines everything in one simple pipe. To have a in-depth look at these functionalities, look up the purrr documentation. df &lt;- iris head(df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa nested_iris &lt;- iris %&gt;% group_by(Species) %&gt;% nest() nested_iris # See that for every species there is a tibble defined ## # A tibble: 3 × 2 ## # Groups: Species [3] ## Species data ## &lt;fct&gt; &lt;list&gt; ## 1 setosa &lt;tibble [50 × 4]&gt; ## 2 versicolor &lt;tibble [50 × 4]&gt; ## 3 virginica &lt;tibble [50 × 4]&gt; str(nested_iris) # Each of these tibbles holds information on the other 4 variables for this species ## grouped_df [3 × 2] (S3: grouped_df/tbl_df/tbl/data.frame) ## $ Species: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 2 3 ## $ data :List of 3 ## ..$ : tibble [50 × 4] (S3: tbl_df/tbl/data.frame) ## .. ..$ Sepal.Length: num [1:50] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## .. ..$ Sepal.Width : num [1:50] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## .. ..$ Petal.Length: num [1:50] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## .. ..$ Petal.Width : num [1:50] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## ..$ : tibble [50 × 4] (S3: tbl_df/tbl/data.frame) ## .. ..$ Sepal.Length: num [1:50] 7 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 ... ## .. ..$ Sepal.Width : num [1:50] 3.2 3.2 3.1 2.3 2.8 2.8 3.3 2.4 2.9 2.7 ... ## .. ..$ Petal.Length: num [1:50] 4.7 4.5 4.9 4 4.6 4.5 4.7 3.3 4.6 3.9 ... ## .. ..$ Petal.Width : num [1:50] 1.4 1.5 1.5 1.3 1.5 1.3 1.6 1 1.3 1.4 ... ## ..$ : tibble [50 × 4] (S3: tbl_df/tbl/data.frame) ## .. ..$ Sepal.Length: num [1:50] 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 ... ## .. ..$ Sepal.Width : num [1:50] 3.3 2.7 3 2.9 3 3 2.5 2.9 2.5 3.6 ... ## .. ..$ Petal.Length: num [1:50] 6 5.1 5.9 5.6 5.8 6.6 4.5 6.3 5.8 6.1 ... ## .. ..$ Petal.Width : num [1:50] 2.5 1.9 2.1 1.8 2.2 2.1 1.7 1.8 1.8 2.5 ... ## - attr(*, &quot;groups&quot;)= tibble [3 × 2] (S3: tbl_df/tbl/data.frame) ## ..$ Species: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 2 3 ## ..$ .rows : list&lt;int&gt; [1:3] ## .. ..$ : int 1 ## .. ..$ : int 2 ## .. ..$ : int 3 ## .. ..@ ptype: int(0) ## ..- attr(*, &quot;.drop&quot;)= logi TRUE map(nested_iris$data, dim) # Display the dimension of every data frame saved in nested_iris (50 entries for 4 features) ## [[1]] ## [1] 50 4 ## ## [[2]] ## [1] 50 4 ## ## [[3]] ## [1] 50 4 # Let&#39;s use the nested data frame to create a data frame holding only the linear models: list_linear_models &lt;- map(nested_iris$data, # Give the list of data frames as input to map ~lm(Sepal.Length ~ ., # For every data frame, create a linear model with lm() where # Sepal.Length is the response and all other variables predictors data = .x)) # The data for each linear model is simply the input .x, i.e. the df list_linear_models %&gt;% map(summary) %&gt;% # Apply summary() on all linear models map(&quot;r.squared&quot;) # Extract the r.squared value of each linear model ## [[1]] ## [1] 0.5751375 ## ## [[2]] ## [1] 0.6050314 ## ## [[3]] ## [1] 0.7652193 # Now we know how to create these models, so let&#39;s add them as a variable to our nested data frame model_iris &lt;- nested_iris %&gt;% mutate(linear_model = map(data, ~lm(Sepal.Length~., data = .x))) # To add models model_iris &lt;- model_iris %&gt;% mutate(summary = map(linear_model, ~summary(.))) # To add summary of each model model_iris ## # A tibble: 3 × 4 ## # Groups: Species [3] ## Species data linear_model summary ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 setosa &lt;tibble [50 × 4]&gt; &lt;lm&gt; &lt;smmry.lm&gt; ## 2 versicolor &lt;tibble [50 × 4]&gt; &lt;lm&gt; &lt;smmry.lm&gt; ## 3 virginica &lt;tibble [50 × 4]&gt; &lt;lm&gt; &lt;smmry.lm&gt; # Putting it all together and extracting the R^2 value for each linear model iris %&gt;% # Use iris data frame group_by(Species) %&gt;% # Select variable to group by nest() %&gt;% # Create nested df by groups mutate(linear_model = map(data, ~lm(Sepal.Length~., data = .x))) %&gt;% # Create lm for each group mutate(summary = map(linear_model, ~summary(.))) %&gt;% # Create summary for each group mutate(rsq = map_dbl(summary, &quot;r.squared&quot;)) %&gt;% # Save r.squared from each lm as rsq select(c(Species,rsq)) # Only pick Species and rsq to display ## # A tibble: 3 × 2 ## # Groups: Species [3] ## Species rsq ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 0.575 ## 2 versicolor 0.605 ## 3 virginica 0.765 5.3 String Manipulations 5.3.1 Introduction to strings The stringr package (Hadley Wickham 2019b) offers a set of very handy tools to work with strings. In this document, we will show you how to do some basic string manipulations with `stringr. Further useful sources are the stringr cheathseet and the Chapter 14 in the book R for Data Science. # Let&#39;s load the package and save two srings library(stringr) s1 &lt;- &quot;A1 BC23 DEF456&quot; s2 &lt;- c(&quot;A1&quot;,&quot;BC23&quot;,&quot;DEF456&quot;) # String lengths str_length(s1) # spacing counts ## [1] 14 str_length(s2) # returns length of each string vector ## [1] 2 4 6 # Combining strings str_c(&quot;x&quot;, &quot;y&quot;) ## [1] &quot;xy&quot; str_c(&#39;p&#39;,s2,&#39;q&#39;) # works on each element of the string vector ## [1] &quot;pA1q&quot; &quot;pBC23q&quot; &quot;pDEF456q&quot; # Changing lower/upper case str_to_lower(s1) ## [1] &quot;a1 bc23 def456&quot; str_to_upper(&quot;paradox? is this now lower or upper case?&quot;) ## [1] &quot;PARADOX? IS THIS NOW LOWER OR UPPER CASE?&quot; str_sub() is inclusive - they include the characters at both start and end positions. For example, str_sub(string, 1, -1) will return the complete substring, from the first character to the last. str_sub(s1, 1, 6) # get a substing from 1st to 6th elements ## [1] &quot;A1 BC2&quot; str_sub(s2, 1, 2) # get substings from 1st to 2nd elements ## [1] &quot;A1&quot; &quot;BC&quot; &quot;DE&quot; str_sub(s1, 8) # get a substring from 8th element onwards ## [1] &quot; DEF456&quot; str_sub(s2, 2) # get substrings from 2nd element onwards ## [1] &quot;1&quot; &quot;C23&quot; &quot;EF456&quot; str_split(string, pattern,...) allows to vectorise strings over pattern. str_split(s2, pattern = &quot; &quot;) ## [[1]] ## [1] &quot;A1&quot; ## ## [[2]] ## [1] &quot;BC23&quot; ## ## [[3]] ## [1] &quot;DEF456&quot; fruits &lt;- c( &quot;apples and oranges and pears and bananas&quot;, &quot;pineapples and mangos and guavas&quot;) str_split(fruits, pattern = &quot; and &quot;) ## [[1]] ## [1] &quot;apples&quot; &quot;oranges&quot; &quot;pears&quot; &quot;bananas&quot; ## ## [[2]] ## [1] &quot;pineapples&quot; &quot;mangos&quot; &quot;guavas&quot; 5.3.2 Matching and extracting patterns str_match(string, pattern,...) returns the first pattern match found in each string, as a matrix with a column for each ( ) group in pattern. str_match_all() returns all matched patterns. The pattern can be a substring from the strinh vectors, or can be a generalized pattern to detect for example certain sequences of alphabetic and numeric characters. Please refer to this info page for an introduction to pattern usages. # Detect certain patterns in a vector of characters str_detect(s1, &#39;A&#39;) # s1 is just one string! ## [1] TRUE str_detect(s2, &#39;A&#39;) # s2 splitted s1 into three strings ## [1] TRUE FALSE FALSE # Detect substring (pattern) pattern1 = &quot;BC&quot; # substring as a pattern str_match(s1, pattern1) ## [,1] ## [1,] &quot;BC&quot; pattern2 = &quot;([[:alpha:]]+)([[:digit:]]+)&quot; # pattern2 = alphabetic characters + digits str_match(s1, pattern2) ## [,1] [,2] [,3] ## [1,] &quot;A1&quot; &quot;A&quot; &quot;1&quot; str_match_all(s1, pattern2) ## [[1]] ## [,1] [,2] [,3] ## [1,] &quot;A1&quot; &quot;A&quot; &quot;1&quot; ## [2,] &quot;BC23&quot; &quot;BC&quot; &quot;23&quot; ## [3,] &quot;DEF456&quot; &quot;DEF&quot; &quot;456&quot; str_extract(string, pattern,...) returns the first pattern match found in each string, as a vector, str_extract_all() again returns all matched patterns. str_extract(s1, pattern2) ## [1] &quot;A1&quot; str_extract_all(s1, pattern2) ## [[1]] ## [1] &quot;A1&quot; &quot;BC23&quot; &quot;DEF456&quot; # an equivalent way using basic R: regmatches(s1, gregexpr(pattern2, s1)) ## [[1]] ## [1] &quot;A1&quot; &quot;BC23&quot; &quot;DEF456&quot; str_replace(string, pattern, replacement,...) replaces matched patterns in a string. Alternatively, you can replace substrings by identifying the substrings with str_sub() and assigning into the results. str_replace_all(s1, &quot; &quot;, &quot;-&quot;) ## [1] &quot;A1-BC23-DEF456&quot; str_sub(s1, 1, 2) &lt;- &quot;XX&quot; # Replace the first two positions in s1 s1 ## [1] &quot;XX BC23 DEF456&quot; 5.3.3 Advanced example Now we look at a slightly more complicated example. First we give an introduction to some of the general patterns: - [a-z]: matches every character between a and z (in Unicode code point order) - [abc]: matches a, b, or c. - {n}: exactly n matches If we look at the pattern defined by phone in the code below: 1. ([2-9][0-9]{2}) means the 1st digit is between 2 and 9, and the 2nd and 3rd digits are both between 0 and 9. 2. [- .] denotes the linkages are symbols, which is one of the 3: “-”, ” “(spacing) or”.”. 3. ([0-9]{3}) means the 4th to 6th digits are all between 0 and 9. 4. ([0-9]{4}) means the last 4 digits are all between 0 and 9. strings &lt;- c(&quot; 219 733 8965&quot;, &quot;329-293-8753 &quot;, &quot;239 923 8115 and 842 566 4692&quot;, &quot;Work: 579-499-7527&quot;, &quot;$1000&quot;, &quot;Home: 543.355.3679&quot;) phone &lt;- &quot;([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})&quot; str_extract_all(strings, phone) ## [[1]] ## [1] &quot;219 733 8965&quot; ## ## [[2]] ## [1] &quot;329-293-8753&quot; ## ## [[3]] ## [1] &quot;239 923 8115&quot; &quot;842 566 4692&quot; ## ## [[4]] ## [1] &quot;579-499-7527&quot; ## ## [[5]] ## character(0) ## ## [[6]] ## [1] &quot;543.355.3679&quot; 5.4 Web-scraping in a nut-shell In this session we review some important concepts from web scraping. We will extract the price category of the Coregonus lavaretus. We will do that first using the techniques from week 4 and then using the API rfishbase. The webpage FishBase for the Coregonus lavaretus contains information on its price category which we want to extract. Let us first load the packages. So the package xml is a tool that is used for Parsing and Generating XML Within R. This package contain many functions e.g. getHTMLLinks, getNodeSet, readHTMLTable etc. Whereas package RCurl is used to get General Network (HTTP/FTP/…) Client Interface for R. It is a wrapper for libcurl and provides functions to compose general HTTP requests and provides convenient functions to fetch URIs, get &amp; post forms, etc. and process the results returned by the Web server. library(XML) library(RCurl) First, we save our fish species in the object x. Next, we use the function paste() to convert its arguments to character strings and concatenate them to get the link of the webpage from which we are going to extract the data. We concatenate the URL in order to get the webpage with the summary of the species Coregonus-lavaretus. We do not put any separation between the arguments, so we use sep = ““. x &lt;- &quot;Coregonus-lavaretus&quot; url &lt;- paste(&quot;http://www.fishbase.de/summary/&quot;,x,&quot;.html&quot;,sep=&quot;&quot;) We will now use the function getURLContent() to retrieve the source of a webpage, which is especially useful for retrieving pages for data processing. We will apply the function htmlParse() to obtain an R object and extract the div blocks. Now we are ready to get the price category of the Coregonus lavaretus. We will use the function xmlValue() to get the value at the node. c &lt;- htmlParse(getURLContent(url, followlocation=TRUE)) c_div &lt;- getNodeSet(c, &quot;//div &quot;) values_nodes &lt;- lapply(c_div,xmlValue) values_nodes ## [[1]] ## [1] &quot;&quot; ## ## [[2]] ## [1] &quot;&quot; ## ## [[3]] ## [1] &quot;\\r\\n\\t\\tYou can sponsor this page\\r\\n\\t\\t\\t\\t\\t\\t&quot; ## ## [[4]] ## [1] &quot;You can sponsor this page\\r\\n\\t\\t\\t\\t\\t&quot; ## ## [[5]] ## [1] &quot;\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\tCommon name (e.g. trout)\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tcontainsbegins withbegins withends withis\\r\\n\\r\\n\\t\\t\\tGenus + Species (e.g. Gadus morhua)\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t \\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\t\\t \\r\\n\\t\\r\\n\\t\\r\\n\\t\\tAbout this page\\r\\n\\t\\t\\r\\n\\t\\t\\t\\tMore Info\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tPlus d&#39;info\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tMais info\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tLanguages\\r\\n\\t\\tArabic\\r\\n\\t\\t\\tBahasa/Malay\\r\\n\\t\\t\\tBangla\\r\\n\\t\\t\\tChinese(Si)\\r\\n\\t\\t\\tChinese(Tr)\\r\\n\\t\\t\\tDeutsch\\r\\n\\t\\t\\tEnglish\\r\\n\\t\\t\\tEspaÃ±ol\\r\\n\\t\\t\\tFarsi\\r\\n\\t\\t\\tFranÃ§ais\\r\\n\\t\\t\\tGreek\\r\\n\\t\\t\\tHindi\\r\\n\\t\\t\\tItaliano\\r\\n\\t\\t\\tJapanese\\r\\n\\t\\t\\tLao\\r\\n\\t\\t\\tNederlands\\r\\n\\t\\t\\tPortuguÃªs(Br)\\r\\n\\t\\t\\tPortuguÃªs(Pt)\\r\\n\\t\\t\\tRussian\\r\\n\\t\\t\\tSwedish\\r\\n\\t\\t\\tThai\\r\\n\\t\\t\\tVietnamese\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tUser feedbacks\\r\\n\\t\\tComments &amp; Corrections\\r\\n\\t\\t\\tFish Forum\\r\\n\\t\\t\\tGuest Book\\r\\n\\t\\t\\tFacebook\\r\\n\\t\\t\\t\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tCitation\\r\\n\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tUploads\\r\\n\\t\\tAttach website\\r\\n\\t\\t\\tUpload photo\\r\\n\\t\\t\\tUpload video\\r\\n\\t\\t\\tUpload references\\r\\n\\t\\t\\tFish Watcher\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tRelated species\\r\\n\\t\\tSpecies in Coregonus\\r\\n\\t\\t\\t\\tSpecies in Salmonidae\\r\\n\\t\\t\\t- Classification -Coregoninae\\r\\n\\t\\t\\t\\t\\tSalmonidae\\r\\n\\t\\t\\t\\t\\tSalmoniformes\\r\\n\\t\\t\\t\\t\\tActinopteri\\r\\n\\t\\t\\t\\t\\tChordata\\r\\n\\t\\t\\t\\t\\tAnimalia\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t \\r\\n\\t\\t\\r\\n\\r\\n\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tCoregonus lavaretus (Linnaeus, 1758)\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tEuropean whitefish\\t\\t\\t\\r\\n\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tUpload your photos and videos\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tPictures | \\r\\n\\t\\t\\t\\t\\tVideos | Stamps, Coins Misc. | Google imageCoregonus lavaretus\\r\\n\\t\\t\\t\\t\\tPicture by\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tÃ˜stergaard, T.\\r\\n\\r\\n$(window).load(function() {\\r\\n\\tif($(\\&quot;#ss-photo-full\\&quot;).length &gt; 0){\\t\\t\\t//the photo is full width\\r\\n\\t\\t$(&#39;#image-slider&#39;).swipeslider({sliderHeight:&#39;27%&#39;, autoPlayTimeout: 8000});\\r\\n\\t}else{\\r\\n\\t\\t$(&#39;#image-slider&#39;).swipeslider({sliderHeight:&#39;52%&#39;, autoPlayTimeout: 8000});\\r\\n\\t}\\r\\n});\\r\\n\\r\\n.swipslider{\\r\\n\\tmax-width: 310px;\\r\\n\\tmargin:\\tauto;\\r\\n}\\r\\n\\r\\n\\r\\n\\t\\t\\t \\r\\n\\t\\t \\r\\n\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\t \\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tClassification / Names \\t\\t\\t\\t \\r\\n\\t\\t\\t\\tCommon names | Synonyms | Catalog of Fishes(genus, species) | ITIS | CoL | WoRMS | Cloffa\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tActinopteri (ray-finned fishes) &gt; Salmoniformes (Salmons) &gt; Salmonidae (Salmonids) &gt; Coregoninae\\r\\n\\t\\t\\t\\t\\t\\t\\tEtymology: Coregonus: Greek, kore = pupils of the eye + Greek, gonia = angle (Ref. 45335). More on author: Linnaeus.\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tEnvironment: milieu / climate zone / depth range / distribution range\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tEcology\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFreshwater; brackish; demersal; pH range: 7.0 - 7.5; dH range: 20 - ?; anadromous (Ref. 51243); depth range - m (Ref. ), usually - m (Ref. ). Temperate; 4°C - 16°C (Ref. 2059); 73°N - 40°N\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tDistribution\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCountries | FAO areas | Ecosystems | Occurrences | Point map | Introductions | Faunafri\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tEurope: Native to Lake Bourget (France) and Geneva (Switzerland, France). Population of Lake Aiguebelette (France) apparently introduced, but a &#39;lavaret&#39; had already been recorded from there in the 17th century (Ref. 59043). Other authors assume it to be a superspecies occurring in Great Britain and Alpine areas of Central Europe. Has been stocked into many other places in Europe outside its native range. There are many parallel and wrong scientific names for this species in use because of the problems in classifying the genus Coregonus (Ref. 7495). Appendix III of the Bern Convention (protected fauna). Asia: introduced to Iran (Ref. 39702).\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tLength at first maturity / Size / Weight / Age\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMaturity: Lm27.1, range 40 - ? cm Max length : 73.0 cm TL male/unsexed; (Ref. 40637); max. published weight: 10.0 kg (Ref. 35388)\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tShort description\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMorphology | Morphometrics\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tDorsal\\r\\n\\t\\t\\t\\tspines\\r\\n\\t\\t\\t\\t(total): 3 - 5;\\r\\n\\t\\t\\t\\t\\tDorsal\\r\\n\\t\\t\\t\\t\\tsoft rays\\r\\n\\t\\t\\t\\t\\t(total): 9-12;\\r\\n\\t\\t\\t\\tAnal\\r\\n\\t\\t\\t\\tspines: 3-5;\\r\\n\\t\\t\\t\\t\\tAnal\\r\\n\\t\\t\\t\\t\\tsoft rays: 10 - 13. Mouth small with protruding upper jaw. Silvery body (Ref. 35388). Caudal fin with 19 soft rays (Ref. 40476).\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tBiology\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t \\r\\n\\t\\t\\t\\tGlossary\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t (e.g. epibenthic)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tGregarious lacustrine forms (Ref. 2163) and anadromous estuarine forms, rarely in full saltwater. Feeds on planktonic crustaceans, or larger benthic crustaceans in brackish water. Movement in water column as a function of zooplankton repartition (Ref. 2163). Spawns in gravel, near shore, in shallow water, in December (Ref. 59043). Likely to benefit from environmental regulations in France passed on 8/12/88 (Ref. 2163).\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tLife cycle and mating behavior\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMaturity | Reproduction | Spawning | Eggs | Fecundity | Larvae\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tSpawning takes place at night. \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\r\\n\\r\\n\\t\\t\\t \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMain reference \\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tUpload your references | References | Coordinator | Collaborators\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tWheeler, A., 1992. A list of the common and scientific names of fishes of the British Isles. J. Fish Biol. 41(suppl.A):1-37. (Ref. 5204)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tIUCN Red List Status (Ref. 124695)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Vulnerable (VU) (D2); Date assessed: 01 January 2008\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCITES \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCMS (Ref. 116361)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t \\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tThreat to humans \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Harmless\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tHuman uses \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFisheries: commercial; aquaculture: commercial; gamefish: yes\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFAO - Aquaculture: production; Fisheries: landings; Publication: search | FishSource | Sea Around Us\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tMore information\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCountriesFAO areasEcosystemsOccurrencesIntroductionsStocksEcologyDietFood itemsFood consumptionRation\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCommon namesSynonymsMetabolismPredatorsEcotoxicologyReproductionMaturitySpawningSpawning aggregationFecundityEggsEgg development\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tAge/SizeGrowthLength-weightLength-lengthLength-frequenciesMorphometricsMorphologyLarvaeLarval dynamicsRecruitmentAbundanceBRUVS\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tReferencesAquacultureAquaculture profileStrainsGeneticsAllele frequenciesHeritabilityDiseasesProcessingNutrientsMass conversion\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCollaboratorsPicturesStamps, Coins Misc.SoundsCiguateraSpeedSwim. typeGill areaOtolithsBrainsVision\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tTools\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tE-book | Field guide | Length-frequency wizard | Life-history tool | Point map | \\t\\t\\t\\t\\t\\tClassification Tree\\r\\n\\t\\t\\t\\t\\t\\t | Catch-MSY | \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tSpecial reports\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCheck for Aquarium maintenance | Check for Species Fact Sheets | Check for Aquaculture Fact Sheets\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tDownload XML\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tSummary page | Point data | Common names | Photos\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tInternet sources\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tAFORO (otoliths) | Alien/Invasive Species database | Aquatic Commons | BHL | Cloffa | BOLDSystems | Websites from users | Check FishWatcher | CISTI | Catalog of Fishes: genus, species | DiscoverLife | DORIS | ECOTOX | FAO - Aquaculture: production; Fisheries: landings; Publication: search | Faunafri | Fishipedia | Fishtrace | GenBank: genome, nucleotide | GloBI | Google Books | Google Scholar | Google | IGFA World Record | MitoFish | National databases | Otolith Atlas of Taiwan Fishes | PubMed | Reef Life Survey | Socotra Atlas | Tree of Life | Wikipedia: Go, Search | World Records Freshwater Fishing | Zoological Record\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tEstimates based on models\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tPreferred temperature (Ref. 115969): 5.8 - 12.5, mean 9.5 °C (based on 528 cells).\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tPhylogenetic diversity index (Ref. 82805): PD50 = 0.5000 [Uniqueness, from 0.5 = low to 2.0 = high].\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n \\r\\n \\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBayesian length-weight: a=0.00447 (0.00379 - 0.00526), b=3.21 (3.17 - 3.25), in cm total length, based on LWR estimates for this species (Ref. 93245).\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tTrophic level (Ref. 69278): 3.1 ±0.0 se; based on diet studies.\\t\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tResilience (Ref. 120179): Medium, minimum population doubling time 1.4 - 4.4 years (K=0.3-0.7; tmax=8; Fecundity=3,800 - 169,800).\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tVulnerability (Ref. 59153): Moderate to high vulnerability (51 of 100) .\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPrice category (Ref. 80766): \\t\\t\\t\\t\\t\\tHigh.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tNutrients (Ref. 124155): Calcium = 21.1 [11.5, 46.0] mg/100g ; Iron = 0.245 [0.161, 0.383] mg/100g ; Protein = 18 [17, 19] % ; Omega3 = 1.66 [0.67, 4.60] g/100g ; Selenium = 25.4 [12.0, 66.9] Î¼g/100g ; VitaminA = 5.5 [1.9, 15.6] Î¼g/100g ; Zinc = 0.543 [0.413, 0.702] mg/100g (wet weight); based on nutrient studies.\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t \\r\\n\\r\\n\\t\\t\\r\\n\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tEntered by Luna, Susan M.\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tModified by Sampang-Reyes, Arlene G.\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tFish Forum\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tComments &amp; Corrections\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tSign our Guest Book\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tBack to Search\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tRandom Species\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tBack to Top\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tAccessed through: Not available\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tFishBase mirror site : Kiel, Germany\\r\\n\\t\\t\\t\\t\\t\\t\\tPage last modified by : mrius-barile - 20 July 2016\\r\\n\\t\\t\\r\\n\\t\\r\\n\\t\\t\\r\\n\\tTotal processing time for the page : 0.8598 seconds\\t\\t\\t\\r\\n\\t\\t\\r\\n\\t&quot; ## ## [[6]] ## [1] &quot;\\r\\n\\t\\t\\tCommon name (e.g. trout)\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tcontainsbegins withbegins withends withis\\r\\n\\r\\n\\t\\t\\tGenus + Species (e.g. Gadus morhua)\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t&quot; ## ## [[7]] ## [1] &quot;\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t \\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\t\\t \\r\\n\\t\\r\\n\\t\\r\\n\\t\\tAbout this page\\r\\n\\t\\t\\r\\n\\t\\t\\t\\tMore Info\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tPlus d&#39;info\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tMais info\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tLanguages\\r\\n\\t\\tArabic\\r\\n\\t\\t\\tBahasa/Malay\\r\\n\\t\\t\\tBangla\\r\\n\\t\\t\\tChinese(Si)\\r\\n\\t\\t\\tChinese(Tr)\\r\\n\\t\\t\\tDeutsch\\r\\n\\t\\t\\tEnglish\\r\\n\\t\\t\\tEspaÃ±ol\\r\\n\\t\\t\\tFarsi\\r\\n\\t\\t\\tFranÃ§ais\\r\\n\\t\\t\\tGreek\\r\\n\\t\\t\\tHindi\\r\\n\\t\\t\\tItaliano\\r\\n\\t\\t\\tJapanese\\r\\n\\t\\t\\tLao\\r\\n\\t\\t\\tNederlands\\r\\n\\t\\t\\tPortuguÃªs(Br)\\r\\n\\t\\t\\tPortuguÃªs(Pt)\\r\\n\\t\\t\\tRussian\\r\\n\\t\\t\\tSwedish\\r\\n\\t\\t\\tThai\\r\\n\\t\\t\\tVietnamese\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tUser feedbacks\\r\\n\\t\\tComments &amp; Corrections\\r\\n\\t\\t\\tFish Forum\\r\\n\\t\\t\\tGuest Book\\r\\n\\t\\t\\tFacebook\\r\\n\\t\\t\\t\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tCitation\\r\\n\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tUploads\\r\\n\\t\\tAttach website\\r\\n\\t\\t\\tUpload photo\\r\\n\\t\\t\\tUpload video\\r\\n\\t\\t\\tUpload references\\r\\n\\t\\t\\tFish Watcher\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tRelated species\\r\\n\\t\\tSpecies in Coregonus\\r\\n\\t\\t\\t\\tSpecies in Salmonidae\\r\\n\\t\\t\\t- Classification -Coregoninae\\r\\n\\t\\t\\t\\t\\tSalmonidae\\r\\n\\t\\t\\t\\t\\tSalmoniformes\\r\\n\\t\\t\\t\\t\\tActinopteri\\r\\n\\t\\t\\t\\t\\tChordata\\r\\n\\t\\t\\t\\t\\tAnimalia\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t \\r\\n\\t\\t&quot; ## ## [[8]] ## [1] &quot;\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t \\r\\n\\t\\t\\t\\r\\n\\t\\t\\t&quot; ## ## [[9]] ## [1] &quot;\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\t\\t \\r\\n\\t\\r\\n\\t\\r\\n\\t\\tAbout this page\\r\\n\\t\\t\\r\\n\\t\\t\\t\\tMore Info\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tPlus d&#39;info\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tMais info\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tLanguages\\r\\n\\t\\tArabic\\r\\n\\t\\t\\tBahasa/Malay\\r\\n\\t\\t\\tBangla\\r\\n\\t\\t\\tChinese(Si)\\r\\n\\t\\t\\tChinese(Tr)\\r\\n\\t\\t\\tDeutsch\\r\\n\\t\\t\\tEnglish\\r\\n\\t\\t\\tEspaÃ±ol\\r\\n\\t\\t\\tFarsi\\r\\n\\t\\t\\tFranÃ§ais\\r\\n\\t\\t\\tGreek\\r\\n\\t\\t\\tHindi\\r\\n\\t\\t\\tItaliano\\r\\n\\t\\t\\tJapanese\\r\\n\\t\\t\\tLao\\r\\n\\t\\t\\tNederlands\\r\\n\\t\\t\\tPortuguÃªs(Br)\\r\\n\\t\\t\\tPortuguÃªs(Pt)\\r\\n\\t\\t\\tRussian\\r\\n\\t\\t\\tSwedish\\r\\n\\t\\t\\tThai\\r\\n\\t\\t\\tVietnamese\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tUser feedbacks\\r\\n\\t\\tComments &amp; Corrections\\r\\n\\t\\t\\tFish Forum\\r\\n\\t\\t\\tGuest Book\\r\\n\\t\\t\\tFacebook\\r\\n\\t\\t\\t\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tCitation\\r\\n\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tUploads\\r\\n\\t\\tAttach website\\r\\n\\t\\t\\tUpload photo\\r\\n\\t\\t\\tUpload video\\r\\n\\t\\t\\tUpload references\\r\\n\\t\\t\\tFish Watcher\\r\\n\\t\\t\\r\\n\\r\\n\\t\\r\\n\\t\\tRelated species\\r\\n\\t\\tSpecies in Coregonus\\r\\n\\t\\t\\t\\tSpecies in Salmonidae\\r\\n\\t\\t\\t- Classification -Coregoninae\\r\\n\\t\\t\\t\\t\\tSalmonidae\\r\\n\\t\\t\\t\\t\\tSalmoniformes\\r\\n\\t\\t\\t\\t\\tActinopteri\\r\\n\\t\\t\\t\\t\\tChordata\\r\\n\\t\\t\\t\\t\\tAnimalia\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t &quot; ## ## [[10]] ## [1] &quot;\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tCoregonus lavaretus (Linnaeus, 1758)\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tEuropean whitefish\\t\\t\\t\\r\\n\\t\\t&quot; ## ## [[11]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tUpload your photos and videos\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tPictures | \\r\\n\\t\\t\\t\\t\\tVideos | Stamps, Coins Misc. | Google imageCoregonus lavaretus\\r\\n\\t\\t\\t\\t\\tPicture by\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tÃ˜stergaard, T.\\r\\n\\r\\n$(window).load(function() {\\r\\n\\tif($(\\&quot;#ss-photo-full\\&quot;).length &gt; 0){\\t\\t\\t//the photo is full width\\r\\n\\t\\t$(&#39;#image-slider&#39;).swipeslider({sliderHeight:&#39;27%&#39;, autoPlayTimeout: 8000});\\r\\n\\t}else{\\r\\n\\t\\t$(&#39;#image-slider&#39;).swipeslider({sliderHeight:&#39;52%&#39;, autoPlayTimeout: 8000});\\r\\n\\t}\\r\\n});\\r\\n\\r\\n.swipslider{\\r\\n\\tmax-width: 310px;\\r\\n\\tmargin:\\tauto;\\r\\n}\\r\\n\\r\\n\\r\\n\\t\\t\\t \\r\\n\\t\\t&quot; ## ## [[12]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tUpload your photos and videos\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tPictures | \\r\\n\\t\\t\\t\\t\\tVideos | Stamps, Coins Misc. | Google imageCoregonus lavaretus\\r\\n\\t\\t\\t\\t\\tPicture by\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tÃ˜stergaard, T.\\r\\n\\r\\n$(window).load(function() {\\r\\n\\tif($(\\&quot;#ss-photo-full\\&quot;).length &gt; 0){\\t\\t\\t//the photo is full width\\r\\n\\t\\t$(&#39;#image-slider&#39;).swipeslider({sliderHeight:&#39;27%&#39;, autoPlayTimeout: 8000});\\r\\n\\t}else{\\r\\n\\t\\t$(&#39;#image-slider&#39;).swipeslider({sliderHeight:&#39;52%&#39;, autoPlayTimeout: 8000});\\r\\n\\t}\\r\\n});\\r\\n\\r\\n.swipslider{\\r\\n\\tmax-width: 310px;\\r\\n\\tmargin:\\tauto;\\r\\n}\\r\\n\\r\\n\\r\\n\\t\\t\\t&quot; ## ## [[13]] ## [1] &quot;&quot; ## ## [[14]] ## [1] &quot;\\r\\n\\t\\t\\t \\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tClassification / Names \\t\\t\\t\\t \\r\\n\\t\\t\\t\\tCommon names | Synonyms | Catalog of Fishes(genus, species) | ITIS | CoL | WoRMS | Cloffa\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tActinopteri (ray-finned fishes) &gt; Salmoniformes (Salmons) &gt; Salmonidae (Salmonids) &gt; Coregoninae\\r\\n\\t\\t\\t\\t\\t\\t\\tEtymology: Coregonus: Greek, kore = pupils of the eye + Greek, gonia = angle (Ref. 45335). More on author: Linnaeus.\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tEnvironment: milieu / climate zone / depth range / distribution range\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tEcology\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFreshwater; brackish; demersal; pH range: 7.0 - 7.5; dH range: 20 - ?; anadromous (Ref. 51243); depth range - m (Ref. ), usually - m (Ref. ). Temperate; 4°C - 16°C (Ref. 2059); 73°N - 40°N\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tDistribution\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCountries | FAO areas | Ecosystems | Occurrences | Point map | Introductions | Faunafri\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tEurope: Native to Lake Bourget (France) and Geneva (Switzerland, France). Population of Lake Aiguebelette (France) apparently introduced, but a &#39;lavaret&#39; had already been recorded from there in the 17th century (Ref. 59043). Other authors assume it to be a superspecies occurring in Great Britain and Alpine areas of Central Europe. Has been stocked into many other places in Europe outside its native range. There are many parallel and wrong scientific names for this species in use because of the problems in classifying the genus Coregonus (Ref. 7495). Appendix III of the Bern Convention (protected fauna). Asia: introduced to Iran (Ref. 39702).\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tLength at first maturity / Size / Weight / Age\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMaturity: Lm27.1, range 40 - ? cm Max length : 73.0 cm TL male/unsexed; (Ref. 40637); max. published weight: 10.0 kg (Ref. 35388)\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tShort description\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMorphology | Morphometrics\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tDorsal\\r\\n\\t\\t\\t\\tspines\\r\\n\\t\\t\\t\\t(total): 3 - 5;\\r\\n\\t\\t\\t\\t\\tDorsal\\r\\n\\t\\t\\t\\t\\tsoft rays\\r\\n\\t\\t\\t\\t\\t(total): 9-12;\\r\\n\\t\\t\\t\\tAnal\\r\\n\\t\\t\\t\\tspines: 3-5;\\r\\n\\t\\t\\t\\t\\tAnal\\r\\n\\t\\t\\t\\t\\tsoft rays: 10 - 13. Mouth small with protruding upper jaw. Silvery body (Ref. 35388). Caudal fin with 19 soft rays (Ref. 40476).\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tBiology\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t \\r\\n\\t\\t\\t\\tGlossary\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t (e.g. epibenthic)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tGregarious lacustrine forms (Ref. 2163) and anadromous estuarine forms, rarely in full saltwater. Feeds on planktonic crustaceans, or larger benthic crustaceans in brackish water. Movement in water column as a function of zooplankton repartition (Ref. 2163). Spawns in gravel, near shore, in shallow water, in December (Ref. 59043). Likely to benefit from environmental regulations in France passed on 8/12/88 (Ref. 2163).\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tLife cycle and mating behavior\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMaturity | Reproduction | Spawning | Eggs | Fecundity | Larvae\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tSpawning takes place at night. \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\r\\n\\r\\n\\t\\t\\t \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMain reference \\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tUpload your references | References | Coordinator | Collaborators\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tWheeler, A., 1992. A list of the common and scientific names of fishes of the British Isles. J. Fish Biol. 41(suppl.A):1-37. (Ref. 5204)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tIUCN Red List Status (Ref. 124695)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Vulnerable (VU) (D2); Date assessed: 01 January 2008\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCITES \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCMS (Ref. 116361)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t \\r\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tThreat to humans \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Harmless\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tHuman uses \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFisheries: commercial; aquaculture: commercial; gamefish: yes\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFAO - Aquaculture: production; Fisheries: landings; Publication: search | FishSource | Sea Around Us\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tMore information\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCountriesFAO areasEcosystemsOccurrencesIntroductionsStocksEcologyDietFood itemsFood consumptionRation\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCommon namesSynonymsMetabolismPredatorsEcotoxicologyReproductionMaturitySpawningSpawning aggregationFecundityEggsEgg development\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tAge/SizeGrowthLength-weightLength-lengthLength-frequenciesMorphometricsMorphologyLarvaeLarval dynamicsRecruitmentAbundanceBRUVS\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tReferencesAquacultureAquaculture profileStrainsGeneticsAllele frequenciesHeritabilityDiseasesProcessingNutrientsMass conversion\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCollaboratorsPicturesStamps, Coins Misc.SoundsCiguateraSpeedSwim. typeGill areaOtolithsBrainsVision\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tTools\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tE-book | Field guide | Length-frequency wizard | Life-history tool | Point map | \\t\\t\\t\\t\\t\\tClassification Tree\\r\\n\\t\\t\\t\\t\\t\\t | Catch-MSY | \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tSpecial reports\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCheck for Aquarium maintenance | Check for Species Fact Sheets | Check for Aquaculture Fact Sheets\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tDownload XML\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tSummary page | Point data | Common names | Photos\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tInternet sources\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tAFORO (otoliths) | Alien/Invasive Species database | Aquatic Commons | BHL | Cloffa | BOLDSystems | Websites from users | Check FishWatcher | CISTI | Catalog of Fishes: genus, species | DiscoverLife | DORIS | ECOTOX | FAO - Aquaculture: production; Fisheries: landings; Publication: search | Faunafri | Fishipedia | Fishtrace | GenBank: genome, nucleotide | GloBI | Google Books | Google Scholar | Google | IGFA World Record | MitoFish | National databases | Otolith Atlas of Taiwan Fishes | PubMed | Reef Life Survey | Socotra Atlas | Tree of Life | Wikipedia: Go, Search | World Records Freshwater Fishing | Zoological Record\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tEstimates based on models\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tPreferred temperature (Ref. 115969): 5.8 - 12.5, mean 9.5 °C (based on 528 cells).\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tPhylogenetic diversity index (Ref. 82805): PD50 = 0.5000 [Uniqueness, from 0.5 = low to 2.0 = high].\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n \\r\\n \\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBayesian length-weight: a=0.00447 (0.00379 - 0.00526), b=3.21 (3.17 - 3.25), in cm total length, based on LWR estimates for this species (Ref. 93245).\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tTrophic level (Ref. 69278): 3.1 ±0.0 se; based on diet studies.\\t\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tResilience (Ref. 120179): Medium, minimum population doubling time 1.4 - 4.4 years (K=0.3-0.7; tmax=8; Fecundity=3,800 - 169,800).\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tVulnerability (Ref. 59153): Moderate to high vulnerability (51 of 100) .\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPrice category (Ref. 80766): \\t\\t\\t\\t\\t\\tHigh.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tNutrients (Ref. 124155): Calcium = 21.1 [11.5, 46.0] mg/100g ; Iron = 0.245 [0.161, 0.383] mg/100g ; Protein = 18 [17, 19] % ; Omega3 = 1.66 [0.67, 4.60] g/100g ; Selenium = 25.4 [12.0, 66.9] Î¼g/100g ; VitaminA = 5.5 [1.9, 15.6] Î¼g/100g ; Zinc = 0.543 [0.413, 0.702] mg/100g (wet weight); based on nutrient studies.\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t&quot; ## ## [[15]] ## [1] &quot;\\r\\n\\t\\t\\t\\tActinopteri (ray-finned fishes) &gt; Salmoniformes (Salmons) &gt; Salmonidae (Salmonids) &gt; Coregoninae\\r\\n\\t\\t\\t\\t\\t\\t\\tEtymology: Coregonus: Greek, kore = pupils of the eye + Greek, gonia = angle (Ref. 45335). More on author: Linnaeus.\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t&quot; ## ## [[16]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFreshwater; brackish; demersal; pH range: 7.0 - 7.5; dH range: 20 - ?; anadromous (Ref. 51243); depth range - m (Ref. ), usually - m (Ref. ). Temperate; 4°C - 16°C (Ref. 2059); 73°N - 40°N\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[17]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tEurope: Native to Lake Bourget (France) and Geneva (Switzerland, France). Population of Lake Aiguebelette (France) apparently introduced, but a &#39;lavaret&#39; had already been recorded from there in the 17th century (Ref. 59043). Other authors assume it to be a superspecies occurring in Great Britain and Alpine areas of Central Europe. Has been stocked into many other places in Europe outside its native range. There are many parallel and wrong scientific names for this species in use because of the problems in classifying the genus Coregonus (Ref. 7495). Appendix III of the Bern Convention (protected fauna). Asia: introduced to Iran (Ref. 39702).\\r\\n\\t\\t\\t\\t&quot; ## ## [[18]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tMaturity: Lm27.1, range 40 - ? cm Max length : 73.0 cm TL male/unsexed; (Ref. 40637); max. published weight: 10.0 kg (Ref. 35388)\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[19]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tDorsal\\r\\n\\t\\t\\t\\tspines\\r\\n\\t\\t\\t\\t(total): 3 - 5;\\r\\n\\t\\t\\t\\t\\tDorsal\\r\\n\\t\\t\\t\\t\\tsoft rays\\r\\n\\t\\t\\t\\t\\t(total): 9-12;\\r\\n\\t\\t\\t\\tAnal\\r\\n\\t\\t\\t\\tspines: 3-5;\\r\\n\\t\\t\\t\\t\\tAnal\\r\\n\\t\\t\\t\\t\\tsoft rays: 10 - 13. Mouth small with protruding upper jaw. Silvery body (Ref. 35388). Caudal fin with 19 soft rays (Ref. 40476).\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[20]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tBiology\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t \\r\\n\\t\\t\\t\\tGlossary\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t (e.g. epibenthic)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t&quot; ## ## [[21]] ## [1] &quot;\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tGregarious lacustrine forms (Ref. 2163) and anadromous estuarine forms, rarely in full saltwater. Feeds on planktonic crustaceans, or larger benthic crustaceans in brackish water. Movement in water column as a function of zooplankton repartition (Ref. 2163). Spawns in gravel, near shore, in shallow water, in December (Ref. 59043). Likely to benefit from environmental regulations in France passed on 8/12/88 (Ref. 2163).\\t\\t\\t\\r\\n\\t\\t\\t&quot; ## ## [[22]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tSpawning takes place at night. \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[23]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tWheeler, A., 1992. A list of the common and scientific names of fishes of the British Isles. J. Fish Biol. 41(suppl.A):1-37. (Ref. 5204)\\r\\n\\t\\t\\t\\t&quot; ## ## [[24]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tIUCN Red List Status (Ref. 124695)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Vulnerable (VU) (D2); Date assessed: 01 January 2008\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[25]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Vulnerable (VU) (D2); Date assessed: 01 January 2008\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[26]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCITES \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[27]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[28]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCMS (Ref. 116361)\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[29]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tNot Evaluated\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[30]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tThreat to humans \\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Harmless\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t &quot; ## ## [[31]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t Harmless\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[32]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFisheries: commercial; aquaculture: commercial; gamefish: yes\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[33]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tFAO - Aquaculture: production; Fisheries: landings; Publication: search | FishSource | Sea Around Us\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[34]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tMore information\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCountriesFAO areasEcosystemsOccurrencesIntroductionsStocksEcologyDietFood itemsFood consumptionRation\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCommon namesSynonymsMetabolismPredatorsEcotoxicologyReproductionMaturitySpawningSpawning aggregationFecundityEggsEgg development\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tAge/SizeGrowthLength-weightLength-lengthLength-frequenciesMorphometricsMorphologyLarvaeLarval dynamicsRecruitmentAbundanceBRUVS\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tReferencesAquacultureAquaculture profileStrainsGeneticsAllele frequenciesHeritabilityDiseasesProcessingNutrientsMass conversion\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tCollaboratorsPicturesStamps, Coins Misc.SoundsCiguateraSpeedSwim. typeGill areaOtolithsBrainsVision\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t&quot; ## ## [[35]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\tCountriesFAO areasEcosystemsOccurrencesIntroductionsStocksEcologyDietFood itemsFood consumptionRation&quot; ## ## [[36]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\tCommon namesSynonymsMetabolismPredatorsEcotoxicologyReproductionMaturitySpawningSpawning aggregationFecundityEggsEgg development&quot; ## ## [[37]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\tAge/SizeGrowthLength-weightLength-lengthLength-frequenciesMorphometricsMorphologyLarvaeLarval dynamicsRecruitmentAbundanceBRUVS&quot; ## ## [[38]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\tReferencesAquacultureAquaculture profileStrainsGeneticsAllele frequenciesHeritabilityDiseasesProcessingNutrientsMass conversion&quot; ## ## [[39]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\tCollaboratorsPicturesStamps, Coins Misc.SoundsCiguateraSpeedSwim. typeGill areaOtolithsBrainsVision&quot; ## ## [[40]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tE-book | Field guide | Length-frequency wizard | Life-history tool | Point map | \\t\\t\\t\\t\\t\\tClassification Tree\\r\\n\\t\\t\\t\\t\\t\\t | Catch-MSY | \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[41]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tCheck for Aquarium maintenance | Check for Species Fact Sheets | Check for Aquaculture Fact Sheets\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[42]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tSummary page | Point data | Common names | Photos\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[43]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tAFORO (otoliths) | Alien/Invasive Species database | Aquatic Commons | BHL | Cloffa | BOLDSystems | Websites from users | Check FishWatcher | CISTI | Catalog of Fishes: genus, species | DiscoverLife | DORIS | ECOTOX | FAO - Aquaculture: production; Fisheries: landings; Publication: search | Faunafri | Fishipedia | Fishtrace | GenBank: genome, nucleotide | GloBI | Google Books | Google Scholar | Google | IGFA World Record | MitoFish | National databases | Otolith Atlas of Taiwan Fishes | PubMed | Reef Life Survey | Socotra Atlas | Tree of Life | Wikipedia: Go, Search | World Records Freshwater Fishing | Zoological Record\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[44]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tPreferred temperature (Ref. 115969): 5.8 - 12.5, mean 9.5 °C (based on 528 cells).\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tPhylogenetic diversity index (Ref. 82805): PD50 = 0.5000 [Uniqueness, from 0.5 = low to 2.0 = high].\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n \\r\\n \\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBayesian length-weight: a=0.00447 (0.00379 - 0.00526), b=3.21 (3.17 - 3.25), in cm total length, based on LWR estimates for this species (Ref. 93245).\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n \\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tTrophic level (Ref. 69278): 3.1 ±0.0 se; based on diet studies.\\t\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tResilience (Ref. 120179): Medium, minimum population doubling time 1.4 - 4.4 years (K=0.3-0.7; tmax=8; Fecundity=3,800 - 169,800).\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\tVulnerability (Ref. 59153): Moderate to high vulnerability (51 of 100) .\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPrice category (Ref. 80766): \\t\\t\\t\\t\\t\\tHigh.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t&quot; ## ## [[45]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\tPhylogenetic diversity index (Ref. 82805): PD50 = 0.5000 [Uniqueness, from 0.5 = low to 2.0 = high].\\t\\t\\t\\t\\t&quot; ## ## [[46]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBayesian length-weight: a=0.00447 (0.00379 - 0.00526), b=3.21 (3.17 - 3.25), in cm total length, based on LWR estimates for this species (Ref. 93245).\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t&quot; ## ## [[47]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\tTrophic level (Ref. 69278): 3.1 ±0.0 se; based on diet studies.\\t\\t\\t\\t\\t&quot; ## ## [[48]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t&quot; ## ## [[49]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\tResilience (Ref. 120179): Medium, minimum population doubling time 1.4 - 4.4 years (K=0.3-0.7; tmax=8; Fecundity=3,800 - 169,800).\\t\\t\\t\\t\\t&quot; ## ## [[50]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t&quot; ## ## [[51]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\tVulnerability (Ref. 59153): Moderate to high vulnerability (51 of 100) .\\t\\t\\t\\t\\t&quot; ## ## [[52]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPrice category (Ref. 80766): \\t\\t\\t\\t\\t\\tHigh.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t&quot; ## ## [[53]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\tNutrients (Ref. 124155): Calcium = 21.1 [11.5, 46.0] mg/100g ; Iron = 0.245 [0.161, 0.383] mg/100g ; Protein = 18 [17, 19] % ; Omega3 = 1.66 [0.67, 4.60] g/100g ; Selenium = 25.4 [12.0, 66.9] Î¼g/100g ; VitaminA = 5.5 [1.9, 15.6] Î¼g/100g ; Zinc = 0.543 [0.413, 0.702] mg/100g (wet weight); based on nutrient studies.\\t\\t\\t\\t\\t&quot; ## ## [[54]] ## [1] &quot;\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tEntered by Luna, Susan M.\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tModified by Sampang-Reyes, Arlene G.\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tFish Forum\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tComments &amp; Corrections\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tSign our Guest Book\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tBack to Search\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tRandom Species\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tBack to Top\\t\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\tAccessed through: Not available\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tFishBase mirror site : Kiel, Germany\\r\\n\\t\\t\\t\\t\\t\\t\\tPage last modified by : mrius-barile - 20 July 2016\\r\\n\\t\\t\\r\\n\\t\\r\\n\\t\\t\\r\\n\\tTotal processing time for the page : 0.8598 seconds\\t\\t\\t\\r\\n\\t\\t&quot; ## ## [[55]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tEntered by Luna, Susan M.\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tModified by Sampang-Reyes, Arlene G.\\t\\t\\t\\t\\r\\n\\t\\t\\t&quot; ## ## [[56]] ## [1] &quot;\\r\\n\\t\\t\\t\\tEntered by Luna, Susan M.\\t\\t\\t\\t&quot; ## ## [[57]] ## [1] &quot;\\r\\n\\t\\t\\t\\tModified by Sampang-Reyes, Arlene G.\\t\\t\\t\\t&quot; ## ## [[58]] ## [1] &quot;\\r\\n\\t\\t\\t\\tFish Forum\\t\\t\\t&quot; ## ## [[59]] ## [1] &quot;\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\tComments &amp; Corrections\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t&quot; ## ## [[60]] ## [1] &quot;\\r\\n\\t\\t\\t\\tSign our Guest Book\\t\\t\\t&quot; ## ## [[61]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tBack to Search\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tRandom Species\\t\\t\\t\\t\\r\\n\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tBack to Top\\t\\t\\t\\t\\r\\n\\t\\t\\t&quot; ## ## [[62]] ## [1] &quot;\\r\\n\\t\\t\\t\\tBack to Search\\t\\t\\t\\t&quot; ## ## [[63]] ## [1] &quot;\\r\\n\\t\\t\\t\\tRandom Species\\t\\t\\t\\t&quot; ## ## [[64]] ## [1] &quot;\\r\\n\\t\\t\\t\\tBack to Top\\t\\t\\t\\t&quot; ## ## [[65]] ## [1] &quot;\\r\\n\\t\\t\\t\\tAccessed through: Not available\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tFishBase mirror site : Kiel, Germany\\r\\n\\t\\t\\t\\t\\t\\t\\tPage last modified by : mrius-barile - 20 July 2016\\r\\n\\t\\t\\r\\n\\t\\r\\n\\t\\t\\r\\n\\tTotal processing time for the page : 0.8598 seconds\\t\\t\\t&quot; ## ## [[66]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tFishBase mirror site : Kiel, Germany\\r\\n\\t\\t\\t\\t\\t\\t\\t&quot; Next we look for the pattern “Price category” in the variable values_nodes. Then we look at which position we can find our information. values_pattern &lt;- sapply(values_nodes, function(x){regexec(pattern=&quot;Price category&quot;, x)[[1]][1]}) values_pattern ## [1] -1 -1 -1 -1 8632 -1 -1 -1 -1 -1 -1 -1 -1 6896 -1 ## [16] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ## [31] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 950 -1 ## [46] -1 -1 -1 -1 -1 -1 14 -1 -1 -1 -1 -1 -1 -1 -1 ## [61] -1 -1 -1 -1 -1 -1 w_Price &lt;- which(values_pattern &gt; 0) w_Price ## [1] 5 14 44 52 Now we can look at the informations contained at the found positions. If w_Price is empty, then we set the price category as NA. Otherwise we get the value at the foud positions using the function xmlValue(). if(length(w_Price)==0){ Price=NA } else { d1_Price &lt;- xmlValue(c_div[[w_Price[length(w_Price)]]]) } d1_Price ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPrice category (Ref. 80766): \\t\\t\\t\\t\\t\\tHigh.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t&quot; Next we can extract the relevant information from d1_price. We can use the function strsplit() to obtain the part of the string after “:”. We need to take the second element in the first position in the list. We now use the function regmatches to extract matched substrings from match data obtained by gregexpr. We get a list with one element, so we can extract this element from the list. Now we use the function gregexpr() to search for alphabetic characters. We then use the function regmatches to extract matched substrings from match data obtained by gregexpr which gives a list with one element, so we can extract this element from the list. (int &lt;- strsplit(d1_Price,&quot;:&quot;)) ## [[1]] ## [1] &quot;\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPrice category (Ref. 80766)&quot; ## [2] &quot; \\t\\t\\t\\t\\t\\tHigh.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t&quot; (int &lt;- int[[1]][2]) ## [1] &quot; \\t\\t\\t\\t\\t\\tHigh.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t&quot; alph_char &lt;- gregexpr(pattern= &quot;[[:alpha:]]+&quot;,int) # find starting position and length of all matches (Price &lt;- regmatches(int, alph_char)) ## [[1]] ## [1] &quot;High&quot; (Price &lt;- Price[[1]]) ## [1] &quot;High&quot; Or alternatively to this entire web scraping part, we can directly use the rfishbase package (Boettiger, Temple Lang, and Wainwright 2012): library(rfishbase) species(&quot;Coregonus lavaretus&quot;, &quot;PriceCateg&quot;) ## # A tibble: 1 × 1 ## PriceCateg ## &lt;chr&gt; ## 1 high Using both the methods we can get the price category of the Coregonus lavaretus and it gives the same results. Using an API to fetch data from the web is very handy as we can see in the example above whereas web scraping is bit tideous. 5.5 Tidyverse’s filter and select 5.5.1 Introduction Writing code is learning a language! - Learn the grammar of a function (i.e. sentence) - Learn how to connect functions - Learn how to express your thoughts - Learn how to get translations from the web (Google is your best friend!) Let’s load tidyverse and have a look at the star wars data: library(tidyverse) dat &lt;- starwars ncol(dat) # Number of Columns ## [1] 14 nrow(dat) # Number of Rows ## [1] 87 dim(dat) # Dimension (Rows x Columns) ## [1] 87 14 colnames(dat) # Look at variables in dat in tidyverse: dat %&gt;% names(.) ## [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; &quot;skin_color&quot; ## [6] &quot;eye_color&quot; &quot;birth_year&quot; &quot;sex&quot; &quot;gender&quot; &quot;homeworld&quot; ## [11] &quot;species&quot; &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; head(dat) # Look at top 6 entries of dat in tidyverse: dat %&gt;% head() ## # A tibble: 6 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… 172 77 blond fair blue 19 male mascu… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 4 Darth V… 202 136 none white yellow 41.9 male mascu… ## 5 Leia Or… 150 49 brown light brown 19 fema… femin… ## 6 Owen La… 178 120 brown, grey light blue 52 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; tail(dat, 10) # Look at last 10 entries of dat in tidyverse: dat %&gt;% tail() ## # A tibble: 10 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Tarfful 234 136 brown brown blue NA male mascu… ## 2 Raymus … 188 79 brown light brown NA male mascu… ## 3 Sly Moo… 178 48 none pale white NA &lt;NA&gt; &lt;NA&gt; ## 4 Tion Me… 206 80 none grey black NA male mascu… ## 5 Finn NA NA black dark dark NA male mascu… ## 6 Rey NA NA brown light hazel NA fema… femin… ## 7 Poe Dam… NA NA brown light brown NA male mascu… ## 8 BB8 NA NA none none black NA none mascu… ## 9 Captain… NA NA unknown unknown unknown NA &lt;NA&gt; &lt;NA&gt; ## 10 Padmé A… 165 45 brown light brown 46 fema… femin… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; tail(names(dat)) # Look at last 6 columns of dat in tidyverse: dat %&gt;% tail(names(.)) ## [1] &quot;gender&quot; &quot;homeworld&quot; &quot;species&quot; &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; dat$films[1] # Accessing the entire list of the first entry ## [[1]] ## [1] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; ## [3] &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ## [5] &quot;The Force Awakens&quot; dat$films[[1]] # Accessing the entire list of the first entry ## [1] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; ## [3] &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ## [5] &quot;The Force Awakens&quot; dat$films[[1]][3] # Accessing the third object in the list of the first entry ## [1] &quot;Return of the Jedi&quot; 5.5.2 Select() select() is a straightforward to pick your varialbes (features) of interes. It can be used in various ways, even by adding functions and operators to its arguments. # When doing selections and filters: Always save your data frame using &quot;&lt;-&quot; new_df &lt;- dat %&gt;% select(1:5) ## Specific dat %&gt;% select(1,2) # select by column number ## # A tibble: 87 × 2 ## name height ## &lt;chr&gt; &lt;int&gt; ## 1 Luke Skywalker 172 ## 2 C-3PO 167 ## 3 R2-D2 96 ## 4 Darth Vader 202 ## 5 Leia Organa 150 ## 6 Owen Lars 178 ## 7 Beru Whitesun lars 165 ## 8 R5-D4 97 ## 9 Biggs Darklighter 183 ## 10 Obi-Wan Kenobi 182 ## # … with 77 more rows dat %&gt;% select(2,1) # rearrange columns ## # A tibble: 87 × 2 ## height name ## &lt;int&gt; &lt;chr&gt; ## 1 172 Luke Skywalker ## 2 167 C-3PO ## 3 96 R2-D2 ## 4 202 Darth Vader ## 5 150 Leia Organa ## 6 178 Owen Lars ## 7 165 Beru Whitesun lars ## 8 97 R5-D4 ## 9 183 Biggs Darklighter ## 10 182 Obi-Wan Kenobi ## # … with 77 more rows dat %&gt;% select(name, height, starships) # select by names ## # A tibble: 87 × 3 ## name height starships ## &lt;chr&gt; &lt;int&gt; &lt;list&gt; ## 1 Luke Skywalker 172 &lt;chr [2]&gt; ## 2 C-3PO 167 &lt;chr [0]&gt; ## 3 R2-D2 96 &lt;chr [0]&gt; ## 4 Darth Vader 202 &lt;chr [1]&gt; ## 5 Leia Organa 150 &lt;chr [0]&gt; ## 6 Owen Lars 178 &lt;chr [0]&gt; ## 7 Beru Whitesun lars 165 &lt;chr [0]&gt; ## 8 R5-D4 97 &lt;chr [0]&gt; ## 9 Biggs Darklighter 183 &lt;chr [1]&gt; ## 10 Obi-Wan Kenobi 182 &lt;chr [5]&gt; ## # … with 77 more rows dat %&gt;% select(H = height, N = name) # renaming Variables ## # A tibble: 87 × 2 ## H N ## &lt;int&gt; &lt;chr&gt; ## 1 172 Luke Skywalker ## 2 167 C-3PO ## 3 96 R2-D2 ## 4 202 Darth Vader ## 5 150 Leia Organa ## 6 178 Owen Lars ## 7 165 Beru Whitesun lars ## 8 97 R5-D4 ## 9 183 Biggs Darklighter ## 10 182 Obi-Wan Kenobi ## # … with 77 more rows # Backwards dat %&gt;% select(tail(names(dat), 2)) # select the last two variables ## # A tibble: 87 × 2 ## vehicles starships ## &lt;list&gt; &lt;list&gt; ## 1 &lt;chr [2]&gt; &lt;chr [2]&gt; ## 2 &lt;chr [0]&gt; &lt;chr [0]&gt; ## 3 &lt;chr [0]&gt; &lt;chr [0]&gt; ## 4 &lt;chr [0]&gt; &lt;chr [1]&gt; ## 5 &lt;chr [1]&gt; &lt;chr [0]&gt; ## 6 &lt;chr [0]&gt; &lt;chr [0]&gt; ## 7 &lt;chr [0]&gt; &lt;chr [0]&gt; ## 8 &lt;chr [0]&gt; &lt;chr [0]&gt; ## 9 &lt;chr [0]&gt; &lt;chr [1]&gt; ## 10 &lt;chr [1]&gt; &lt;chr [5]&gt; ## # … with 77 more rows ## Deletion dat %&gt;% select(-name) # using a minus sign for deleting column name ## # A tibble: 87 × 13 ## height mass hair_color skin_color eye_color birth_year sex gender ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 172 77 blond fair blue 19 male masculine ## 2 167 75 &lt;NA&gt; gold yellow 112 none masculine ## 3 96 32 &lt;NA&gt; white, blue red 33 none masculine ## 4 202 136 none white yellow 41.9 male masculine ## 5 150 49 brown light brown 19 female feminine ## 6 178 120 brown, grey light blue 52 male masculine ## 7 165 75 brown light blue 47 female feminine ## 8 97 32 &lt;NA&gt; white, red red NA none masculine ## 9 183 84 black light brown 24 male masculine ## 10 182 77 auburn, white fair blue-gray 57 male masculine ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; dat %&gt;% select(-(1:4)) # same as select((5:length(dat))) ## # A tibble: 87 × 10 ## skin_color eye_color birth_year sex gender homeworld species films vehicles ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;list&gt; ## 1 fair blue 19 male mascu… Tatooine Human &lt;chr… &lt;chr [2… ## 2 gold yellow 112 none mascu… Tatooine Droid &lt;chr… &lt;chr [0… ## 3 white, bl… red 33 none mascu… Naboo Droid &lt;chr… &lt;chr [0… ## 4 white yellow 41.9 male mascu… Tatooine Human &lt;chr… &lt;chr [0… ## 5 light brown 19 fema… femin… Alderaan Human &lt;chr… &lt;chr [1… ## 6 light blue 52 male mascu… Tatooine Human &lt;chr… &lt;chr [0… ## 7 light blue 47 fema… femin… Tatooine Human &lt;chr… &lt;chr [0… ## 8 white, red red NA none mascu… Tatooine Droid &lt;chr… &lt;chr [0… ## 9 light brown 24 male mascu… Tatooine Human &lt;chr… &lt;chr [0… ## 10 fair blue-gray 57 male mascu… Stewjon Human &lt;chr… &lt;chr [1… ## # … with 77 more rows, and 1 more variable: starships &lt;list&gt; ## Range dat %&gt;% select(1:4) # 1:4 is a vector from 1 to 4 ## # A tibble: 87 × 4 ## name height mass hair_color ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalker 172 77 blond ## 2 C-3PO 167 75 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; ## 4 Darth Vader 202 136 none ## 5 Leia Organa 150 49 brown ## 6 Owen Lars 178 120 brown, grey ## 7 Beru Whitesun lars 165 75 brown ## 8 R5-D4 97 32 &lt;NA&gt; ## 9 Biggs Darklighter 183 84 black ## 10 Obi-Wan Kenobi 182 77 auburn, white ## # … with 77 more rows dat %&gt;% select(seq(1, length(dat), 2)) # seq() creates a sequence; selects every 2nd column ## # A tibble: 87 × 7 ## name mass skin_color birth_year gender species vehicles ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 Luke Skywalker 77 fair 19 masculine Human &lt;chr [2]&gt; ## 2 C-3PO 75 gold 112 masculine Droid &lt;chr [0]&gt; ## 3 R2-D2 32 white, blue 33 masculine Droid &lt;chr [0]&gt; ## 4 Darth Vader 136 white 41.9 masculine Human &lt;chr [0]&gt; ## 5 Leia Organa 49 light 19 feminine Human &lt;chr [1]&gt; ## 6 Owen Lars 120 light 52 masculine Human &lt;chr [0]&gt; ## 7 Beru Whitesun lars 75 light 47 feminine Human &lt;chr [0]&gt; ## 8 R5-D4 32 white, red NA masculine Droid &lt;chr [0]&gt; ## 9 Biggs Darklighter 84 light 24 masculine Human &lt;chr [0]&gt; ## 10 Obi-Wan Kenobi 77 fair 57 masculine Human &lt;chr [1]&gt; ## # … with 77 more rows Advanced: Using selection helpers (additional functions). More examples can be found here. dat %&gt;% select(where(is.numeric)) ## # A tibble: 87 × 3 ## height mass birth_year ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 172 77 19 ## 2 167 75 112 ## 3 96 32 33 ## 4 202 136 41.9 ## 5 150 49 19 ## 6 178 120 52 ## 7 165 75 47 ## 8 97 32 NA ## 9 183 84 24 ## 10 182 77 57 ## # … with 77 more rows dat %&gt;% select(last_col(0:2)) # Last 3 columns ## # A tibble: 87 × 3 ## starships vehicles films ## &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;chr [2]&gt; &lt;chr [2]&gt; &lt;chr [5]&gt; ## 2 &lt;chr [0]&gt; &lt;chr [0]&gt; &lt;chr [6]&gt; ## 3 &lt;chr [0]&gt; &lt;chr [0]&gt; &lt;chr [7]&gt; ## 4 &lt;chr [1]&gt; &lt;chr [0]&gt; &lt;chr [4]&gt; ## 5 &lt;chr [0]&gt; &lt;chr [1]&gt; &lt;chr [5]&gt; ## 6 &lt;chr [0]&gt; &lt;chr [0]&gt; &lt;chr [3]&gt; ## 7 &lt;chr [0]&gt; &lt;chr [0]&gt; &lt;chr [3]&gt; ## 8 &lt;chr [0]&gt; &lt;chr [0]&gt; &lt;chr [1]&gt; ## 9 &lt;chr [1]&gt; &lt;chr [0]&gt; &lt;chr [1]&gt; ## 10 &lt;chr [5]&gt; &lt;chr [1]&gt; &lt;chr [6]&gt; ## # … with 77 more rows dat %&gt;% select(last_col(2:0)) # Careful last_col() inedexes last column as 0 ## # A tibble: 87 × 3 ## films vehicles starships ## &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;chr [5]&gt; &lt;chr [2]&gt; &lt;chr [2]&gt; ## 2 &lt;chr [6]&gt; &lt;chr [0]&gt; &lt;chr [0]&gt; ## 3 &lt;chr [7]&gt; &lt;chr [0]&gt; &lt;chr [0]&gt; ## 4 &lt;chr [4]&gt; &lt;chr [0]&gt; &lt;chr [1]&gt; ## 5 &lt;chr [5]&gt; &lt;chr [1]&gt; &lt;chr [0]&gt; ## 6 &lt;chr [3]&gt; &lt;chr [0]&gt; &lt;chr [0]&gt; ## 7 &lt;chr [3]&gt; &lt;chr [0]&gt; &lt;chr [0]&gt; ## 8 &lt;chr [1]&gt; &lt;chr [0]&gt; &lt;chr [0]&gt; ## 9 &lt;chr [1]&gt; &lt;chr [0]&gt; &lt;chr [1]&gt; ## 10 &lt;chr [6]&gt; &lt;chr [1]&gt; &lt;chr [5]&gt; ## # … with 77 more rows dat %&gt;% select(starts_with(&quot;h&quot;)) # Selects all variables that start with an &quot;h&quot; ## # A tibble: 87 × 3 ## height hair_color homeworld ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 172 blond Tatooine ## 2 167 &lt;NA&gt; Tatooine ## 3 96 &lt;NA&gt; Naboo ## 4 202 none Tatooine ## 5 150 brown Alderaan ## 6 178 brown, grey Tatooine ## 7 165 brown Tatooine ## 8 97 &lt;NA&gt; Tatooine ## 9 183 black Tatooine ## 10 182 auburn, white Stewjon ## # … with 77 more rows dat %&gt;% select(ends_with(&quot;color&quot;)) # Selects all variables that end with &quot;color&quot; ## # A tibble: 87 × 3 ## hair_color skin_color eye_color ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 blond fair blue ## 2 &lt;NA&gt; gold yellow ## 3 &lt;NA&gt; white, blue red ## 4 none white yellow ## 5 brown light brown ## 6 brown, grey light blue ## 7 brown light blue ## 8 &lt;NA&gt; white, red red ## 9 black light brown ## 10 auburn, white fair blue-gray ## # … with 77 more rows dat %&gt;% select(contains(&quot;_&quot;)) ## # A tibble: 87 × 4 ## hair_color skin_color eye_color birth_year ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blond fair blue 19 ## 2 &lt;NA&gt; gold yellow 112 ## 3 &lt;NA&gt; white, blue red 33 ## 4 none white yellow 41.9 ## 5 brown light brown 19 ## 6 brown, grey light blue 52 ## 7 brown light blue 47 ## 8 &lt;NA&gt; white, red red NA ## 9 black light brown 24 ## 10 auburn, white fair blue-gray 57 ## # … with 77 more rows dat %&gt;% select(matches(&quot;height&quot;)) # Same as select(height) ## # A tibble: 87 × 1 ## height ## &lt;int&gt; ## 1 172 ## 2 167 ## 3 96 ## 4 202 ## 5 150 ## 6 178 ## 7 165 ## 8 97 ## 9 183 ## 10 182 ## # … with 77 more rows dat %&gt;% select(-any_of(ends_with(&quot;color&quot;))) # Delete any variables that end with &quot;color&quot;. Note that any_of can only be used within select()! ## # A tibble: 87 × 11 ## name height mass birth_year sex gender homeworld species films vehicles ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;list&gt; ## 1 Luke S… 172 77 19 male mascu… Tatooine Human &lt;chr… &lt;chr [2… ## 2 C-3PO 167 75 112 none mascu… Tatooine Droid &lt;chr… &lt;chr [0… ## 3 R2-D2 96 32 33 none mascu… Naboo Droid &lt;chr… &lt;chr [0… ## 4 Darth … 202 136 41.9 male mascu… Tatooine Human &lt;chr… &lt;chr [0… ## 5 Leia O… 150 49 19 fema… femin… Alderaan Human &lt;chr… &lt;chr [1… ## 6 Owen L… 178 120 52 male mascu… Tatooine Human &lt;chr… &lt;chr [0… ## 7 Beru W… 165 75 47 fema… femin… Tatooine Human &lt;chr… &lt;chr [0… ## 8 R5-D4 97 32 NA none mascu… Tatooine Droid &lt;chr… &lt;chr [0… ## 9 Biggs … 183 84 24 male mascu… Tatooine Human &lt;chr… &lt;chr [0… ## 10 Obi-Wa… 182 77 57 male mascu… Stewjon Human &lt;chr… &lt;chr [1… ## # … with 77 more rows, and 1 more variable: starships &lt;list&gt; # (Using conditions is rarely meaningful...) dat %&gt;% select(1:4 | length(dat)) # Select columns one to for &quot;or&quot; the last one ## # A tibble: 87 × 5 ## name height mass hair_color starships ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;list&gt; ## 1 Luke Skywalker 172 77 blond &lt;chr [2]&gt; ## 2 C-3PO 167 75 &lt;NA&gt; &lt;chr [0]&gt; ## 3 R2-D2 96 32 &lt;NA&gt; &lt;chr [0]&gt; ## 4 Darth Vader 202 136 none &lt;chr [1]&gt; ## 5 Leia Organa 150 49 brown &lt;chr [0]&gt; ## 6 Owen Lars 178 120 brown, grey &lt;chr [0]&gt; ## 7 Beru Whitesun lars 165 75 brown &lt;chr [0]&gt; ## 8 R5-D4 97 32 &lt;NA&gt; &lt;chr [0]&gt; ## 9 Biggs Darklighter 183 84 black &lt;chr [1]&gt; ## 10 Obi-Wan Kenobi 182 77 auburn, white &lt;chr [5]&gt; ## # … with 77 more rows dat %&gt;% select(1:4 &amp; (starts_with(&quot;h&quot;))) # Select cols 1:4 and all which start with &quot;h&quot; ## # A tibble: 87 × 2 ## height hair_color ## &lt;int&gt; &lt;chr&gt; ## 1 172 blond ## 2 167 &lt;NA&gt; ## 3 96 &lt;NA&gt; ## 4 202 none ## 5 150 brown ## 6 178 brown, grey ## 7 165 brown ## 8 97 &lt;NA&gt; ## 9 183 black ## 10 182 auburn, white ## # … with 77 more rows Sidenote on using slice(): slice() is similar to select() but less intuitive, thus better use filter() dat %&gt;% slice(1) # Select first entry ## # A tibble: 1 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… 172 77 blond fair blue 19 male mascul… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; dat %&gt;% slice(1:10) # Select top 10 entries ## # A tibble: 10 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke S… 172 77 blond fair blue 19 male mascu… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 4 Darth … 202 136 none white yellow 41.9 male mascu… ## 5 Leia O… 150 49 brown light brown 19 fema… femin… ## 6 Owen L… 178 120 brown, grey light blue 52 male mascu… ## 7 Beru W… 165 75 brown light blue 47 fema… femin… ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… ## 9 Biggs … 183 84 black light brown 24 male mascu… ## 10 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; dat %&gt;% slice(nrow(dat)) # Select last entry ## # A tibble: 1 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Padmé A… 165 45 brown light brown 46 female femin… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; dat %&gt;% slice(-(1:nrow(dat)-1)) # Delete everything from 1 to second last entry ## # A tibble: 1 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Padmé A… 165 45 brown light brown 46 female femin… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; 5.5.3 Filter() filter() provides a quick tool to pick certain entries using functions and conditions. Here’s a quick overview on math and logic operators: Math Operators Logic Operators Equal == a AND b a, b (or a &amp; b) Not Equal != a OR b a | b Bigger &gt; EITHER a or b xor(a, b) Equal or Smaller &gt;= NOT a !a # Reducing data for comprehensive output dat1 &lt;- dat %&gt;% select(1:8) # Numeric Criteria dat1 %&gt;% filter(height &gt; 100) # Entries with height above 100 ## # A tibble: 74 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalk… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none ## 3 Darth Vader 202 136 none white yellow 41.9 male ## 4 Leia Organa 150 49 brown light brown 19 fema… ## 5 Owen Lars 178 120 brown, grey light blue 52 male ## 6 Beru Whitesu… 165 75 brown light blue 47 fema… ## 7 Biggs Darkli… 183 84 black light brown 24 male ## 8 Obi-Wan Keno… 182 77 auburn, whi… fair blue-gray 57 male ## 9 Anakin Skywa… 188 84 blond fair blue 41.9 male ## 10 Wilhuff Tark… 180 NA auburn, grey fair blue 64 male ## # … with 64 more rows dat1 %&gt;% filter(height &gt; 100 , mass &gt; 125) # Entries with height above 100 and mass above 125 ## # A tibble: 5 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Darth Vader 202 136 none white yellow 41.9 male ## 2 Jabba Desi… 175 1358 &lt;NA&gt; green-tan, … orange 600 hermap… ## 3 IG-88 200 140 none metal red 15 none ## 4 Grievous 216 159 none brown, white green, ye… NA male ## 5 Tarfful 234 136 brown brown blue NA male dat1 %&gt;% filter(height &gt; 100 &amp; mass &gt; 125) # &quot; ## # A tibble: 5 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Darth Vader 202 136 none white yellow 41.9 male ## 2 Jabba Desi… 175 1358 &lt;NA&gt; green-tan, … orange 600 hermap… ## 3 IG-88 200 140 none metal red 15 none ## 4 Grievous 216 159 none brown, white green, ye… NA male ## 5 Tarfful 234 136 brown brown blue NA male dat1 %&gt;% filter(mass &gt;= 50 &amp; mass &lt;= 55) # Entries with more than 50 and below 55 mass ## # A tibble: 4 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ayla Secura 178 55 none blue hazel 48 fema… ## 2 Adi Gallia 184 50 none dark blue NA fema… ## 3 Barriss Offee 166 50 black yellow blue 40 fema… ## 4 Zam Wesell 168 55 blonde fair, green,… yellow NA fema… dat1 %&gt;% filter(between(mass, 50, 55)) # &quot; ## # A tibble: 4 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ayla Secura 178 55 none blue hazel 48 fema… ## 2 Adi Gallia 184 50 none dark blue NA fema… ## 3 Barriss Offee 166 50 black yellow blue 40 fema… ## 4 Zam Wesell 168 55 blonde fair, green,… yellow NA fema… dat1 %&gt;% filter(between(height, 85, 115)) # Entries with height between 85 and 115 ## # A tibble: 6 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none ## 2 R5-D4 97 32 &lt;NA&gt; white, red red NA none ## 3 Wicket Systri W… 88 20 brown brown brown 8 male ## 4 Sebulba 112 40 none grey, red orange NA male ## 5 Dud Bolt 94 45 none blue, grey yellow NA male ## 6 R4-P17 96 NA none silver, r… red, blue NA none dat1 %&gt;% filter(near(height, 100, 15)) # Entries with height 100 +/- 15 (same as line 106) ## # A tibble: 6 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none ## 2 R5-D4 97 32 &lt;NA&gt; white, red red NA none ## 3 Wicket Systri W… 88 20 brown brown brown 8 male ## 4 Sebulba 112 40 none grey, red orange NA male ## 5 Dud Bolt 94 45 none blue, grey yellow NA male ## 6 R4-P17 96 NA none silver, r… red, blue NA none # Character Criteria dat1 %&gt;% filter(eye_color == &quot;red&quot;) # Entries with red eyes ## # A tibble: 5 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 R2-D2 96 32 &lt;NA&gt; white, blue red 33 none ## 2 R5-D4 97 32 &lt;NA&gt; white, red red NA none ## 3 IG-88 200 140 none metal red 15 none ## 4 Bossk 190 113 none green red 53 male ## 5 Nute Gunray 191 90 none mottled green red NA male dat1 %&gt;% filter(eye_color != &quot;red&quot;) # Entries without red eyes ## # A tibble: 82 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalk… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none ## 3 Darth Vader 202 136 none white yellow 41.9 male ## 4 Leia Organa 150 49 brown light brown 19 fema… ## 5 Owen Lars 178 120 brown, grey light blue 52 male ## 6 Beru Whitesu… 165 75 brown light blue 47 fema… ## 7 Biggs Darkli… 183 84 black light brown 24 male ## 8 Obi-Wan Keno… 182 77 auburn, whi… fair blue-gray 57 male ## 9 Anakin Skywa… 188 84 blond fair blue 41.9 male ## 10 Wilhuff Tark… 180 NA auburn, grey fair blue 64 male ## # … with 72 more rows dat1 %&gt;% filter(!(eye_color == &quot;red&quot;)) # Entries without red eyes ## # A tibble: 82 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalk… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none ## 3 Darth Vader 202 136 none white yellow 41.9 male ## 4 Leia Organa 150 49 brown light brown 19 fema… ## 5 Owen Lars 178 120 brown, grey light blue 52 male ## 6 Beru Whitesu… 165 75 brown light blue 47 fema… ## 7 Biggs Darkli… 183 84 black light brown 24 male ## 8 Obi-Wan Keno… 182 77 auburn, whi… fair blue-gray 57 male ## 9 Anakin Skywa… 188 84 blond fair blue 41.9 male ## 10 Wilhuff Tark… 180 NA auburn, grey fair blue 64 male ## # … with 72 more rows # NA&#39;s dat1 %&gt;% drop_na # Removes all entries with at least 1 NA ## # A tibble: 32 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalk… 172 77 blond fair blue 19 male ## 2 Darth Vader 202 136 none white yellow 41.9 male ## 3 Leia Organa 150 49 brown light brown 19 fema… ## 4 Owen Lars 178 120 brown, grey light blue 52 male ## 5 Beru Whitesu… 165 75 brown light blue 47 fema… ## 6 Biggs Darkli… 183 84 black light brown 24 male ## 7 Obi-Wan Keno… 182 77 auburn, whi… fair blue-gray 57 male ## 8 Anakin Skywa… 188 84 blond fair blue 41.9 male ## 9 Chewbacca 228 112 brown unknown blue 200 male ## 10 Han Solo 180 80 brown fair brown 29 male ## # … with 22 more rows # Pick values from a vector using %in% eyes &lt;- c(&quot;red&quot;, &quot;black&quot;, &quot;none&quot;) # Defining vector with wanted eye colors dat1 %&gt;% filter(eye_color %in% eyes) # Entries with wanted eye colors ## # A tibble: 15 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 R2-D2 96 32 &lt;NA&gt; white, blue red 33 none ## 2 R5-D4 97 32 &lt;NA&gt; white, red red NA none ## 3 Greedo 173 74 &lt;NA&gt; green black 44 male ## 4 IG-88 200 140 none metal red 15 none ## 5 Bossk 190 113 none green red 53 male ## 6 Nien Nunb 160 68 none grey black NA male ## 7 Nute Gunray 191 90 none mottled green red NA male ## 8 Gasgano 122 NA none white, blue black NA male ## 9 Kit Fisto 196 87 none green black NA male ## 10 Plo Koon 188 80 none orange black 22 male ## 11 Lama Su 229 88 none grey black NA male ## 12 Taun We 213 NA none grey black NA fema… ## 13 Shaak Ti 178 57 none red, blue, wh… black NA fema… ## 14 Tion Medon 206 80 none grey black NA male ## 15 BB8 NA NA none none black NA none dat1 %&gt;% filter(eye_color %in% eyes &amp; is.na(birth_year)) # Entries with wanted eye colors and missing birthdate ## # A tibble: 10 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 R5-D4 97 32 &lt;NA&gt; white, red red NA none ## 2 Nien Nunb 160 68 none grey black NA male ## 3 Nute Gunray 191 90 none mottled green red NA male ## 4 Gasgano 122 NA none white, blue black NA male ## 5 Kit Fisto 196 87 none green black NA male ## 6 Lama Su 229 88 none grey black NA male ## 7 Taun We 213 NA none grey black NA fema… ## 8 Shaak Ti 178 57 none red, blue, wh… black NA fema… ## 9 Tion Medon 206 80 none grey black NA male ## 10 BB8 NA NA none none black NA none # Negations dat1 %&gt;% filter(!(eye_color %in% eyes)) # No (!) entries with wanted eye colors ## # A tibble: 72 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalk… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none ## 3 Darth Vader 202 136 none white yellow 41.9 male ## 4 Leia Organa 150 49 brown light brown 19 fema… ## 5 Owen Lars 178 120 brown, grey light blue 52 male ## 6 Beru Whitesu… 165 75 brown light blue 47 fema… ## 7 Biggs Darkli… 183 84 black light brown 24 male ## 8 Obi-Wan Keno… 182 77 auburn, whi… fair blue-gray 57 male ## 9 Anakin Skywa… 188 84 blond fair blue 41.9 male ## 10 Wilhuff Tark… 180 NA auburn, grey fair blue 64 male ## # … with 62 more rows dat1 %&gt;% filter(eye_color %in% eyes &amp; !is.na(birth_year)) # Entries with wanted eye colors but no missing birthdate ## # A tibble: 5 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 R2-D2 96 32 &lt;NA&gt; white, blue red 33 none ## 2 Greedo 173 74 &lt;NA&gt; green black 44 male ## 3 IG-88 200 140 none metal red 15 none ## 4 Bossk 190 113 none green red 53 male ## 5 Plo Koon 188 80 none orange black 22 male dat1 %&gt;% filter(is.na(sex)) # Entries where sex is NA ## # A tibble: 4 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ric Olié 183 NA brown fair blue NA &lt;NA&gt; ## 2 Quarsh Panaka 183 NA black dark brown 62 &lt;NA&gt; ## 3 Sly Moore 178 48 none pale white NA &lt;NA&gt; ## 4 Captain Phasma NA NA unknown unknown unknown NA &lt;NA&gt; dat1 %&gt;% filter(is.na(sex)) %&gt;% nrow # Number of entries where sex is NA ## [1] 4 dat1 %&gt;% filter(!is.na(sex) | !is.na(hair_color)) # Remove entries where sex or hair color is missing ## # A tibble: 87 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalk… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none ## 4 Darth Vader 202 136 none white yellow 41.9 male ## 5 Leia Organa 150 49 brown light brown 19 fema… ## 6 Owen Lars 178 120 brown, grey light blue 52 male ## 7 Beru Whitesu… 165 75 brown light blue 47 fema… ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none ## 9 Biggs Darkli… 183 84 black light brown 24 male ## 10 Obi-Wan Keno… 182 77 auburn, whi… fair blue-gray 57 male ## # … with 77 more rows # Filter functions dat1 %&gt;% top_n(3) # Selects highest values of given number of numeric columns ## # A tibble: 6 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none ## 2 R2-D2 96 32 &lt;NA&gt; white, blue red 33 none ## 3 R5-D4 97 32 &lt;NA&gt; white, red red NA none ## 4 IG-88 200 140 none metal red 15 none ## 5 R4-P17 96 NA none silver, red red, blue NA none ## 6 BB8 NA NA none none black NA none dat1 %&gt;% top_n(-3) # Selects lowest values of given number of numeric columns ## # A tibble: 16 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Leia Organa 150 49 brown light brown 19 fema… ## 2 Beru White… 165 75 brown light blue 47 fema… ## 3 Mon Mothma 150 NA auburn fair blue 48 fema… ## 4 Shmi Skywa… 163 NA black fair brown 72 fema… ## 5 Ayla Secura 178 55 none blue hazel 48 fema… ## 6 Adi Gallia 184 50 none dark blue NA fema… ## 7 Cordé 157 NA brown light brown NA fema… ## 8 Luminara U… 170 56.2 black yellow blue 58 fema… ## 9 Barriss Of… 166 50 black yellow blue 40 fema… ## 10 Dormé 165 NA brown light brown NA fema… ## 11 Zam Wesell 168 55 blonde fair, green, … yellow NA fema… ## 12 Taun We 213 NA none grey black NA fema… ## 13 Jocasta Nu 167 NA white fair blue NA fema… ## 14 Shaak Ti 178 57 none red, blue, wh… black NA fema… ## 15 Rey NA NA brown light hazel NA fema… ## 16 Padmé Amid… 165 45 brown light brown 46 fema… dat1 %&gt;% group_by(sex) %&gt;% top_n(-1, birth_year) # Give the youngest of each sex ## # A tibble: 5 × 8 ## # Groups: sex [5] ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Leia Organa 150 49 brown light brown 19 female ## 2 Jabba Desi… 175 1358 &lt;NA&gt; green-tan, … orange 600 hermaph… ## 3 IG-88 200 140 none metal red 15 none ## 4 Wicket Sys… 88 20 brown brown brown 8 male ## 5 Quarsh Pan… 183 NA black dark brown 62 &lt;NA&gt; Other useful filter functions: filter_if(), filter_at(). Examples can be found here. 5.5.4 Exercises How many pale characters are there from planet Ryloth or Naboo? Who is the oldest of the tallest 5 characters? Who has the most starships? Hints: Try to Google: Unlist into new columns or check this stackoverflow post Find name and starship of the smallest character in “Return of the Jedi” Hints: filter_at or this stackoverflow post 5.5.5 Solutions # a. dat %&gt;% filter(skin_color == &quot;pale&quot;, homeworld == &quot;Naboo&quot; | homeworld == &quot;Ryloth&quot;) %&gt;% nrow ## [1] 2 # b. dat1 %&gt;% top_n(5, height) %&gt;% top_n(1, birth_year) ## # A tibble: 1 × 8 ## name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Chewbacca 228 112 brown unknown blue 200 male # c. dat %&gt;% unnest_wider(starships) %&gt;% filter_at(vars(contains(&quot;...&quot;)), all_vars(!is.na(.))) ## # A tibble: 1 × 18 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Obi-Wan… 182 77 auburn, wh… fair blue-gray 57 male mascu… ## # … with 9 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, ...1 &lt;chr&gt;, ...2 &lt;chr&gt;, ...3 &lt;chr&gt;, ...4 &lt;chr&gt;, ...5 &lt;chr&gt; # d. dat %&gt;% unnest(starships) %&gt;% filter(films == &quot;Return of the Jedi&quot;) %&gt;% top_n(-1, height) %&gt;% select(name, starships) ## # A tibble: 1 × 2 ## name starships ## &lt;chr&gt; &lt;chr&gt; ## 1 Nien Nunb Millennium Falcon 5.6 Preparing data for ggplot() Here we will also use a new package ggridges(Claus O. Wilke 2021), which allows you to make ridgeline plots with ggplot2. library(tidyverse) library(ggplot2) library(ggridges) ?pivot_longer Example 1: head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa iris.long &lt;- iris[,1:4] %&gt;% pivot_longer(cols=1:4, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) head(iris.long) ## # A tibble: 6 × 2 ## variable value ## &lt;chr&gt; &lt;dbl&gt; ## 1 Sepal.Length 5.1 ## 2 Sepal.Width 3.5 ## 3 Petal.Length 1.4 ## 4 Petal.Width 0.2 ## 5 Sepal.Length 4.9 ## 6 Sepal.Width 3 iris.long %&gt;% ggplot(aes(x = value, y = variable)) + geom_density_ridges() Example 2: head(relig_income) ## # A tibble: 6 × 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 122 ## 2 Atheist 12 27 37 52 35 70 73 ## 3 Buddhist 27 21 30 34 33 58 62 ## 4 Catholic 418 617 732 670 638 1116 949 ## 5 Don’t kn… 15 14 15 11 10 35 21 ## 6 Evangeli… 575 869 1064 982 881 1486 949 ## # … with 3 more variables: $100-150k &lt;dbl&gt;, &gt;150k &lt;dbl&gt;, ## # Don&#39;t know/refused &lt;dbl&gt; relig_income ## # A tibble: 18 × 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 122 ## 2 Atheist 12 27 37 52 35 70 73 ## 3 Buddhist 27 21 30 34 33 58 62 ## 4 Catholic 418 617 732 670 638 1116 949 ## 5 Don’t k… 15 14 15 11 10 35 21 ## 6 Evangel… 575 869 1064 982 881 1486 949 ## 7 Hindu 1 9 7 9 11 34 47 ## 8 Histori… 228 244 236 238 197 223 131 ## 9 Jehovah… 20 27 24 24 21 30 15 ## 10 Jewish 19 19 25 25 30 95 69 ## 11 Mainlin… 289 495 619 655 651 1107 939 ## 12 Mormon 29 40 48 51 56 112 85 ## 13 Muslim 6 7 9 10 9 23 16 ## 14 Orthodox 13 17 23 32 32 47 38 ## 15 Other C… 9 7 11 13 13 14 18 ## 16 Other F… 20 33 40 46 49 63 46 ## 17 Other W… 5 2 3 4 2 7 3 ## 18 Unaffil… 217 299 374 365 341 528 407 ## # … with 3 more variables: $100-150k &lt;dbl&gt;, &gt;150k &lt;dbl&gt;, ## # Don&#39;t know/refused &lt;dbl&gt; relig_income_long &lt;- relig_income %&gt;% pivot_longer(-religion, names_to = &quot;income&quot;, values_to = &quot;count&quot;) relig_income_long %&gt;% ggplot(aes(x = income, y = count)) + geom_bar(stat=&quot;identity&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) Example 3: df &lt;- tibble(country=c(&#39;Afghanistan&#39;, &#39;Brazil&#39;,&#39;China&#39;), &#39;1999&#39;=c(745,37737,212258), &#39;2000&#39;=c(2666,80488,213768)) df ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213768 df_long &lt;- df %&gt;% pivot_longer(-country, names_to=&quot;year&quot;, values_to = &quot;cases&quot;) # equivalently: df_long &lt;- df %&gt;% pivot_longer(c(&#39;1999&#39;,&#39;2000&#39;), names_to=&quot;year&quot;, values_to = &quot;cases&quot;) df_long ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213768 df_long %&gt;% ggplot(aes(x = year, y = cases, shape= country, color=country)) + geom_point(size=4) 5.7 Base R functions ## lapply # returns a list of the same length as X, ?lapply head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa lapply(iris[,-5],mean) # exclude the Species column ## $Sepal.Length ## [1] 5.843333 ## ## $Sepal.Width ## [1] 3.057333 ## ## $Petal.Length ## [1] 3.758 ## ## $Petal.Width ## [1] 1.199333 ## sapply # a user-friendly version and wrapper of lapply # by default returning a vector, matrix sapply(iris[,-5],mean) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843333 3.057333 3.758000 1.199333 ## substr() substr(&#39;APPLE&#39;,2,4) ## [1] &quot;PPL&quot; substr(&#39;AP PLE&#39;,2,4) # spacing counts ## [1] &quot;P P&quot; ## gregexpr() ?gregexpr sequences&lt;-c(&quot;ACATGTCATGTCC&quot;,&quot;CTTGTATGCTG&quot;) gregexpr(&quot;ATG&quot;,sequences) ## [[1]] ## [1] 3 8 ## attr(,&quot;match.length&quot;) ## [1] 3 3 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE ## ## [[2]] ## [1] 6 ## attr(,&quot;match.length&quot;) ## [1] 3 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE ## regexec() and gregexpr() ?regexec() pattern &lt;- &quot;([[:alpha:]]+)([[:digit:]]+)&quot; # alphabetic characters + digits ######123456789 s &lt;- &quot;Test: A1 BC23 DEF456&quot; regexec(pattern,s) ## [[1]] ## [1] 7 7 8 ## attr(,&quot;match.length&quot;) ## [1] 2 1 1 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE # only record the location of first match regmatches(s, regexec(pattern,s)) ## [[1]] ## [1] &quot;A1&quot; &quot;A&quot; &quot;1&quot; # returns only the first match gregexpr(pattern, s) ## [[1]] ## [1] 7 10 15 ## attr(,&quot;match.length&quot;) ## [1] 2 4 6 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE # get the location of all matched substrings regmatches(s, gregexpr(pattern, s)) ## [[1]] ## [1] &quot;A1&quot; &quot;BC23&quot; &quot;DEF456&quot; # extract all matched substrings lapply( regmatches(s, gregexpr(pattern, s)), # all matched strings # get the matching substings and elements function(e) regmatches(e, regexec(pattern, e))) ## [[1]] ## [[1]][[1]] ## [1] &quot;A1&quot; &quot;A&quot; &quot;1&quot; ## ## [[1]][[2]] ## [1] &quot;BC23&quot; &quot;BC&quot; &quot;23&quot; ## ## [[1]][[3]] ## [1] &quot;DEF456&quot; &quot;DEF&quot; &quot;456&quot; References "],["ch-06.html", "Chapter 6 Supervised Machine Learning I 6.1 Introduction 6.2 Tutorial 6.3 Exercise", " Chapter 6 Supervised Machine Learning I 6.1 Introduction 6.1.1 Learning objectives After this learning unit, you will be able to … Differentiate machine learning from classical statistical modelling Describe the different variants of machine learning Conceptualize model training as an optimization problem Describe overfitting and how it can be measured (training vs. validation error). Formulate a model in R. Discuss why, when, and how to pre-process data. Measure and minimize loss for regression and classification (video) Describe the fundamentals of gradient descent (video) 6.1.2 Important points from the lecture Machine learning refers to a class of algorithms that automatically generate statistical models of data. There are two main types of machine learning: Unsupervised machine learning: A class of algorithms that automatically detect patterns in data without using labels or ‘learning without a teacher’. Examples are: PCAs, k-means clustering, autoencoders, self-organizing maps, etc. Supervised machine learning: A class of algorithms that automatically learn an input-output relationships based on example input-output pairs. Examples include: support vector machines, random forests, decision trees, neural networks, etc. Supervised machine learning requires three ingredients: (1) Input data, (2) Output data, and (3) A measure of model performance (a.k.a. “loss”). Supervised machine learning be used for regression (predict a continuous label) or classification (predict a categorical label). Loss is a concept central to many supervised machine learning algorithms. It measures how well our predicted model values fit the actual observed model values. During training, the machine learning algorithm optimizes the loss. Typically, this is a minimization of some error metric, e.g. the mean absolute error or the root mean square error. 6.2 Tutorial 6.2.1 Overfitting Machine learning (ML) may appear magical. The ability of ML algorithms to detect patterns and make predictions is fascinating. However, several challenges have to be met in the process of formulating, training, and evaluating the models. In this practical we will discuss some basics of supervised ML and how to achieve best predictive results. In general, the aim of supervised ML is to find a model \\(\\hat{Y} = f(X)\\) that is trained (calibrated) using observed relationships between a set of features (also known as predictors, or labels, or independent variables) \\(X\\) and the target variable \\(Y\\). Note, that \\(Y\\) is observed. The hat on \\(\\hat{Y}\\) indicates that it is an estimate, provided by the model \\(f\\) . Some algorithms can even handle predictions of multiple target variables simultaneously (e.g., neural networks). ML algorithms consist of (more or less) flexible mathematical models with a certain structure and set of parameters. At the simple extreme end of the model spectrum is the univariate linear regression. You may not want to call this a ML algorithm because there is no iterative learning involved in fitting a linear regression model. Nevertheless, also a univariate linear regression provides a prediction \\(\\hat{Y} = f(X)\\), just like other (proper) ML algorithms do. The functional form of a univariate linear regression is not particularly flexible (just a straight line for the best fit between predictors and targets) and it has only two parameters (slope and intercept). At the other extreme end are, for example, deep neural networks. They are extremely flexible, can learn highly non-linear relationships and deal with interactions between a large number of predictors. They also contain very large numbers of parameters, typically on the order of thousands. You can imagine that this allows these types of algorithms to very effectively learn from the data, but also bears the risk of overfitting. What is overfitting? The following example illustrates it. Let’s assume that there is some true underlying relationship between a predictor \\(x\\) and the target variable \\(y\\). We don’t know this relationship (in the code below, this is true_fun()) and the observations contain a (normally distributed) error (y = true_fun(x) + 0.1 * rnorm(n_samples)). Based on our training data (df_train), we fit polynomial models of degree 1, 4, and 15 to the observations. A polynomial of degree N is given by: \\[ y = \\sum_{n=0}^N a_n x^n \\] \\(a_n\\) are the coefficients, i.e., model parameters. The goal of the training is to get the coefficients \\(a_n\\). From the above definition, the polynomial of degree 15 has 16 parameters, while the polynomial of degree 1 has two parameters (and corresponds to a simple linear regression). You can imagine that the polynomial of degree 15 is much more flexible and should thus yield the closest fit to the training data. This is indeed the case. We can use the same fitted models on unseen data - the validation data. This is what’s done below. Again, the same true underlying relationship is used, but we sample a new set of data points in x and add a new sample of errors on top of the true relationship. You see that, using the validation set, we find that “poly4” actually performs the best - it has a much lower RMSE that “poly15”. Apparently, “poly15” was overfitted. Apparently, it indeed used its flexibility to fit not only the shape of the true underlying relationship, but also the observation errors on top of it. This has obviously the implication that, when this model is used to make predictions for data that was not used for training (calibration), it will yield misguided predictions that are affected by the errors in the training set. In the above pictures we can also conclude that “poly1” was underfitted. It gets even worse when applying the fitted polynomial models to data that extends beyond the range in \\(x\\) that was used for model training. Here, we’re extending just 10% to the left and to the right. You see that the RMSE for “poly15” literally explodes. The model is hopelessly overfitted and completely useless for prediction, although it looked like it fit the data best when we considered at the training results. This is a fundamental challenge in ML - finding the model with the best generalisability. That is, a model that not only fits the training data well, but also performs well on unseen data. The phenomenon of fitting/overfitting as a function of the model “flexibility” is also referred to as bias vs. variance trade-off. The bias describes how well a model matches the training set (average error). A model with low bias will match the data set closely and vice versa. The variance describes how much a model changes when you train it using different portions of your data set. “poly15” has a high variance, but much of its variance is the result of misled training on observation errors. On the other extreme, “poly1” has a high bias. It’s not affected by the noise in observations, but its predictions are also far off the observations. In ML, we are challenged to balance this trade-off. The next chapters introduce methods to achieve the best model generalisability and to find the sweet spot between high bias and high variance. The steps to get there include the splitting of data into training and testing sets, pre-processing of data and model training which “steers” the model towards what is considered a good model fit in terms of its generalisation power. You have learned in video 6a about the basic setup of supervised ML, with input data containing the features (or predictors) \\(X\\), predicted (\\(\\hat{Y}\\)) and observed target values (\\(Y\\), also known as labels). In video 6b on loss and it’s minimization, you learned about the loss function which quantifies the agreement between \\(Y\\) and \\(\\hat{Y}\\) and defines the objective of the model training. Here, you’ll learn how all of this can be implemented in R. Depending on your application or research question, it may also be of interest to evaluate the relationships embodied in \\(f(X)\\) or to quantify the importance of different predictors in our model. This is referred to as model interpretation and is introduced in the respectively named subsection. Finally, we’ll get into feature selection in the next Application session. The topic of supervised machine learning methods covers enough material to fill two sessions. Therefore, we split this part in two. Model training, implementing the an entire modelling workflow, model evaluation and interpretation will be covered in the next session’s tutorial (Supervised Machine Learning Methods II). Of course, a plethora of algorithms exist that do the job of \\(Y = f(X)\\). Each of them has its own strengths and limitations. It is beyond the scope of this course to introduce a larger set of algorithms. Subsequent sessions will focus primarily on Artificial Neural Networks (ANN) - a type of ML algorithm that has gained popularity for its capacity to efficiently learn patterns in large data sets. For illustration purposes in this and the next chapter, we will briefly introduce three relatively simple alternative “ML” methods, linear regression, K-nearest-neighbors, and Random Forest (RF). They have quite different characteristics and are therefore great for illustration purposes in this chapter. RF has become very popular for a wide variety of applications in environmental sciences. It is a particularly “user-friendly” algorithm as it performs well “out-of-the-box”. 6.2.2 Modelling challenge In this tutorial, we formulate a model for predicting ecosystem gross primary production (photosynthesis) from environmental covariates. This is to say that GPP_NT_VUT_REF is the target variable, and other available variables available in the dataset from Chapter ?? can be used as predictors. Abiotic factors largely determine ecosystem-atmosphere exchange fluxes of water vapour and CO2. Temporally changing mass exchange fluxes can be continuously measured with the eddy covariance technique, while abiotic variables (meteorological variables, soil moisture) can be measured in parallel. This offers an opportunity for building models that predict mass exchange fluxes from a set of abiotic predictors. Data is provided here at daily resolution for a site (‘CH-Dav’) located in the Swiss alps (Davos). This is one of the longest-running eddy covariance sites globally and measures fluxes in an evergreen coniferous forest with cold winters and temperate, relatively moist summers. For more information of the variables in the dataset, see the FLUXNET 2015 website, and Pastorello et al., 2020 for a comprehensive documentation of variable definitions and methods. Available variables are: TIMESTAMP: Day of measurement. TA_F: Air temperature. The meaning of suffix _F is described in Pastorello et al., 2020. SW_IN_F: Shortwave incoming radiation LW_IN_F: Longwave incoming radiation VPD_F: Vapour pressure deficit (relates to the humidity of the air) PA_F: Atmospheric pressure P_F: Precipitation WS_F: Wind speed GPP_NT_VUT_REF: Gross primary production - the target variable NEE_VUT_REF_QC: Quality control information for GPP_NT_VUT_REF. Specifies the fraction of high-quality underlying high-frequency data from which the daily data is derived. 0.8 = 80% underlying high-quality data, remaining 20% of the high-frequency data is gap-filled. 6.2.3 A selection of model types 6.2.3.1 Linear regression The simplest form of a linear model (LM) is the univariate linear regression, where we assume a linear relationship between \\(X\\) and \\(Y\\): \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\;\\;\\; i = 1, 2, ...n \\;, \\] where \\(Y_i\\) is the i-th observation of the target variable, and \\(X_i\\) is the i-th value of the (single) predictor variable. The errors \\(\\epsilon_i\\) are assumed to be independent from each other (no autocorrelation), normally distributed, have mean of zero and a constant variance. \\(\\beta_0\\) and \\(\\beta_1\\) are constant coefficients (model parameters). Fitting a linear regression is finding the values for \\(\\beta_0\\) and \\(\\beta_1\\) so that the sum of the square errors is minimized, that is: \\[ \\sum_i \\epsilon_i^2 = \\sum_i (Y_i - \\beta_0 - \\beta_1 X_i)^2 = \\text{argmin}. \\] Since the expected value of \\(\\epsilon\\) is zero (because it’s normally distributed with mean zero), predictions of a linear regression model are obtained by \\(Y_\\text{new} = \\beta_0 + \\beta_1 X_\\text{new}\\), and provide unbiased estimates. It’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of \\(p\\) predictor variables: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\; ... \\; + \\beta_p X_p + \\epsilon \\;. \\]Note that here, \\(X\\), \\(Y\\), and \\(\\epsilon\\) are vectors of length corresponding to the number of observations in our data set (\\(n\\) - as above). Analogously, calibrating the \\(p\\) coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) is to minimize the sum of square errors \\(\\sum_i \\epsilon_i^2\\). While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and so on. 6.2.3.2 K-nearest neighbours As the name suggests, K-nearest neighbour (KNN) uses the \\(k\\) observations that are “nearest” to the new record for which we want to make a prediction. It then calculates their average (in regression) or most frequent value (in classification) as the prediction. “Nearest” is determined by some distance metric evaluated based on the values of the predictors. In our example (GPP_NT_VUT_REF ~ .), KNN would determine the \\(k\\) days where conditions, given by our set of predictors, were most similar (nearest) to the day for which we seek a prediction. Then, it calculates the prediction as the average (mean) GPP value of these days. Determining “nearest” neighbors is commonly based on either the Euclidean or Manhattan distances between two data points \\(x_a\\) and \\(x_b\\), considering all \\(p\\) predictors \\(j\\). Euclidean distance: \\[ \\sqrt{ \\sum_{j=1}^p (x_{a,j} - x_{b,j})^2 } \\\\ \\] Manhattan distance: \\[ \\sum_{j=1}^p | x_{a,j} - x_{b,j} | \\] In two-dimensional space, the Euclidean distance measures the length of a straight line between two points (remember Pythagoras!). The Manhattan distance is called this way because it measures the distance you would have to walk to get from point \\(a\\) to point \\(b\\) in Manhattan, New York, where you cannot cut corners but have to follow a rectangular grid of streets. \\(|x|\\) is the absolute (positive) value of \\(x\\) ( \\(|-x| = x\\)). KNN is a simple algorithm that uses knowledge of the “local” data structure for prediction. A drawback is that the model training has to be done for each prediction step and the computation time of the training increases with \\(n \\times p\\). KNNs are often used to impute values (fill missing values) and have the advantage that predicted values are always within the range of observed values of the target variable. 6.2.3.3 Random forest Random forest (RF) models are based on decision trees, where binary decisions for predicting the target’s values are based on thresholds of the predictors’ values. The depth of a decision tree refers to the number of such decisions. The deeper a tree, the more likely the model will overfit. Just as forests are made up by trees, Random Forest models make use of random subsets of the original data and of available predictions and respective decision trees. Predictions are then made by averaging predictions of individual base learners (the decision trees). The number of predictors considered at each decision step is a tunable parameter (a hyperparameter, typically called \\(m_{try}\\)). Introducing this randomness is effective because decision trees tend to overfit and because of the wisdom of the crowd - i.e., the power of aggregating individual predictions with their random error (and without systematic bias) for generating accurate and relatively precise predictions. Random forest models have gained particular popularity and are widely applied in environmental sciences not only for their power, but also for their ease of use. No pre-processing (centering, scaling) is necessary, they can deal with skewed data, and can effectively learn interactions between predictors. You can learn more on how random forests work in the book Hands On Machine-Learning in R. Before we move on to the actual implementation of LM, KNN and RF, we first have to understand how we can properly prepare our data in order to fit the respective model’s requirements and how to validate model performance. 6.2.4 Data splitting 6.2.4.1 Reading and wrangling data There is a difference between data wrangling and pre-processing as part of the modelling workflow. Data wrangling can be considered to encompass the steps to prepare the data set prior to modelling, including, the combination of variables from different sources, removal of bad or missing data, and aggregating to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample). See the Quartz Guide to Bad Data for an overview of how to deal with different types of bad data. In contrast, data pre-processing refers to the additional steps that are either required by the ML algorithm (e.g. centering and scaling for KNN or neural networks) or the transformation of variables guided by the resulting improvement of the predictive power of the ML model. In other words, pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data. Let’s read the data, select relevant variables, convert the time stamp column to a time object and interpret missing values (encoded -9999 in the file). library(tidyverse) ddf &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) %&gt;% ## select only the variables we are interested in select(TIMESTAMP, GPP_NT_VUT_REF, # the target NEE_VUT_REF_QC, # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological variables -contains(&quot;JSB&quot;) # weird useless variable ) %&gt;% ## convert to a nice date object mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %&gt;% ## set all -9999 to NA na_if(-9999) %&gt;% ## drop QC variables (no longer needed), except NEE_VUT_REF_QC select(-ends_with(&quot;_QC&quot;), NEE_VUT_REF_QC) The column NEE_VUT_REF_QC provides information about the fraction of gap-filled half-hourly data used to calculate daily aggregates. Let’s use only GPP_NT_VUT_REF data, where at least 80% of the underlying half-hourly data was good quality measured data, and not gap-filled. Make sure to not actually remove the respective rows, but rather replace values with NA. ddf &lt;- ddf %&gt;% mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF)) At this stage, we won’t use NEE_VUT_REF_QC any longer. So we can drop it. ddf &lt;- ddf %&gt;% select(-NEE_VUT_REF_QC) 6.2.4.2 Splitting into testing and training sets The introductory example impressively demonstrated the importance of validating the fitted model with data that was not used for training. Thus, we can test the model’s generalisability. The essential step that enables us to assess the model’s generalization error is to hold out part of the data from training, and set it aside (leaving it absolutely untouched) for testing. There is no fixed rule for how much data are to be used for training and testing, respectively. We have to balance the trade-off between: Spending too much data for training will leave us with too little data for testing and the test results may not be robust. In this case, the sample size for getting robust validation statistics is not sufficiently large and we don’t know for sure whether we are safe from an over-fit model. Spending too much data for validation will leave us with too little data for training. In this case, the ML algorithm may not be successful at finding real relationships due to insufficient amounts of training data. Typical splits are between 60-80% for training. However, in cases where the number of data points is very large, the gains from having more training data are marginal but come at the cost of adding to the already high computational burden of model training. In environmental sciences, the number of predictors is often smaller than the sample size (\\(p &lt; n\\)), because it’s typically easier to collect repeated observations of a particular variable than to expand the set of variables being observed. Nevertheless, in cases where the number \\(p\\) gets large, it is important, and for some algorithms mandatory, to maintain \\(p &lt; n\\) for model training. An important aspect to consider when splitting the data is to make sure that all “states” of the system for which we have data are approximately equally represented in training and testing sets. This is to make sure that the algorithm learns relationships \\(f(X)\\) also under rare conditions \\(X\\), for example meteorological extreme events. In other words, we want a balanced training set. Several alternative functions for the data splitting step are available from different packages in R. We will use the the rsample package because it allows to additionally make sure that data from the full range of a given variable’s values (VPD_F in the example below) are well covered in both training and testing sets. library(rsample) set.seed(123) # for reproducibility split &lt;- initial_split(ddf, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- training(split) ddf_test &lt;- testing(split) Plot the distribution of values in the training and testing sets. ddf_train %&gt;% mutate(split = &quot;train&quot;) %&gt;% bind_rows(ddf_test %&gt;% mutate(split = &quot;test&quot;)) %&gt;% pivot_longer(cols = 2:9, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value, y = ..density.., color = split)) + geom_density() + facet_wrap(~variable, scales = &quot;free&quot;) 6.2.5 Pre-processing 6.2.5.1 The use of recipes Skewed data, outliers, and values covering multiple orders of magnitude can create difficulties for certain ML algorithms, e.g., K-nearest neighbours or neural networks. Other algorithms, like tree-based methods (e.g., Random Forest), are more robust against such issues. When defining any pre-processing step, it should be specified as a “recipe” or “blueprint”, and not actually executed on the data itself before we start with the model training. Such a “recipe” can then be applied to any new data, while the parameters of the data pre-processing transformations are different each time. We will introduce an example below where we divide all values in the testing and training dataset by the standard deviation of the respective dataset. Note that, because the two datasets hold different values, they also have different standard deviations. The same “recipe” (i.e., division by standard deviation) will be applied to both datasets but the transformation is done with a different parameter (i.e., the different standard deviations in either dataset). The reason for not actually executing the same data transformation with the same parameters on both dataset is the risk of data leakage. This happens when information from the validation data somehow finds its way into the training step. Don’t worry if this sounds incomprehensible. We’ll learn more about it below. For now, you can focus on the types of pre-processing steps, what they do, and how they are implemented as part of a pre-processing “recipe” in R. Many pre-processing steps can be done either by hand or using different R packages. Below, we give you a mix of both approaches to emphasize on how pre-processing is done. The recipes package offers a powerful way to specify pre-processing steps in R and is gaining traction as part of the tidymodels ecosystem. Moreover, it is compatible with the caret package that will be used to formulate the different ML models. 6.2.5.2 Target engineering [BONUS] Target engineering refers to pre-processing of the target variable. Its application can enable improved predictions, particularly for models that make assumptions about errors (e.g., normally distributed errors in linear regression) and when the target variable follows a “special” distribution (e.g., heavily skewed distribution, or where the target variable is a fraction that is naturally bounded by 0 and 1). A simple log-transformation of the target variable can often resolve issues with skewed distributions. An implication of a log-transformation is that errors in predicting values in the upper end of the observed range do not affect the model disproportionately compared to errors in the lower range. In our data set of half-hourly ecosystem flux and meteorological measurements, the variable WS_F (wind speed) is skewed. The target variable that we have considered so far (GPP_NT_VUT_REF) is not skewed. In a case where we would consider WS_F to be our target variable, we would thus consider applying a log-transformation. library(patchwork) # to combine two plots into separate panels of a single plot gg1 &lt;- ddf_train %&gt;% ggplot(aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;Original&quot;) gg2 &lt;- ddf_train %&gt;% ggplot(aes(x = log(WS_F), y = ..density..)) + geom_histogram() + labs(title = &quot;Log-transformed &#39;by hand&#39;&quot;) gg1 + gg2 # the + is from the patchwork library Now, how can we do this using the recipes package? The syntax is that we define a model with target and predictor variables, the data the recipe should be applied to, and the steps of the recipe: library(recipes) recipe_example &lt;- recipe(WS_F ~ ., data = ddf) %&gt;% step_log(all_outcomes()) recipe_example ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Log transformation on all_outcomes() The formula, specified as an argument in the recipe() function, defines variable “roles” (in recipes-speak). What’s left of the ~ is interpreted as an “outcome” (the target variable). By writing ~ ., we specify all remaining variables in the data frame as “predictors”. As an argument in the step_log() function, we write all_outcomes() to declare that the log transformation is applied only to the “outcome” (target) variable. The output of recipe_example tells us that the recipe holds a log-transformation on all outcome variables (here, 1). As it is in the real world, writing a recipe does not mean to actually cook it, respectively in the recipes terminology to bake it. There are two more steps involved to get there. This might seem a nuisance at first but their separation is actually quite beautiful and translates the conception of the pre-processing as a “blueprint” into the way we write the code. You’ll understand why this is so useful throughout the next Chapters. The full routine from writing the recipe to preparing and baking it is as follows: ## First, we have to prepare everything, taking the prepared recipe and our ingredients (data) prep_example &lt;- prep(recipe_example, training = ddf_train) ## Second, we have to bake our mixture, to get a nice meal df_baked &lt;- bake(prep_example, new_data = ddf_train) The log-transformed data from the recipe-routine should look the same as when simply using the log() command as was done above. gg3 &lt;- df_baked %&gt;% ggplot(aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;log-transformed &#39;by recipe&#39;&quot;) (gg1 + gg2) / (plot_spacer() + gg3) Yes! You can see, both approaches to do a log-transformation resulted in a more-or-less normal distribution of our data. Of course, for a log-transformation this recipe-routine may seem a bit over-the-top. But depending on your data and the distribution that is needed for your model to work, different transformations can be done. For example, the log-transformation does not necessarily result in a perfect normal distribution. Therefore, one can use the Box-Cox transformation which is less straight-forward to do. Fortunately, the recipes package already provides the function step_BoxCox(),so that we do not need to implement a Box-Cox transformation by hand. 6.2.5.3 Standardization Several algorithms explicitly require data to be standardized. That is, values of all predictors to vary within a comparable range. The necessity of this step becomes obvious when considering KNN where the distance would be dominated by variables that range over higher absolute values then others. In other words, inputs have to vary over the same range, expecting a mean of zero and standard deviation of one. To get a quick overview of the distribution of all variables (columns) in our data frame, we can use the skimr package. library(skimr) skim(ddf_train) Table 6.1: Data summary Name ddf_train Number of rows 4600 Number of columns 9 _______________________ Column type frequency: Date 1 numeric 8 ________________________ Group variables None Variable type: Date skim_variable n_missing complete_rate min max median n_unique TIMESTAMP 0 1 1997-01-01 2014-12-31 2005-12-22 4600 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist GPP_NT_VUT_REF 275 0.94 3.23 2.73 -4.23 0.78 2.90 5.44 12.26 ▁▇▆▃▁ TA_F 0 1.00 3.51 6.64 -19.41 -1.55 3.45 8.72 20.69 ▁▃▇▇▂ SW_IN_F 0 1.00 150.37 84.54 5.32 78.13 137.69 214.28 365.89 ▆▇▆▅▂ LW_IN_F 0 1.00 269.44 41.86 138.12 239.32 272.57 302.69 364.91 ▁▃▇▇▂ VPD_F 0 1.00 2.86 2.38 0.00 0.99 2.24 4.06 16.57 ▇▃▁▁▁ PA_F 0 1.00 83.57 0.72 80.37 83.16 83.68 84.07 85.33 ▁▁▃▇▂ P_F 0 1.00 2.31 5.83 0.00 0.00 0.00 1.70 92.10 ▇▁▁▁▁ WS_F 0 1.00 1.99 0.66 0.33 1.54 1.92 2.33 6.54 ▃▇▁▁▁ We see for example, that typical values of LW_IN_F are by a factor 100 larger than values of VPD_F. KNN uses the distance from neighbouring points for predictions. Obviously, in this case here, any distance would be dominated by LW_IN_F and distances in the “direction” of VPD_F, even when relatively large, would not be influential, neither for a Euclidean nor a Manhattan distance (see 1.1). Therefore, it is reasonable to scale distances in a dataset by standardization so that such absolute differences become irrelevant. Standardization is done by dividing each variable, that is all values in one column, by the standard deviation of that variable, and then subtracting its mean. This way, the resulting standardized values are centered around 0, and scaled such that a value of 1 means that the data point is one standard deviation above the mean of the respective variable (column). When applied to all predictors individually, the absolute values of their variations can be directly compared and only then it can be meaningfully used for determining the distance. Standardization can be done not only by centering and scaling (as described above), but also by scaling to within range, where values are scaled such that the minimum value within each variable (column) is 0 and the maximum is 1. In order to avoid data leakage, centering and scaling has to be done separately for each split into training and validation data (more on that later). In other words, don’t center and scale the entire data frame with the mean and standard deviation derived from the entire data frame, but instead center and scale with mean and standard deviation derived from the training portion of the data, and apply that also to the validation portion, when evaluating. The caret package takes care of this. The R package caret provides a unified interface for using different ML algorithms implemented in separate packages. The preprocessing steps applied with each resampling fold can be specified using the function preProcess(). More on resampling in Chapter 6.3.3.1. library(caret) pp &lt;- preProcess(ddf_train, method = c(&quot;center&quot;, &quot;scale&quot;)) pp ## Created from 4325 samples and 9 variables ## ## Pre-processing: ## - centered (8) ## - ignored (1) ## - scaled (8) As seen above for the feature engineering example, this does not return a standardized version of the data frame ddf. Rather, it returns the information that allows us to apply the same standardization also to other data sets. In other words, we use the distribution of values in the data set to which we applied the function to determine the centering and scaling (here: mean and standard deviation). Note that the “ignored” variable is the TIMESTAMP which is formatted as a date object and thus cannot be scaled and centered. Using the recipes package, the code goes as follows: pp &lt;- recipe(GPP_NT_VUT_REF ~ ., data = ddf_train) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) pp ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() Here, we used selectors to apply the recipe step to several variables at once. The first selector, all_numeric(), selects all variables that are either integers or real values. The second selector, -all_outcomes() removes any outcome (target) variables from this recipe step. As you can see from the output of pp, both recipes hold the same information to be applied when training a ML model. 6.2.5.4 Zero-variance predictors [BONUS] Sometimes, the data generation process yields variables that have the same value in each observation. This can be due to failure of the measurement device or another bug in the data collection pipeline. Either way, this may cause some algorithms to crash or become unstable. Such “zero-variance” predictors are usually removed altogether. The same applies also to variables with “near-zero variance”. That is, variables where only a few unique values occur in the entire data set with a high frequency. The danger is that, when data is split into training and testing sets, the variable may effectively become a “zero-variance” variable within the training subset. We can test for zero-variance or near-zero variance predictors by quantifying the following metrics: Frequency ratio: Ratio of the frequency of the most common predictor over the second most common predictor. This should be near 1 for well-behaved predictors and get very large for problematic ones. Percent unique values: The number of unique values divided by the total number of rows in the data set (times 100). For problematic variables, this ratio gets small (approaches 1/100). The function nearZeroVar of the caret package flags suspicious variables (zeroVar = TRUE or nzv = TRUE). In our data set, we don’t find any: nearZeroVar(ddf, saveMetrics= TRUE) ## freqRatio percentUnique zeroVar nzv ## TIMESTAMP 1.000000 100.000000 FALSE FALSE ## GPP_NT_VUT_REF 1.000000 93.732887 FALSE FALSE ## TA_F 1.000000 87.085488 FALSE FALSE ## SW_IN_F 1.500000 98.843931 FALSE FALSE ## LW_IN_F 1.000000 97.916033 FALSE FALSE ## VPD_F 1.142857 62.062671 FALSE FALSE ## PA_F 1.090909 38.469729 FALSE FALSE ## P_F 10.268072 5.978096 FALSE FALSE ## WS_F 1.083333 35.853362 FALSE FALSE Using the recipes package, we can add a step that removes zero-variance predictors. Note that the following code picks up the recipe_example holding the log-transformation from above and extends the entire recipe by a removal of zero-variance predictors. recipe_example &lt;- recipe_example %&gt;% step_zv(all_predictors()) recipe_example ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 8 ## ## Operations: ## ## Log transformation on all_outcomes() ## Zero variance filter on all_predictors() 6.2.5.5 One-hot encoding For ML algorithms that require that all predictors be numerical (e.g., neural networks, or KNN), categorical predictors have to be pre-processed and converted into new numerical predictors. The most common transformation is one-hot encoding, where a categorical feature (predictor) with \\(N\\) levels is replaced by \\(N\\) new features that contain either zeros or ones depending whether the value of the categorical feature corresponds to the respective column. This creates perfect co-linearity between these new columns which allows to drop one column. In other words, one needs \\(N-1\\) new features to describe a categorical variable with \\(N\\) levels. This is referred to as dummy encoding. Check out Figure 6.1 for a one-hot encoding visualization. Figure 6.1: Visualization of one-hot encoding from Kaggle. Let’s have a hands-on example to get a better grasp on this. The following code chunk loads the iris dataset which holds three different levels for the categorical feature Species: library(datasets) df &lt;- iris %&gt;% dplyr::select(Sepal.Length, Sepal.Width, Species) %&gt;% arrange(Sepal.Width) df %&gt;% head() %&gt;% knitr::kable() Sepal.Length Sepal.Width Species 5.0 2.0 versicolor 6.0 2.2 versicolor 6.2 2.2 versicolor 6.0 2.2 virginica 4.5 2.3 setosa 5.5 2.3 versicolor df$Species %&gt;% levels() ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Now, let’s use the handy recipes package to turn Species into several new columns via the step_dummy() command: ## To get a feeling for our dataset, we can also print a summary of our recipe ## Note that the formula below does not define any target variable and that the type of &#39;Species&#39; is nominal recipe( ~ ., data = df) %&gt;% summary() ## # A tibble: 3 × 4 ## variable type role source ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sepal.Length numeric predictor original ## 2 Sepal.Width numeric predictor original ## 3 Species nominal predictor original ## Let&#39;s bake our recipe! baked_df &lt;- recipe( ~ ., data = iris) %&gt;% step_dummy(Species, one_hot = T) %&gt;% prep(., data = df) %&gt;% bake(., new_data = df) baked_df %&gt;% arrange(Sepal.Width) %&gt;% head() %&gt;% knitr::kable() Sepal.Length Sepal.Width Species_setosa Species_versicolor Species_virginica 5.0 2.0 0 1 0 6.0 2.2 0 1 0 6.2 2.2 0 1 0 6.0 2.2 0 0 1 4.5 2.3 1 0 0 5.5 2.3 0 1 0 Compare the output of the dataframe before and after baking. Can you see how Species has been split into three numerical features and how the describe the different levels? Also, do you understand now, why this process is called 1-hot encoding? 6.2.5.6 Dealing with missingness and bad data Several ML algorithms require missing values to be removed. That is, if any of the rows has a missing value in at least one cell, the entire row gets removed. Data may be missing for several reasons. Some reasons yield random patterns of missing data, others not. In the latter case, we can speak of informative missingness (Kuhn &amp; Johnson, 2003) and its information can be used for predictions. For categorical data, we may replace such data with \"none\" (instead of NA), while randomly missing data may be dropped altogether. Some ML algorithms (mainly tree-based methods, e.g., random forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand. Visualising missing data is essential for making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal). The cells with missing data in a data frame can be easily be visualised e.g. with vis_miss() from the visdat package. library(visdat) vis_miss( ddf, cluster = FALSE, warn_large_data = FALSE ) The question about what is “bad data” and whether or when it should be removed is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human writing the paper, it’s often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during its process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions. 6.2.5.7 More pre-processing Depending on the algorithm and the data, additional pre-processing steps may be required. You can find more information about this in the great and freely available online tutorial Hands-On Machine Learning in R. One such additional pre-processing step is imputation, where missing values are imputed (gap-filled), for example by the mean of each variable respectively. Also imputation is prone to cause data leakage and must therefore be implemented as part of the resampling and training workflow. The recipes package offers a great way to deal with imputation (and also all other pre-processing steps). Here is a link to learn more about it. 6.2.6 Model formulation The aim of supervised ML is to find a model \\(\\hat{Y} = f(X)\\) so that \\(\\hat{Y}\\) agrees well with observations \\(Y\\). We typically start with a research question where \\(Y\\) is given - naturally - by the problem we are addressing and we have a data set at hand where one or multiple predictors (or features) \\(X\\) are recorded along with \\(Y\\). From our data, we have information about how GPP (ecosystem-level photosynthesis) depends on set of abiotic factors, mostly meteorological measurements. 6.2.6.1 Formula notation In R, it is common to use the formula notation to specify the target and predictor variables. You have probably encountered formulas before, e.g., for a linear regression using the lm() function. To specify a linear regression model for GPP_NT_VUT_REF with three predictors SW_F_IN, VPD_F, and TA_F, we write: lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf) As mentioned throughout this tutorial, the caret package provides a unified interface to define different ML models and pre-processing recipes. In the next tutorial, we will learn more about how to do so. In this tutorial’s exercise you will deepen your understanding of data wrangling, splitting and pre-processing as well as setting up LM using the formula notation. 6.3 Exercise Now that you are familiar with the basic steps for supervised machine learning, you can get your hands on the data yourself and implement code for addressing the modelling task outlined in Chapter 6.2.2. 6.3.1 Reading and cleaning Read the CSV file \"FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\", select all variables with name ending with \"_F\", the variables \"TIMESTAMP\", \"GPP_NT_VUT_REF\", and \"NEE_VUT_REF_QC\", and drop all variables that contain \"JSB\" in their name. Then convert the variable \"TIMESTAMP\" to a date-time object with the function ymd() from the lubridate package, and interpret all values -9999 as missing values. Then, set all values of \"GPP_NT_VUT_REF\" to missing if the corresponding quality control variable indicates that less than 90% are measured data points. Finally, drop the variable \"NEE_VUT_REF_QC\" - we won’t use it anymore. ## write your code here 6.3.2 Data splitting Split the data into a training and testing set, that contain 70% and 30% of the total available data, respectively. ## write your code here 6.3.3 Linear model 6.3.3.1 Training Fit a linear regression model using the base-R function lm() and the training set. The target variable is \"GPP_NT_VUT_REF\", and predictor variables are all available meterological variables in the dataset. Answer the following questions: What is the \\(R^2\\) of predicted vs. observed \"GPP_NT_VUT_REF\"? Is the linear regression slope significantly different from zero for all predictors? ## write your code here 6.3.3.2 Prediction With the model containing all predictors and fitted on ddf_train, make predictions using first ddf_train and then ddf_test. Compute the \\(R^2\\) and the root-mean-square error, and visualise modelled vs. observed values to evaluate both predictions. Do you expect the linear regression model trained on ddf_train to predict substantially better on ddf_train than on ddf_test? Why (not)? Hints: To calculate predictions, use the generic function predict() with the argument newdata = .... The \\(R^2\\) can be extracted from the model object as summary(model_object)$r.squared, or is (as the RMSE) given in the metrics data frame returned by metrics() from the yardstick library. For visualisation the model performance, consider a scatterplot, or (better) a plot that reveals the density of overlapping points. (We’re plotting information from over 4000 data points here!) ## write your code here 6.3.4 Pre-processing To get a better understanding of writing and baking recipes for pre-processing, load the dataset starwars from the dplyr package. Note that for this part of the exercise you do not need to split the data, just work with the full dataset. Do the following tasks: Encoding of factor levels: Load the dataset and select the features height, mass and species Drop all NA entries from the dataset Apply an one-hot encoding and a dummy encoding on the feature species Answer the following questions: How many columns does the un-processed original data frame have and how many columns are in the freshly “baked” data frames created by the One-hot encoding and how many in the one from the dummy-encoding? Explain the differences. Which column was created by the One-hot-encoding but not by the dummy-encoding? ## write your code here df &lt;- starwars # tidyverse has to be loaded for this code line! Sequential pre-processing: Load the dataset and select the features height, mass and species Drop all NA entries from the dataset Write and bake a recipe with the following steps: Filter out zero or near-zero variance features Standardize (center and scale) numeric features Dummy encode categorical features. Visualise the distribution of the numerical variables Answer the question: Does the order in which pre-processing steps are defined in the recipe matter? ## write your code here "],["ch-07.html", "Chapter 7 Supervised Machine Learning II 7.1 Introduction 7.2 Tutorial 7.3 Exercise 7.4 KNN 7.5 Random forest", " Chapter 7 Supervised Machine Learning II 7.1 Introduction 7.1.1 Learning objectives After this learning unit, you will be able to … Explain the effect of hyper-parameter tuning. Assess the generalisability of a trained model. Understand the purpose of resampling. Avoid data leakage during model training. 7.1.2 Key points from the lecture Training data is the data that is used to train our model - “to fit a curve to the data points”. Testing data is set aside at the initial split and not “touched” during model training. It is key to test a model’s predictive power and whether it is overfitted. Validation data is used for determining the loss during model training. The reason for distinguishing between testing and validation data is to assure we’re not misleading model training by some peculiarities of the training data and we get an assessment of generalisability based on data that was not seen during model training. This distinction might be somewhat confusing for now, have a look at this blog post for additional explanations. Model training minimises the loss. In other words, it optimises the agreement between predicted and observed values. The loss is most commonly measured by the root mean square error (RMSE). To tune a model, you can set hyperparameters that determine model structure or calibrate the coefficients. The k in KNN is such a hyperparameter. Generalisability refers to the model’s performance on data not seen during the training - the testing data. To avoid overfitting, model generalisability is desired already during model training. One method to guide model training is cross validation. Data leakage is when information from the testing dataset “creeps” into the training data. To avoid this the testing set must be left completely untouched! 7.2 Tutorial 7.2.1 Model formulation using train() In the previous tutorial you learned how to wrangle, split and pre-process data (remember the recipes) and how to formulate a linear model in base R. To learn more about model creation, let us first repeat the data preparation from the previous tutorial: ## Load data and extrat relevant subset library(tidyverse) ddf &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) %&gt;% select(starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, -contains(&quot;JSB&quot;)) %&gt;% mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %&gt;% na_if(-9999) %&gt;% select(-ends_with(&quot;_QC&quot;), NEE_VUT_REF_QC) %&gt;% mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.9, NA, GPP_NT_VUT_REF)) %&gt;% select(-NEE_VUT_REF_QC) %&gt;% drop_na() ## Split data into testing and training data library(rsample) set.seed(1982) # for reproducibility split &lt;- initial_split(ddf, prop = 0.7) ddf_train &lt;- training(split) ddf_test &lt;- testing(split) ## Base R formulation for linear model lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf) ## ## Coefficients: ## (Intercept) SW_IN_F VPD_F TA_F ## 1.08298 0.01334 -0.34573 0.30404 Ideally, the way we formulate a model should be independent of the algorithm, or engine that takes care of fitting \\(f(X)\\). As mentioned in the previous tutorial, the R package caret provides a unified interface for using different ML algorithms implemented in separate packages. In other words, caret acts as a wrapper for multiple different model fitting, or ML algorithms. This has the advantage that it unifies the interface (the way arguments are provided). caret also provides implementations for a set of commonly used tools for data processing, model training, and evaluation. We’ll use caret for model training with the function train(). Using caret for specifying the same linear regression model as above, using the base-R lm() function, can be done with caret in a generalized form as: library(caret) train( form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf, method = &quot;lm&quot; ) ## Linear Regression ## ## 5893 samples ## 3 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 5893, 5893, 5893, 5893, 5893, 5893, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.58713 0.668969 1.228773 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Of course, this is an overkill compared to just writing lm(...). But the advantage of the unified interface is that we can simply replace the method argument to use a different ML algorithm. For example, to use a random forest model implemented by the ranger package, we can write: train( form = GPP_NT_VUT_REF ~ SW_F_IN + VPD_F + TA_F, data = ddf, method = &quot;ranger&quot;, ... ) The ... hints at the fact that there are a few more arguments to be specified for training a random forest model with ranger. We learn more about these arguments below in Chapter 6.3.3.1. Another useful aspect of using caret to formulate models is that it takes recipes from the recipes packages as input. This way, we can make sure that the exact same pre-processing steps are taken for the training data, testing data and as will be introduced later, the validation data. library(recipes) pp &lt;- recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) The object pp can then be supplied to train() as its first argument: train( pp, data = ddf, method = &quot;ranger&quot;, ... ) 7.2.2 Model training Model training in supervised ML is guided by the match (or mismatch) between the predicted and observed target variable(s), that is, between \\(\\hat{Y}\\) and \\(Y\\). The loss function quantifies this mismatch (\\(L(\\hat{Y}, Y)\\)), and the algorithm takes care of progressively reducing the loss during model training. Let’s say the ML model contains two parameters and predictions can be considered a function of the two (\\(\\hat{Y}(w_1, w_2)\\)). \\(Y\\) doesn’t change as a function of $w_1$ and $w_2$. Thus, the loss function is effectively a function \\(L(w_1, w_2)\\). Therefore, we can consider the model training as a search of the parameter space of the machine learning model \\((w_1, w_2)\\) to find the minimum of the loss. Common loss functions are the root mean square error (RMSE), or the mean square error (MSE), or the mean absolute error (MAE). Loss minimization is a general feature of ML model training. Figure 7.1: Visualization of a loss function as a plane spanned by the two parameters \\(w_1\\) and \\(w_2\\). Model training is implemented in R for different algorithms in different packages. Some algorithms are even implemented by multiple packages (e.g., nnet and neuralnet for artificial neural networks). Again, caret comes to the rescue as it offers a large selection of ML model implementations from different packages (see here for an overview of available models). The train() function takes an argument metric, which specifies the loss function and defaults to the RMSE for regression models and to accuracy for classification (see chapter on metrics below). 7.2.2.1 Hyperparameter tuning All ML algorithms have some “knobs” to turn in order to achieve efficient model training and predictive performance. Such “knobs” are called hyperparameters and their meaning and effect depend on the ML algorithm. For k-nearest neighbour (KNN), it is k - the number of neighbours to consider for determining distances. There is always an optimum \\(k\\). Obviously, if \\(k = n\\), we consider all observations as neighbours and each prediction is simply the mean of all observed target values \\(Y\\), irrespective of the predictor values. This cannot be optimal and such a model is likely underfit. On the other extreme, with \\(k = 1\\), the model will be strongly affected by the noise in the single nearest neighbour and its generalisability will suffer. This should be reflected in a poor performance on the validation data. For random forests from the ranger package, hyperparameters are: mtry: the number of variables to consider to make decisions, often taken as \\(p/3\\), where \\(p\\) is the number of predictors. min.node.size: the number of data points at the “bottom” of each decision tree splitrule: the function applied to data in each branch of a tree, used for determining the goodness of a decision Hyperparameters usually have to be “tuned” and their optimal values depend on the data. Therefore they cannot be known a priori but must be found on-the-go. In caret, hyperparameter tuning is implemented as part of the train() function. Values of hyperparameters to consider are to be specified by the argument tuneGrid, which takes a data frame with column(s) named according to the name(s) of the hyperparameter(s) and rows for each combination of hyperparameters to consider. To specify the three hyperparameters of a random forest model would look like this: train( form = GPP_NT_VUT_REF ~ SW_F_IN + VPD_F + TA_F, data = ddf, method = &quot;ranger&quot;, tuneGrid = expand.grid( .mtry = floor(6 / 3), .min.node.size = c(3, 5, 9,15, 30), .splitrule = c(&quot;variance&quot;, &quot;maxstat&quot;)), ... ) Here, expand.grid() is used to generate a data frame with all possible combinations of individual vectors (e.g. min node size) and their provided values (e.g. 3, 5, 9, 15, 30). For a KNN, it suffices to enter a dataframe with one vector of numbers as argument to tuneGrid (e.g., data.frame(k = c(2, 5, 10, 15, 18, 20, 22, 24, 26, 30, 35, 40, 60, 100))). The train() function will then create a KNN for each of these numbers to find the optimal k. 7.2.2.2 Resampling The goal of model training is to achieve the best possible model generalisability. That is, the best possible model performance when predicting to data that was not used for training - the test data. Resampling mimicks the comparison of predictions to the test data. Instead of using all training data, the training data is resampled into a number further splits - pairs of training and validation data. Model training is then guided by minimising the average loss determined on each resample of the validation data. Having multiple resamples (multiple folds of training-validation splits) avoids the loss minimization from being misguided by random peculiarities in the training and/or validation data. A common resampling method is k-fold cross validation, where the training data is split into k equally sized subsets (folds). Then, there will be k iterations, where each fold is used for validation once (while the remaining folds are used for training). An extreme case is leave-one-out cross validation, where k corresponds to the number of data points. Figure 7.2: Cross-validation resampling (figure from Boehmke &amp; Greenwell (2019)). To do a k-fold cross validation during model training in R, we don’t have to implement the loops around folds ourselves. The resampling procedure can be specified in the caret function train() with the argument trControl. The object that this argument takes is the output of a function call to trainControl(). This can be implemented in two steps. For example, to do a 10-fold cross-validation, we can write: train( pp, data = ddf_train, method = &quot;ranger&quot;, tuneGrid = expand.grid( .mtry = floor(6 / 3), .min.node.size = c(3, 5, 9,15, 30), .splitrule = c(&quot;variance&quot;, &quot;maxstat&quot;)), trControl = trainControl(method = &quot;cv&quot;, number = 10), ... ) In certain cases, data points stem from different “groups”, and generalisability across groups is critical. In such cases, data from a given group must not be used both in the training and validation sets. Instead, splits should be made along group delineations. The caret function groupKFold() offers the solution for this case. 7.2.3 Model evaluation In previous chapters, you have already encountered different metrics that can be used to quantify the agreement between predicted (\\(\\hat{y}\\)) and observed (\\(y\\)) values (e.g., the root mean square error). These metrics are essential to guide model training and inform the evaluation. Different metrics measure different aspects of the model-data agreement and different metrics are used for regression and classification models. 7.2.3.1 Metrics for regression models Different metrics measure, for example, the correlation between modeled and observed values or the magnitude of the errors. To get an intuitive understanding of their different abilities, compare the scatterplots in Figure 7.3 and how different aspects of the model-data agreement are measured by different metrics. Their definitions will follow below. Figure 7.3: Comparison of model metrics on different data sets. MSE: The mean squared error is defined, as its name suggests, as: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2 \\]It measures accuracy, i.e., the magnitude of the errors, and is minimized during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is particularly sensitive to large errors in single points (including outliers). You may notice the difference to the error variance \\(\\widehat{\\sigma}^2\\). It was defined in Chapter 6 similar to the definition of the MSE above, but with \\(n-p\\) in the denominator instead of \\(n\\). The denominator \\(n-p\\) corresponds to the degrees of freedom (size of our sample minus the number of parameters to estimate). Here we want to compute the mean of the errors, hence we divide by \\(n\\). RMSE: The root mean squared error is, as its name suggests, the root of the MSE: \\[ \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2} \\]Like the MSE, the RMSE also measures accuracy (the magnitude of the errors) and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data \\(y\\) and is less sensitive to outliers as the MSE. Checkpoint Implement the formula for RMSE using simple “low-level” functions like sqrt() and mean(). Confirm that the function rmse() from the yardstick package computes the RMSE the same way. Solution ## generate random data df_demo &lt;- tibble(x = rnorm(100)) %&gt;% mutate(y_obs = x + rnorm(100), y_pred = x) yardstick::rmse(df_demo, y_obs, y_pred) %&gt;% pull(.estimate) ## [1] 1.01994 sqrt(mean((df_demo$y_pred - df_demo$y_obs)^2)) ## [1] 1.01994 \\(R^2\\), also called the coefficient of determination, describes the proportion of variation in \\(y\\) that is captured by modelled values \\(\\hat{y}\\). In this case, the goal is to maximize the metric, thus trying the explain as much variation as possible. In contrast to the MSE and RMSE, \\(R^2\\) measures consistency, or correlation, or goodness of fit, and not accuracy. It is traditionally defined as: \\[ R^2 = 1 - \\frac{\\sum_i (\\hat{y}_i - y_i)^2}{\\sum_i (y_i - \\bar{y})^2} \\]A perfect fit is quantified by \\(R^2 = 1\\) and uninformative estimates have an \\(R^2\\) approaching zero when compared to observations. Pearson’s \\(r^2\\): The linear association between to variables (here \\(y\\) and \\(\\hat{y}\\)) is measured by the Pearson’s correlation coefficient \\(r\\). Its square is closely related to the coefficient of determination and in common cases of prediction-observation comparisons almost identical. \\[ r = \\frac{\\sum_i (y_i - \\bar{y}) (\\hat{y_i} - \\hat{\\bar{y}}) }{\\sqrt{ \\sum_i(y_i-\\bar{y})^2(\\hat{y_i}-\\hat{\\bar{y}})^2 } } \\] The distinction between uppercase and lowercase nomenclature is often not consistent in the literature. The uppercase \\(R^2\\) is commonly used in the context of comparing observed and predicted values with the coefficient of determination. When the correlation between two different variables in a sample is quantified, the lowercase \\(r^2\\) is commonly used. In a linear regression with an estimated intercept, the coefficient of determination and the square of the Pearson’s correlation coefficient are equal. However, when comparing estimated and observed values, the coefficient of determination can return negative values for uninformative estimates. Metrics for correlation should not be used as a loss function because they do not penalise biased models. This is illustrated also in the plots above. The yardstick library implements the definition of the coefficient of determination with its function rsq_trad(). ## generate random data df_demo &lt;- tibble(x = rnorm(100)) %&gt;% mutate(y_obs = x + rnorm(100), y_pred = x) ## the equation given above for the coefficient of determination corresponds to &#39;rsq_trad()&#39; 1 - sum((df_demo$y_pred - df_demo$y_obs)^2) / sum((df_demo$y_obs - mean(df_demo$y_obs))^2) ## [1] 0.4611166 yardstick::rsq_trad(df_demo, y_obs, y_pred) %&gt;% pull(.estimate) ## [1] 0.4611166 The square of the Pearson’s correlation coefficient, as defined above, is implemented by the yardstick function rsq(), and corresponds also to the value reported for Multiple R-squared by summary() on a linear model object, or simply to cor(...)^2. ## the equation given above for the squared Pearson&#39;s correlation coefficient corresponds to &#39;rsq()&#39;, &#39;cor()^2&#39;, and `summary()$r.squared (sum((df_demo$y_pred - mean(df_demo$y_pred))*(df_demo$y_obs - mean(df_demo$y_obs))))^2/ (sum((df_demo$y_obs - mean(df_demo$y_obs))^2)*sum((df_demo$y_pred - mean(df_demo$y_pred))^2)) ## [1] 0.506538 yardstick::rsq(df_demo, y_obs, y_pred) %&gt;% pull(.estimate) ## [1] 0.506538 summary(lm(y_obs ~ y_pred, data = df_demo))$r.squared ## [1] 0.506538 cor(df_demo$y_obs, df_demo$y_pred)^2 ## [1] 0.506538 The \\(R^2\\) generally increases when predictors are added to a model, even if predictors are not informative. This is particularly critical in the context of machine learning when we compare alternative models that differ by their number of predictors. In other words, the \\(R^2\\) of a model with a large number of predictors tends to give an overconfident estimate of its predictive power. Above, we have seen how cross-validation can be used to assess generalisability (model performance on the validation set). This is the “gold-standard”. But when the number of data points is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalising by the number of predictors \\(p\\). Different metrics are available. Adjusted \\(R^2\\): The adjusted \\(R^2\\) discounts values by the number of predictors as \\[ \\bar{R}^2 = 1 - (1-R^2) \\; \\frac{n-1}{n-p-1} \\;, \\]where \\(n\\) (as before) is the number of observations and \\(p\\) the number of predictors. As for \\(R^2\\), the goal is to maximize it. For a fitted model in R modl, it is returned by summary(modl)$adj.r.squared. AIC: the Akaike’s Information Criterion is defined as \\[ \\text{AIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + 2(p+2) \\]where \\(N\\) is the number of observations used for estimation, \\(p\\) is the number of predictors in the model and SSE is the sum of squared estimate of errors (SSE\\(= \\sum_i (y_i-\\hat{y_i})^2\\)). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for forecasting. For large values of \\(n\\), minimising the AIC is equivalent to minimising the cross-validated MSE. AIC\\(_c\\): For small values of \\(n\\) the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as: \\[ \\text{AIC}_c = \\text{AIC} + \\frac{2(p + 2)(p + 3)}{n-p-3} \\]Also AIC\\(_c\\) is minimised for an optimal predictive model. BIC: the Schwarz’s Bayesian Information Criterion is defined as \\[ \\text{BIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + (p+2) \\log(n) \\]Also in this case our goal is to minimize the BIC. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select the model with fewer predictors than AIC. 7.2.3.2 Metrics for classification In the examples for this course, we have thus far focused on regression models. For classification, different metrics for measuring the agreement between predicted and observed values are used. They will be introduced in a later chapter. If you’re curious already now, good overviews are provided in the following links: Hands On Machine Learning in R, Bradley &amp; Boehmke for brief definitions. Wikipedia on Confusion Matrix Blogpost by M. Sunasra on Medium 7.2.3.3 Residual analysis Quantifying metrics is one part of model evaluation. The other part is to get an intuitive understanding of the model-observation agreement and where and why they fail. Getting there is an integral part of exploratory data analysis. One of the first steps after obtaining results from your initial model is to investigate its residuals (the difference between predicted and observed values) and their pattern. If you can detect a clear pattern or trend in your residuals, then your model has room for improvement. Our example data set contains time series of multiple variables. Above, we have not used time as a predictor but values in the data frame are ordered by time along rows. An obvious first step is to look at residuals and their relationship with time (or simply row number in our case). A handy function to add predictions and residuals from a fitted model to the data (must contain the variables used in the model), is augment from the tidyverse broom package (Robinson, Hayes, and Couch 2021). It adds information about observations to a data set. Let’s fit a linear regression model GPP_NT_VUT_REF ~ SW_IN_F and look at the residuals. library(broom) linmod1 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = ddf) df1 &lt;- augment(linmod1, data = ddf) df1 %&gt;% mutate(id = row_number()) %&gt;% ggplot(aes(id, .resid)) + geom_point(size = 1, alpha = .4) + xlab(&quot;Row number&quot;) + ylab(&quot;Residuals&quot;) + labs(title = &quot;Univariate linear regression&quot;, subtitle = &quot;GPP_NT_VUT_REF ~ SW_IN_F&quot;) Apparently, there is a clear pattern in the residuals with an apparent autocorrelation (the residual in one data point is correlated with the residual in its preceding data point). This suggests that there is temporal information that we did not account for in our model. This could either be from additional predictors, not included here, that have a temporal pattern (note that we only included one predictor in this model), or from the inherent dynamics of the system itself, which is not captured by the predictors (e.g., memory effects). We can also plot residuals versus additional, potentially influential but not included predictors to guide the revision of the model. However, it is often not possible to determine effects by eye, especially if interactions between variables are important. We learn about methods to sequentially add predictors to a linear regression model in the ‘Application’ session (see Chapter 8). 7.2.4 Bonus: Model interpretation Until now, we have seen how to build and train models, tune the hyperparameters and evaluate models. ML models work so well because they can effectively make use of large amounts of data and are flexible enough to model non-linear relationships and interactions, and predict rare and faint phenomena. This is their great advantage over classical statistical methods. However, this flexibility is underpinned by a high model complexity and a large number of parameters. This complexity also contrasts with traditional statistical methods, e.g., linear regression where fitted coefficients (\\(\\beta_i\\) in Chapter 6) can directly be interpreted and yield information about the sensitivity of the target variable to the different predictors and even about the statistical significance of their effect (we haven’t looked at that part). In contrast, the complexity of ML models renders their interpretation difficult. The ML algorithms’ predictive power comes at the cost of reduced model interpretability. ML models often appear to be “black boxes” which may limit their usefulness in typical applications in research, where we’re often not only interested in predicting, but also in understanding how the model arrived at its predictions. We are often interested in identifying patterns in the data that would otherwise not be visible, but that the algorithm apparently identified and learned. In order to interpret a ML model and understand its inner workings, we have to “ask specific questions” that can be translated into an evaluation of the trained model that then yields the answer we want. There are basically two types of questions we can ask: How “important” is each predictor variable in our model? This is answered by variable importance analysis. What’s the functional relationship between the target variable and each predictor? This is answered by partial residual analysis. 7.2.4.1 Variable importance Model specific approach (for linear regression) Some of the approaches for evaluating feature importances are model-specific. For instance, the absolute value of t-statistic, in case of linear models, as a measure of feature importance. Such model-specific interpretation tools are limited to their respective model classes. There can be some advantages to using these model-specific approaches as they are more closely linked to the model and its performance, and can thus directly use parameters (coefficients) of the fitted model. For linear regression models, we can quantify the significance of its coefficients \\(\\beta\\), by testing a null-hypothesis that the coefficient is, in reality, zero. As an example, let’s assume that our data are generated from the following linear model \\[ y = \\beta_0 + \\beta_1 x + \\epsilon \\] where we assume that our errors \\(\\epsilon\\) are independent and come from a normal distribution. Given only one realization of our data, our aim is to estimate \\(\\beta_0\\) and \\(\\beta_1\\). Let’s generate random data for a univariate linear relationship where the true coefficients are \\(\\beta_1 = 3\\) and \\(\\beta_0 =2\\). Note that in practice we do not know the real coefficients and we don’t know what the true predictors are. Let’s consider an additional predictor that is available in our data but does not contribute to the actual data generation process (x2). We include it in the model and want to know whether the respective coefficient it yields is significant. How can we determine information about its importance? As we’ve seen before, the summary() function generates a human-readable output. What information does it provide about variable importance? ## generate random data library(dplyr) set.seed(123) n &lt;- 100 # sample size b_1 &lt;- 3 b_0 &lt;- 2 df_demo &lt;- data.frame(x1 = rnorm(n), x2 = rnorm(n)) %&gt;% mutate(y = b_1 * x1 + b_0 + rnorm(n ,mean = 0 , sd = 1)) # no x2 here ## fit model linmod_demo &lt;- lm(y ~ x1 + x2, data = df_demo) # x2 here because it&#39;s in the data - is it significant? summary(linmod_demo) ## ## Call: ## lm(formula = y ~ x1 + x2, data = df_demo) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8730 -0.6607 -0.1245 0.6214 2.0798 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.13507 0.09614 22.208 &lt;2e-16 *** ## x1 2.86683 0.10487 27.337 &lt;2e-16 *** ## x2 0.02381 0.09899 0.241 0.81 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9513 on 97 degrees of freedom ## Multiple R-squared: 0.8853, Adjusted R-squared: 0.8829 ## F-statistic: 374.3 on 2 and 97 DF, p-value: &lt; 2.2e-16 You see that the linear regression fit yields estimates of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that are relatively close to the real ones (2.13507 for \\(\\beta_0\\) instead of 2, 2.86683 for \\(\\beta_1\\) instead of 3). We also see that the estimate (Estimate) for (the fake) coefficient x2 is close to zero. The standard error of the coefficient estimate Std. Error is an estimate of the standard deviation of coefficient estimates we would get if the above random data generation process was repeated many times. It decreases with the sample size (n in the code above). The t-statistic (t value) is \\(\\frac{\\mbox{Estimate}}{\\mbox{Std.Error}}\\). Assuming a t-distribution of coefficient estimates, the p-value (Pr(&gt;|t|)) quantifies the probability of our coefficient estimate (considering its t-statistic) if the true coefficient was zero - our null hypothesis. In our example, the p-value for the x2 estimate is 0.81. This is not significant at any significance level (indicated by the * to the right of the reported p-value, key given by Signif. codes). In contrast, the estimates for coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are highly significant. Their very low p-values indicate that it is highly unlikely that their true value was zero. This is illustrated below. n &lt;- 100 grid &lt;- seq(-7, 7, length.out = 1000) t_dist &lt;- dt(grid, df = n-3) # 3 is the number of estimated coefficients ggplot()+ geom_line(aes(x = grid, y = t_dist,color = &#39;Null Distribution&#39;), lwd=1.5) + geom_vline(aes(xintercept = 22.208, linetype = &#39; t value \\n for intercept&#39;), color=&#39;red&#39;) + scale_color_manual(name = &#39;&#39;,values = &#39;black&#39;) + scale_linetype_manual(name=&#39;&#39;, values = 2) + xlim(c(-5,25)) + labs(x = &#39;t value&#39;, y = &#39;Density&#39;, title = &#39;Students t-distribution&#39;) + theme(legend.position = &#39;top&#39;) + guides(color = guide_legend(order = 1), linetype = guide_legend(order = 2)) ggplot()+ geom_line(aes(x = grid, y = t_dist, color = &#39;Null Distribution&#39;), lwd=1.5) + geom_vline(aes(xintercept = 0.241, linetype = &#39; t value \\n for x2&#39;), color=&#39;red&#39;) + scale_color_manual(name = &#39;&#39;,values = &#39;black&#39;) + scale_linetype_manual(name=&#39;&#39;, values = 2) + xlim(c(-5,25)) + labs(x = &#39;t value&#39;, y = &#39;Density&#39;, title = &#39;Students t-distribution&#39;) + theme(legend.position = &#39;top&#39;) + guides(color = guide_legend(order = 1), linetype = guide_legend(order = 2)) Variable importance in a fitted model can be assessed in R using the vip package (Greenwell and Boehmke 2020). If we provide a linear regression model as argument, it automatically uses the absolute value of the t-statistic for quantifying the variable importance. library(vip) linmod3 &lt;- lm(GPP_NT_VUT_REF ~ ., data = ddf %&gt;% dplyr::select(-TIMESTAMP)) vip(linmod3) Checkpoint Verify that the bars plotted in the vip plot correspond to the t-statistic of the linear regression model. Solution summary(linmod3) # compare with values under `t-value` ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ ., data = ddf %&gt;% dplyr::select(-TIMESTAMP)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1828 -1.0365 -0.1325 0.8901 7.7224 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.929937 2.899304 -1.355 0.175 ## TA_F 0.158395 0.008958 17.682 &lt; 2e-16 *** ## SW_IN_F 0.013815 0.000326 42.381 &lt; 2e-16 *** ## LW_IN_F 0.020617 0.001004 20.535 &lt; 2e-16 *** ## VPD_F -0.137645 0.016515 -8.334 &lt; 2e-16 *** ## PA_F -0.004198 0.033688 -0.125 0.901 ## P_F -0.017538 0.003979 -4.407 1.06e-05 *** ## WS_F -0.150129 0.033470 -4.486 7.41e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.537 on 5885 degrees of freedom ## Multiple R-squared: 0.6906, Adjusted R-squared: 0.6902 ## F-statistic: 1877 on 7 and 5885 DF, p-value: &lt; 2.2e-16 The results of the feature importance plot using vip(), agree with the statistics obtaind from the summary() function. The variables LW_IN_F and VPD_F have largest \\(|t|\\) statistic values (thus, the values on the tail ends of the t-distribution have low measure), which results in a small p-value, thus indicating that these are the most important features. On the other end, the variable P_F has the smallest \\(|t|\\) statistc, which results in a large p-value, thus indicating that this variable is not as important. To sum this up, variable importance can be measured for linear regression models using the t-statistic. While t-statistic values provide information for feature importance analysis that is straight-forward to quantify and interpret, corresponding information has to be extracted by other means for other ML algorithms (e.g., accuracy degradation in random forests). This makes it difficult to compare their results across different classes of ML models. Model-Agnostic approaches Model-Agnostic approaches offer a solution to the intercomparability problem for feature importance analysis. These treat the ML algorithm as a “black-box”, and the separation of the specific model types from their interpretability measure enables us to compare the feature importance across different models and ML algorithms. Permutation-based feature importance is such an algorithm. Its idea is that if we permute (re-shuffle) the values of a feature in the training data, we effectively destroy the relationship between that particular feature and the target variable. Thus, if the feature is important, the model performance should degrade substantially. In contrast, for unimportant features, permuting their values should not degrade the model performance much. Algorithmically speaking, this works as follows: We first compute the loss for the original model to establish a reference loss. For predictor \\(i\\) in \\(\\{1,2, …, p\\}\\) Randomly permute values of predictor \\(i\\) in the dataset. Train the model. Compute the loss. Compute the feature importance, e.g., as the ratio or difference of the loss with the permuted data and the reference loss. Store this as the feature importance for feature \\(i\\) Sort the variable by their feature importance This can be easily implemented using the vip package along with a few extra parameters Arguments to the function vip(), for permutation based feature importance linmod3: A fitted model object train : the training data to be used to compute the feature importances method : \"permute\" for permutation based method target : specifying the target variable for the train datset metric : \"RMSE\" root mean squared error for the regression based task n_sim : number of times the simulation is to be repeated, the result is averaged over all the simulations ; choose greater values to increase the stability of estimates sample_frac : specifies the proportion of the training data to be used in each simulation. Default value is NULL i.e. all of training data is used pred_wrapper : Prediction function that requires two arguments, object and newdata. The output of this function should be determined by the metric being used - A numeric vector of predicted outcomes for a regression task - A vector of predicted class labels, or a matrix of predicted class probabilitites for a classification task type : Character string specifying how to compare the baseline and permuted performance metrics; default value is 'difference', and the other option is 'ratio' Note: In case of large datasets, n_sim and sample_frac can be used to reduce the execution time of the algorithm, as permutation based approaches can become slow as the number of predictors grows. # inline function defined to output the predictions for newdata given a particular model object pfun &lt;- function(object, newdata) predict(object, newdata = newdata) ## variable importance plot using the permute method, with type == &#39;ratio&#39; vip( linmod3, train = ddf %&gt;% dplyr::select(-TIMESTAMP), method = &quot;permute&quot;, target = &quot;GPP_NT_VUT_REF&quot;, metric = &quot;RMSE&quot;, nsim = 5, sample_frac = 0.8, pred_wrapper = pfun, type = &#39;ratio&#39; ) ## variable importance plot using the permute method, with type == &#39;difference&#39; vip( linmod3, train = ddf %&gt;% dplyr::select(-TIMESTAMP), method = &quot;permute&quot;, target = &quot;GPP_NT_VUT_REF&quot;, metric = &quot;RMSE&quot;, nsim = 5, sample_frac = 0.8, pred_wrapper = pfun, type = &#39;difference&#39; ) Now we can compare the importance plots obtained from the model-specific t-statistic approach and the model agnostic permuation based method. The feature importances obtained from both approaches differ a little, because of the underlying differences in the feature importance evaluation. Despite of the differences, both methods robustly return the key important features, particularly their ranks. Checkpoint Compute the variable importance for the best fit knn model, which we obtained earlier in the tutorial, using the vip() function in default mode. Repeat the same with permuatation based method, with type as ‘difference’ and ‘ratio’. Do the two return the same top-4 features ? Verify. Solution ## Data wrangling done above ## Set up recipe library(recipes) pp &lt;- recipe(GPP_NT_VUT_REF ~ ., data = ddf_train %&gt;% dplyr::select(-TIMESTAMP)) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% # normalizes numeric data to have a mean of zero step_scale(all_numeric(), -all_outcomes()) ## Set up knn my_cv &lt;- trainControl( method = &quot;repeatedcv&quot;, # method define the resampling method such as &#39;boot&#39;, &#39;none&#39;, &#39;cv&#39;, etc. number = 10, # number of folds or number of resampling iterations repeats = 5 # the number of complete sets of folds to compute (only for repeated k-fold cross-validation) ) hyper_grid &lt;- data.frame(k = c(2, 5, 10, 15, 18, 20, 22, 24, 26, 30, 35, 40, 60, 100)) set.seed(123) knn_fit &lt;- train( pp, data = ddf_train %&gt;% dplyr::select(-TIMESTAMP), method = &quot;knn&quot;, trControl = my_cv, tuneGrid = hyper_grid, metric = &quot;RMSE&quot; ) ## variable importance plot without the permute method vip(knn_fit) ## variable importance plot using the permute method, with type == &#39;ratio&#39; vip( knn_fit, train = ddf %&gt;% dplyr::select(-TIMESTAMP), method = &quot;permute&quot;, target = &quot;GPP_NT_VUT_REF&quot;, metric = &quot;RMSE&quot;, nsim = 5, sample_frac = 0.8, pred_wrapper = pfun, type = &#39;difference&#39; ) ## variable importance plot using the permute method, with type == &#39;difference&#39; vip( knn_fit, train = ddf %&gt;% dplyr::select(-TIMESTAMP), method = &quot;permute&quot;, target = &quot;GPP_NT_VUT_REF&quot;, metric = &quot;RMSE&quot;, nsim = 5, sample_frac = 0.8, pred_wrapper = pfun, type = &#39;ratio&#39; ) 7.2.4.2 Partial dependence plots While variable importance analysis is helpful for understanding which predictors are helping us predict, partial dependence plots (PDPs) yield information on what the relationship between one individual predictor and the target variable looks like. It quantifies how the response of the target variable changes when we vary one particular predictor and hold all the remaining predictors at a constant value. If this is still a little muddy, the pseudo-code of this algorithm should make things clear (see this by Brad Boehmke and Greenwell (2019)). For a selected predictor (x) 1. Construct a grid of j evenly spaced values across the range of x: {x1, x2, ..., xj} 2. For i in {1,...,j} do | Copy the training data and replace the original values of x with the constant xi | Apply the fitted ML model to obtain vector of predictions for each data point. | Average predictions across all data points. End 3. Plot the averaged predictions against x1, x2, ..., xj As the steps explain, PDPs plot the change in the average predicted value (\\(\\hat{y}\\)) as a function of a specified feature. As you will see in later chapters, PDPs become more useful when non-linear relationships are present. However, PDPs of linear models help illustrate how a fixed change in \\(x_i\\) relates to a fixed linear change in \\(\\hat{y_i}\\) while taking into account the average effect of all the other features in the model. For linear regression models, the slope of the PDP is equal to the corresponding coefficient of the model. Considering Figure 7.4, to compute the PDP of target variable with respect to Gr_Liv_Area, we modify the values of this variable for all the rows in the data set, for each [j1, j2, …, j20] in the grid. Feed this modified dataset to the pre-trained model and compute the mean of our target variable (mean taken over all rows). Finally at the end of the algorithm we have 20 mean responses of the target variable for 20 different values of Gr_Liv_Area. Finally we plot the mean response of the target variable with respect to the changing variable Gr_Liv_Area to get the parital dependence plot. Figure 7.4: Visualiation of a partial dependence plot (from Boehmke &amp; Greenwell (2019)). Lucky for us, this code has already been implemented in the pdp package in R (Greenwell 2017). We use the partial() function to plot the partial dependence plot for a single predictor variable, SW_IN_F or VPD_F, or for a subset of both these variables. Since our models are linear, we expect the PDP curves to be linear as well. The first curve shows a positive linear relationship between SW_IN_F and the target y_hat (GPP_NT_VUT_REF), i.e. as the SW_IN_F increases, the target increases. Whereas the second curve shows us that there is a negative linear dependence between VPD_F and GPP_NT_VUT_REF. The final plot shows a 2D partial dependence plot with SW_IN_F and VPD_F, and this follows a linear relationship too. As expected, we get the highest value for our target variable in the lower right corner and the smallest value in the upper left corner. library(pdp) partial(linmod3, &quot;SW_IN_F&quot;, plot = TRUE) partial(linmod3, &quot;VPD_F&quot;, plot = TRUE) partial(linmod3, c(&quot;SW_IN_F&quot;, &quot;VPD_F&quot;), plot = TRUE) The pdp package works with various model classes. For classes that are not available, a prediction wrapper function has to be specified as described for example here (Brad Boehmke and Greenwell 2019). If you want to learn more about PDP, have a look the online book by Molnar (2019). Note: The computational demand of partial dependence analysis rapidly increases as the number of search grid points (j in the algorithm description above) rises. For the interested: If you do not want to average the y_hat, predictions over all the rows for each value of \\(X_i\\), look into Individual conditional expectation (ICE) curves, which rather than averaging the predicted values across all observations, observe and plot the individual observation-level predictions. 7.3 Exercise 7.3.1 Setup The following exercise builds on the exercise of the previous tutorial. So let’s load the work done there first: ## Load data and extrat relevant subset library(tidyverse) ddf &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) %&gt;% select(starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, -contains(&quot;JSB&quot;)) %&gt;% mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %&gt;% na_if(-9999) %&gt;% select(-ends_with(&quot;_QC&quot;), NEE_VUT_REF_QC) %&gt;% mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.9, NA, GPP_NT_VUT_REF)) %&gt;% select(-NEE_VUT_REF_QC) %&gt;% drop_na() ## Split data into testing and training data library(rsample) set.seed(1982) # for reproducibility split &lt;- initial_split(ddf, prop = 0.7) ddf_train &lt;- training(split) ddf_test &lt;- testing(split) 7.3.2 Linear Model In the previous exercise, you fitted a linear regression model using the base-R function lm() and the training set. The target variable was \"GPP_NT_VUT_REF\", and predictor variables were all available meterological variables in the dataset. First, repeat these same steps but use caret and the function train() for fitting the same linear regression model (with all predictors) on the same data. You will have to set the argument trControl accordingly to avoid resampling, and instead fit the model on all data in ddf_train. Does it yield identical results as using lm() directly? Second, repeat the visualisation and display of model metrics. Feel free to copy-paste from the previous exercise /solutions. You are repeating this step mainly to refresh the previous results and to compare them to your work on the next tasks. Third, in the previous exercise we found that PA_F was not significant. Build another linear model with train() but this time without using PA_F as predictor. Can you find a model metric that supports dropping PA_Ffrom further analysis? Hints: To specify the argument trControl you will have to use the function trainControl. See the tutorial for the correct ssyntax and enter ?trainControl into the console to find out how to specify the right method. You can use summary() also on the object returned by the function train() (similar to the object returned by lm()). To support dropping PA_F, you might want to consider one of the introduced information criteria. ## write your code here ## 1. Setting up a linear model with train() ## 2. Repeat visualisation and display of model metrics from Exercise 06 ## 3. Compare model performance with and without `PA_F` as predictor 7.4 KNN 7.4.1 Check data The variable PA_F was not significant in the linear model. Exclude it from further analysis by specifying a “recipe” where GPP_NT_VUT_REF is the target variable, and all available variables except PA_F and TIMESTAMP are used as predictors. ## write your code here 7.4.2 Training Fit two KNN models on ddf_train (excluding \"PA_F\"), one with \\(k = 2\\) and one with \\(k = 30\\), both without resampling. Use the RMSE as the loss function. Center and scale data as part of the pre-processing and model formulation specification using the function recipe(). ## write your code here 7.4.3 Prediction With the two models fitted above, predict \"GPP_NT_VUT_REF\" for both and training and the testing sets, and evaluate them as above (metrics and visualisation). Which model do you expect to perform better on the training set and which to perform better on the testing set? Why? Do you find evidence for overfitting in any of the models? Hint: Make use of the visualization code that you used above and in Exercise 06. ## write your code here 7.4.4 Sample hyperparameters Train a KNN model with hyperparameter (\\(k\\)) tuned, and with five-fold cross validation, using the training set. As the loss function, use RMSE. Sample the following values for \\(k\\): 2, 5, 10, 15, 18, 20, 22, 24, 26, 30, 35, 40, 60, 100. Visualise the RMSE as a function of \\(k\\). Hint: The visualisation of cross-validation results can be visualised with the plot(model_object) of ggplot(model_object). ## write your code here 7.5 Random forest 7.5.1 Training Fit a random forest model with ddf_train and all predictors excluding \"PA_F\" and five-fold cross validation. Use RMSE as the loss function. Hints: Use the package ranger which implements the random forest algorithm. See here for information about hyperparameters available for tuning with caret. Set the argument savePredictions = \"final\" of function trainControl(). ## write your code here 7.5.2 Prediction Evaluate the trained model on the training and on the test set, giving metrics and a visualisation as above. How are differences in performance to be interpreted? Compare the performances of linear regression, KNN, and random forest, considering the evaluation on the test set. Hint: Make use of the visualization code that you used above and in Exercise 06. ## write your code here References "],["ch-08.html", "Chapter 8 Application 1: Variable selection 8.1 Introduction 8.2 Application", " Chapter 8 Application 1: Variable selection 8.1 Introduction In Chapter 7, we noted that the coefficient of determination \\(R^2\\) may increase even when uninformative predictors are added to a model. This will ascribe some predictive power to an uninformative predictor that is in fact misguided by its (random) correlation with the target variable. Often, we start out formulating models, not knowing beforehand what predictors should be considered and we are tempted to use them all because the full model will always yield the best \\(R^2\\). In such cases, we’re prone to building overconfident models that perform well on the training set, but will not perform well when predicting to new data. In this application session, we’ll implement an algorithm that sequentially searches the best additional predictor to be included in our model, starting from a single one. This is called stepwise-forward regression (see definition below). There is also stepwise-backward regression where predictors are sequentially removed from a model that includes them all. The challenge is that we often lack the possibility to confidently assess generalisability. The effect of spuriously increasing \\(R^2\\) by adding uninformative predictors can be mitigated, as we noted in Chapter 7, by considering alternative metrics that penalize the number of predictors in a model. They balance the tradeoff between model complexity (number of variables in the linear regression case) and goodness of fit. Such metrics include the adjusted-\\(R^2\\),the Akaike Information Criterion (AIC), or the Bayesian Information Criterion (BIC). In cases, where sufficient data is available, also cross-validation can be used for assessing the generalisability of alternative models. Here, we’ll assess how these different metrics behave for a sequence of linear regression models with an increasing number of predictors. You’ll learn to write code that implements an algorithm determining the order in which variables enter the model, starting from one and going up to fourteen predictors. You’ll write your own stepwise-forward regression code. Let’s get started! 8.2 Application 8.2.1 Warm-up 1: Nested for-loop Given a matrix A and a vector B (see below), do the following tasks: Replace the missing values (NA) in the first row of A by the largest value of B. After using that element of B for imputing A, drop that element from the vector B and proceed with imputing the second row of A, using the (now) largest value of the updated vector B, and drop that element from B after using it for imputing A. Repeat the same procedure for all four rows in A. After imputing (replacing) in each step, calculate the mean of the remaining values in B and record it as a single-row data frame with two columns row_number and avg, where row_number is the row number of A where the value was imputed, and avg is the mean of remaining values in B. As the algorithm proceeds through rows in A, sequentially bind the single-row data frame together so that after completion of the algorithm, the data frame contains four rows (corresponding to the number of rows in A). A &lt;- matrix(c(6, 7, 3, NA, 15, 6, 7, 8, 9, 12, 6, 11, NA, 3, 9, 4, 7, 3, 21, NA, 6, 7, 19, 6, NA, 15, 8, 10), nrow = 4, byrow = TRUE) B &lt;- c(8, 4, 12, 9, 15, 6) Before implementing these tasks, try to write down a pseudo code. This is code-like text that may not be executable, but describes the structure of real code and details where and how major steps are implemented. Next, you’ll need to write actual R code. For this, you will need to find answers to the following questions: How to go through each of the element in matrix? How to detect NA value? How to drop an element of a given value from a vector? How to add a row to an existing data frame? Solution: library(tidyverse) summ &lt;- data.frame() for (i in 1:nrow(A)){ for (j in 1:ncol(A)){ if (is.na(A[i,j])){ A[i,j] &lt;- max(B) } } B &lt;- B[-which(B == max(B))] # update the B vector removing the biggest values summ &lt;- bind_rows(summ, data.frame(row_number = i, avg = mean(B))) } summ ## row_number avg ## 1 1 7.80 ## 2 2 6.75 ## 3 3 6.00 ## 4 4 5.00 8.2.2 Warm-up 2: Find the best single predictor The math behind forward stepwise regression: Let \\(\\mathcal{M_0}\\) denote the null model, which contains no predictors. For \\(k=0,..,p-1\\): Consider all \\(p − k\\) models that augment \\(\\mathcal{M}_k\\) with one additional predictor. Choose the best model among these \\(p − k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having the highest \\(R^2\\) . Select a single best model from among \\(\\mathcal{M}_0\\), . . . , \\(\\mathcal{M}_p\\) using cross-validated prediction error, AIC, BIC, or adjusted \\(R^2\\). The first step of a stepwise forward regression is to find the single most powerful predictor in a univariate linear regression model for the target variable GPP_NT_VUT_REF among all fourteen available predictors in our data set (all except those of type date or character). Implement this first part of the search, using the definition of the stepwise-forward algorithm above. Remove all rows with at least one missing value before starting the predictor search. Which predictor achieves the highest \\(R^2\\)? What value is the \\(R^2\\)? Visualise \\(R^2\\) for all univariate models, ordered by their respective \\(R^2\\) values. Do you note a particular pattern? Which variables yield similar \\(R^2\\)? How do you expect them to be included in multivariate models of the subsequent steps in the stepwise forward regression? Hints: Model structure: The “counter” variables in the for loop can be provided as a vector, and the counter will sequentially take on the value of each element in that vector. For example: for (var in all_predictors){ ... }. Algorithm: To record \\(R^2\\) values for the different models, you may start by creating an empty vector (vec &lt;- c()) before the loop and then sequentially add elements to that vector inside the loop (vec &lt;- c(vec, new_element)). Alternatively, you can do something similar, but with a data frame (initialising with df_rsq &lt;- data.frame() before the loop, and adding rows by df_rsq &lt;- bind_rows(df_rsq, data.frame(pred = predictor_name, rsq = rsq_result)) inside the loop). A clever way how to construct formulas dynamically is described, for example in this stackoverflow post. Value retrieving: Extract the \\(R^2\\) from the linear model object: summary(fit_lin)[[\"r.squared\"]] Visualising: Search for solutions for how to change the order of levels to be plotted yourself. library(tidyverse) ## read CSV and drop missing values in one step df &lt;- read_csv(&quot;./data/ddf_for_08_application.csv&quot;) %&gt;% drop_na() ## specify target variable target &lt;- &#39;GPP_NT_VUT_REF&#39; ## determine predictors as all except site ID, timestamp, and the target (should be 14) preds &lt;- df %&gt;% dplyr::select(-siteid, -TIMESTAMP, -GPP_NT_VUT_REF) %&gt;% names() ## initialise an empty data frame (necessary, because otherwise we cannot use bind_rows() below) df_rsq &lt;- data.frame() # rsq_list &lt;- c() # alternative for vector for (var in preds){ ## create formula dynamically forml &lt;- as.formula(paste(target, &quot;~&quot;, var)) ## fit linear model fit_lin &lt;- lm(forml, data = df) ## extract R2 from linear model rsq &lt;- summary(fit_lin)[[&quot;r.squared&quot;]] ## add a row to the data frame that holds the results df_rsq &lt;- bind_rows(df_rsq, data.frame(pred = var, rsq = rsq)) # rsq_list &lt;- c(rsq_list,rsq) # alternative with vector } ## print best single predictor and its R2 df_rsq %&gt;% arrange(-rsq) %&gt;% # arrange by R2 in descending order (highest R2 on top) slice(1) # print only the first row (with highest R2) ## pred rsq ## 1 PPFD_IN 0.5276123 library(ggplot2) ## alternative: determine the first variable to enter into our model # preds[which.max(rsq_list)] ## use the data frame that holds the results for plotting df_rsq %&gt;% ggplot(aes(x = reorder(pred, rsq), y = rsq)) + geom_bar(stat = &quot;identity&quot;) + labs(y = expression(italic(R)^2), x = &quot;Variable&quot;) + coord_flip() Solution: PPFD_IN achieves the highest \\(R^2\\) value, and the corresponding \\(R^2\\) values is 0.5276123. We could see from the above plot that LW_IN_F_MDS and LW_IN_F, VPD_F and VPD_F_MDS, SW_IN_F and SW_IN_F_MDS share basically the same \\(R^2\\) values. Those variables are highly correlated and are explaining the same amount of variance in the target variable. In the subsequent steps of the stepwise forward regression, since each of the two variables are highly correlated with each other, it’s likely that only one of them will be selected into our final model. 8.2.3 Full stepwise regression Now, we take it to the next level and implement a full stepwise forward regression as described above. For each step (number of predictors \\(k\\)), record the following metrics: \\(R^2\\), adjusted-\\(R^2\\), the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and the 5-fold cross-validation \\(R^2\\) and RMSE. Write pseudo-code for how you plan to implement the algorithm first. Implement the algorithm in R, run it and display the order in which predictors enter the model. Display a table with the metrics of all \\(k\\) steps, and the single variable, added at each step. Hints: Model structure: Recall what you learned in the breakout session, you may use the same idea on this task. Try to think of the blueprint (pseudo-code) first: How to go through different models in each forward step? How to store predictors added to the model and how to update candidate predictors? Algorithm: A complication is that the set of predictors is sequentially complemented at each step of the search through \\(k\\). You may again use vec &lt;- list() to create an empty vector, and then add elements to that vector by vec &lt;- c(vec, new_element). It may be helpful to explicitly define a set of “candidate predictors” that may potentially be added to the model as a vector (e.g., preds_candidate), and define predictors retained in the model from the previous step in a separate vector (e.g., preds_retained). In each step, search through preds_candidate, select the best predictor, add it to preds_retained and remove it from preds_candidate. At each step, record the metrics and store them in a data frame for later plots. As in the first “warm-up” exercise, you may record metrics at each step as a single-row data frame and sequentially stack (bind) them together. (As above) A clever way how to construct formulas dynamically is described, for example in this stackoverflow post. The metrics for the \\(k\\) models are assessed after the order of added variables is determined. To be able to determine the metrics, the \\(k\\) models can be saved by constructing a list of models and sequentially add elements to that list (mylist[[ name_new_element ]] &lt;- new_element). You can also fit the model again after determining which predictor worked best. Your code will most certainly have bugs at first. To debug efficiently, write code first in a simple R script and use the debugging options in RStudio (see here). Value retrieving To get AIC and BIC values for a given model, use the base-R functions AIC() and BIC(). To get the cross-validated \\(R^2\\) and RMSE, use the caret function train() with RMSE as the loss function, and method = \"lm\" (to fit a linear regression model). Then extract the values by trained_model$results$Rsquared and trained_model$results$RMSE. Displaying: To display a table nicely as part of the RMarkdown html output, use the function knitr::kable() To avoid reordering of the list of variable names in plotting, change the type of variable names from “character” to “factor” by pred &lt;- factor(pred, levels = pred) library(caret) # need train() for cross-validation ## specify target variable (as above) target &lt;- &#39;GPP_NT_VUT_REF&#39; ## determine predictors as all except site ID, timestamp, and the target (should be 14) (as above) preds &lt;- df %&gt;% dplyr::select(-siteid, -TIMESTAMP, -GPP_NT_VUT_REF) %&gt;% names() # This is the vector of candidate predictors to be added in the model. To begin with, consider all as candidates. preds_candidate &lt;- preds # predictors retained in the model from the previous step. To begin with, is empty. preds_retained &lt;- c() ## work with lists as much as possible (more flexible!) df_metrics &lt;- data.frame() ## outer loop for k predictors for (k_index in 1:length(preds)){ # rsq_candidates &lt;- c() df_rsq_candidates &lt;- data.frame() linmod_candidates &lt;- list() ## inner loop for single additional predictor for (ipred in preds_candidate){ # variable vector (new variable + retained variables) used in regression pred_add &lt;- c(preds_retained, ipred) # define formulate with newly-added predictor forml &lt;- as.formula(paste( target, &#39;~&#39;, paste(pred_add, collapse = &#39;+&#39;))) # fit linear model fit_lin &lt;- lm(forml, data = df) # add model object to list, and name the element according to the added variable linmod_candidates[[ ipred ]] &lt;- fit_lin # record metrics for all candidates rsq &lt;- summary(fit_lin)[[&quot;r.squared&quot;]] df_rsq_candidates &lt;- bind_rows(df_rsq_candidates, data.frame(pred = ipred, rsq = rsq)) # when storing R2 in a data frame # rsq_candidates &lt;- c(rsq_candidates, rsq) # when storing R2 as a vector } ## get name of candidate predictor that achieved the highest R2. pred_add &lt;- df_rsq_candidates %&gt;% # when storing R2 in a data frame arrange(desc(rsq)) %&gt;% slice(1) %&gt;% pull(pred) %&gt;% as.character() # pred_add &lt;- preds_candidate[ which.max(rsq_candidates) ] # when storing R2 as a vector ## add best predictors to retained predictors preds_retained &lt;- c(preds_retained, pred_add) ## get the cross-validation R2 forml &lt;- as.formula(paste( target, &#39;~&#39;, paste(preds_retained, collapse = &#39;+&#39;))) control &lt;- trainControl(method = &quot;cv&quot;, number = 5) set.seed(123) # cv_modl &lt;- train(forml, data = df, method = &quot;lm&quot;, , metric = &quot;Rsquared&quot;, trControl = control) cv_modl &lt;- train(forml, data = df, method = &quot;lm&quot;, metric = &quot;RMSE&quot;, trControl = control) # record AIC and BIC and adjusted-R2 of the respective model df_metrics &lt;- df_metrics %&gt;% bind_rows( data.frame( pred = pred_add, rsq = summary(linmod_candidates[[ pred_add ]])[[&quot;r.squared&quot;]], adj_rsq = summary(linmod_candidates[[ pred_add ]])[[&quot;adj.r.squared&quot;]], cv_rsq = cv_modl$results$Rsquared, cv_rmse = cv_modl$results$RMSE, aic = AIC(linmod_candidates[[ pred_add ]]), bic = BIC(linmod_candidates[[ pred_add ]]) ) ) # remove the selected variable from the candidate variable list preds_candidate &lt;- preds_candidate[-which(preds_candidate == pred_add)] # preds_candidate &lt;- setdiff(preds_candidate,pred_add) # alternative } data.frame(df_metrics$pred) # order in which variables enter the model ## df_metrics.pred ## 1 PPFD_IN ## 2 LW_IN_F_MDS ## 3 VPD_F ## 4 WS_F ## 5 SW_IN_F_MDS ## 6 CO2_F_MDS ## 7 TA_F ## 8 PA_F ## 9 TA_F_MDS ## 10 USTAR ## 11 P_F ## 12 VPD_F_MDS ## 13 LW_IN_F ## 14 SW_IN_F df_metrics %&gt;% knitr::kable() pred rsq adj_rsq cv_rsq cv_rmse aic bic PPFD_IN 0.5276123 0.5274601 0.5286098 2.946732 15529.18 15547.30 LW_IN_F_MDS 0.6237535 0.6235110 0.6244311 2.631186 14824.62 14848.78 VPD_F 0.6482531 0.6479128 0.6488913 2.544541 14617.55 14647.76 WS_F 0.6640335 0.6636000 0.6644851 2.486970 14477.03 14513.28 SW_IN_F_MDS 0.6669387 0.6664013 0.6672468 2.476793 14452.07 14494.35 CO2_F_MDS 0.6696209 0.6689811 0.6695521 2.467823 14428.96 14477.28 TA_F 0.6708789 0.6701350 0.6703978 2.464609 14419.11 14473.48 PA_F 0.6726644 0.6718185 0.6718171 2.459555 14404.22 14464.63 TA_F_MDS 0.6730699 0.6721192 0.6642220 2.487336 14402.37 14468.82 USTAR 0.6733802 0.6723246 0.6639334 2.488412 14401.42 14473.91 P_F 0.6735098 0.6723487 0.6638548 2.488747 14402.19 14480.72 VPD_F_MDS 0.6735755 0.6723086 0.6624537 2.493879 14403.57 14488.14 LW_IN_F 0.6736125 0.6722398 0.6623996 2.494079 14405.22 14495.83 SW_IN_F 0.6736372 0.6721585 0.6620120 2.495602 14406.98 14503.63 Visualise all metrics as a function of the number of predictors (add labels for the variable names of the added predictor). Highlight the best-performing model based on the respective metric. How many predictors are in the best performing model, when assessed based on each metric? library(ggplot2) df_metrics$pred &lt;- factor(df_metrics$pred, levels = df_metrics$pred) ggplot() + geom_point(data = df_metrics, aes(x = pred, y = rsq)) + geom_point(data = filter(df_metrics, rsq == max(rsq)), aes(x = pred, y = rsq), color = &quot;red&quot;) + labs(title = expression(italic(R)^2)) + coord_flip() ggplot() + geom_point(data = df_metrics, aes(x = pred, y = adj_rsq)) + geom_point(data = filter(df_metrics, adj_rsq == max(adj_rsq)), aes(x = pred, y = adj_rsq), color = &quot;red&quot;) + labs(title = expression(paste(&quot;Adjusted-&quot;, italic(R)^2))) + coord_flip() ggplot() + geom_point(data = df_metrics, aes(x = pred, y = cv_rsq)) + geom_point(data = filter(df_metrics, cv_rsq == max(cv_rsq)), aes(x = pred, y = cv_rsq), color = &quot;red&quot;) + labs(title = expression(paste(&quot;Cross-validated &quot;, italic(R)^2))) + coord_flip() ggplot() + geom_point(data = df_metrics, aes(x = pred, y = cv_rmse)) + geom_point(data = filter(df_metrics, cv_rmse == min(cv_rmse)), aes(x = pred, y = cv_rmse), color = &quot;red&quot;) + labs(title = expression(paste(&quot;Cross-validated &quot;, RMSE))) + coord_flip() ggplot() + geom_point(data = df_metrics, aes(x = pred, y = aic)) + geom_point(data = filter(df_metrics, aic == min(aic)), aes(x = pred, y = aic), color = &quot;red&quot;) + labs(title = &quot;AIC&quot;)+ coord_flip() ggplot() + geom_point(data = df_metrics, aes(x = pred, y = bic)) + geom_point(data = filter(df_metrics, bic == min(bic)), aes(x = pred, y = bic), color = &quot;red&quot;) + labs(title = &quot;BIC&quot;)+ coord_flip() Observe which predictors are selected into the final model and which not. Why? Solution: Take the best model selected by BIC as an example, VPD_F is selected into the model and VPD_F_MDS is not. They won’t enter the model both because they are highly correlated and when one is in, the other doesn’t add information, which proves our expectation in the previous question. Whether it matters to the order of variables entering into the model which metric (i.e. AIC, BIC or \\(R^2\\)) is used? Why? Solution: No, it doesn’t matter. For each inner loop, when deciding which variable is sequentially chosen, we compare models of the same level (i.e. same number of variables). In this sense, model complexities for all the models are the same, and AIC/BIC only compares the goodness of fit part, which works the same as \\(R^2\\). Discuss your results. Which metric do you trust most? Why? Try out your algorithm without evaluating the cross-validated \\(R^2\\). Do you get a computational efficiency gain? Which metric comes closest to the cross-validated \\(R^2\\) in terms of selecting the same number predictors? Solution: Cross-validated \\(R^2\\) is the “gold-standard” for assessing generalisability. However, it is computationally costly and may not provide robust results when the data set is small. Since AIC and BIC both penalise additional predictors, they both don’t select the full model with 14 variables as the best model (However, it could happen that the best model selected by AIC/BIC is the full model). The BIC selects for the same number of predictors as the cross-validated \\(R^2\\) and RMSE, but without the disadvantage of the computational costs. It provides a conservative and apparently robust estimate for the model generalisability. Note that the order of added predictors is determined by the simple \\(R^2\\) and is therefore independent of the final metrics we use for choosing the best \\(k\\). 8.2.4 Bonus: Stepwise regression out-of-the-box In R, you can also conduct the above variable selection procedures automatically. regsubsets() from leaps package provides a convenient way to do so. It does model selection by exhaustive search, including forward or backward stepwise regression. Do a stepwise forward regression using regsubsets(). Create a data frame with the same metrics as above (except cross-validated \\(R^2\\)). Visualise metrics as above. Are your result consistent? Hints: Specify stepwise forward by setting method = \"forward\". Specify the number of predictors to examine, that is all fourteen, by setting nvmax = 14. AIC values of each step are stored in summary(regfit.fwd)$cp and BIC values of each step are in summary(regfit.fwd)$bic, etc. Get order in which predictors \\(k\\) are added (which corresponds to values returned by summary(regfit.fwd)$bic), by all_predictors[regfit.fwd$vorder[2:15]-1]. vorder is the order of variables entering into model. Note that Intercept is counted here as the first to enter into the model and should be removed. To avoid reordering of the list of variable names in plotting, change the type of variable names from “character” to “factor” by preds_enter &lt;- factor(preds_enter, levels = preds_enter) library(leaps) regfit.fwd &lt;- regsubsets(as.formula(paste(target, &quot;~&quot;, paste(preds,collapse=&quot; + &quot;))), data = df, method = &quot;forward&quot;, nvmax = 14) reg.summary &lt;- summary(regfit.fwd) # get variables in their order added to the model preds_enter &lt;- preds[regfit.fwd$vorder[2:15]-1] ## create metrics data frame df_metrics_auto &lt;- data.frame( preds = factor(preds_enter, levels = preds_enter), rsq = reg.summary$rsq, adj_rsq = reg.summary$adjr2, aic = reg.summary$cp, bic = reg.summary$bic ) ggplot() + geom_point(data = df_metrics_auto, aes(x = preds, y = rsq)) + geom_point(data = filter(df_metrics_auto, rsq == max(rsq)), aes(x = preds, y = rsq), color = &quot;red&quot;) + labs(title = expression(italic(R)^2)) + coord_flip() ggplot() + geom_point(data = df_metrics_auto, aes(x = preds, y = adj_rsq)) + geom_point(data = filter(df_metrics_auto, adj_rsq == max(adj_rsq)), aes(x = preds, y = adj_rsq), color = &quot;red&quot;) + labs(title = expression(paste(&quot;Adjusted-&quot;, italic(R)^2))) + coord_flip() ggplot() + geom_point(data = df_metrics_auto, aes(x = preds, y = aic)) + geom_point(data = filter(df_metrics_auto, aic == min(aic)), aes(x = preds, y = aic), color = &quot;red&quot;) + labs(title = &quot;AIC&quot;)+ coord_flip() ggplot() + geom_point(data = df_metrics_auto, aes(x = preds, y = bic)) + geom_point(data = filter(df_metrics_auto, bic == min(bic)), aes(x = preds, y = bic), color = &quot;red&quot;) + labs(title = &quot;BIC&quot;)+ coord_flip() 8.2.5 Bonus: Best Subset Selection - Not included in application for students (too long…) First we generate our data. set.seed(41) #data size n = 500 #predictors x1 = rnorm(n) x2 = rnorm(n) x3 = rnorm(n) #generate output y = 4*x1 + 3*x3 + rnorm(n) df_data = data.frame(y = y, x1 = x1,x2 = x2, x3 = x3) head(df_data) ## y x1 x2 x3 ## 1 3.5554726 -0.7943683 -0.08821170 2.3419290 ## 2 -7.2212212 0.1972575 -0.10062528 -2.3234496 ## 3 3.9247631 1.0017043 1.26570409 0.4460660 ## 4 0.7932959 1.2888254 -0.02288462 -0.9644862 ## 5 3.2901783 0.9057534 -0.24908328 -0.5791359 ## 6 -2.4142594 0.4936675 0.02378659 -1.4772818 The true underline model uses as predictors the variables x1 and x3. However in practise we do not know this information. What we really see is the df_data file. For this exercise we want to find those variables that are most informatives. Since we have 3 available predictors we can create 7 different models using all possible combinations of predictors. This type of variable selection is reffered as Best Subset Selection. formulas = c(&#39;y~x1&#39;,&#39;y~x2&#39;,&#39;y~x3&#39;,&#39;y~x1+x2&#39;,&#39;y~x1+x3&#39;,&#39;y~x2+x3&#39;,&#39;y~x1+x2+x3&#39;) cat(&quot;All possible models \\n&quot;,paste(formulas,collapse = &#39; , &#39;),&#39;\\n&#39;) ## All possible models ## y~x1 , y~x2 , y~x3 , y~x1+x2 , y~x1+x3 , y~x2+x3 , y~x1+x2+x3 In order to find the optimal model we will use two methods: BIC Cross Validated MSE We note here that the BIC criterion uses all the available data while the MSE uses the cross validated test data. 8.2.5.1 BIC #create vector that stores the BIC results for all the models bic_s = rep(NA,length(formulas)) #iterate over the models for (i in 1:length(formulas)){ #construct formula formula = as.formula(formulas[i]) #fit linear model lm_fit = lm(formula,data = df_data) #calculate bic bic_s[i] = BIC(lm_fit) } find the optimal model according to BIC #find the optimal model according to BIC cat(&quot;Optimal Model BIC: &quot;,formulas[which.min(bic_s)]) ## Optimal Model BIC: y~x1+x3 8.2.5.2 Cross Validated MSE #shuffle the data shuffled_id = sample(x = 1:n, size = n) df_data = df_data[shuffled_id,] #specify the number of folds n_folds = 5 #folds assignments (in which folder each point is assigned) folds = cut(1:n,breaks = n_folds,labels = FALSE) #create matrix to store the MSE from each test folder for all models : dimension 7 x n_folds mse_s = matrix(NA,nrow = length(formulas),ncol = n_folds) #iterate over the models for (i in 1:length(formulas)){ #construct formula formula = as.formula(formulas[i]) #cross validation for the i-th model for(j in 1:n_folds){ #take train indixes for all folders except j train_ind = (folds != j) #take test indexes for folder j test_ind = (folds == j) #train data that contains all the points that do not belong in folder j train_data = df_data[train_ind,] #test data that contains all the points that belong to folder j test_data = df_data[test_ind,] #fit linear model using the training data lm_fit = lm(formula, data = train_data) # calculate mse on test data ## make predictions y_pred = predict(lm_fit,test_data) #calculate mse of test folder --&gt; jth folder mse = mean((y_pred - test_data$y)^2) mse_s[i,j] = mse } } Check the MSE error for each test folder for every model. mse_s ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10.002892 11.2851440 11.046805 11.013381 10.312195 ## [2,] 25.203231 27.9475015 30.480528 26.647778 24.636545 ## [3,] 16.788547 16.7338477 19.004786 17.587077 15.700868 ## [4,] 10.140623 11.3226846 11.217637 11.081723 10.312199 ## [5,] 1.193298 0.8728441 1.054410 1.197727 1.035414 ## [6,] 17.112266 16.8149810 19.118716 17.582590 15.841135 ## [7,] 1.192072 0.8820639 1.053181 1.196406 1.035338 Calculate Cross Validated MSE for every model. cv_mse = rowMeans(mse_s) cv_mse ## [1] 10.732083 26.983117 17.163025 10.814973 1.070739 17.293938 1.071812 Find the optimal model according to Cross Validated MSE. #find the optimal model according to CV cat(&quot;Optimal Model CV_MSE: &quot;,formulas[which.min(cv_mse)]) ## Optimal Model CV_MSE: y~x1+x3 "],["ch-09.html", "Chapter 9 Supervised Neural Networks I 9.1 Introduction 9.2 Tutorial 9.3 Exercise", " Chapter 9 Supervised Neural Networks I 9.1 Introduction 9.1.1 Learning objectives After this learning unit, you will be able to … * Practice some of the fundamentals of data cleaning. * Prepare data for use in machine learning by shuffling and splitting the data into training and testing sets. * Gain hands-on experience with a Keras, an important API for machine learning * Apply gradient descent in the context of linear regression, specifically to learn w and b in the model y = wx + b. * Observe how gradient descent (nearly) converges on the exact solution for y = wx + b * Explore how a hyper-parameter (the learning rate) influences whether the model converges and if so, how quickly. * Visualize the solution space for this problem to get an intuition for why gradient descent is so effective in finding the (nearly) optimal parameter combination. * Apply logistic regression to a simple classification problem. * Recall and apply measures of model performance and loss for classification, specifically accuracy and cross-entropy. 9.1.2 Important points from the lecture It is always worthwhile to perform an initial exploratory analysis of your data, e.g. to identify outliers, missing values, etc. To train a statistical model using machine learning, we split our data into training and testing sets. Sometimes we also include a validation set. Loss is a measure of how well our trained model predicts training labels. Loss is high when predictions are poor. Loss is low when predictions are good. There are several ways to measure loss. RMSE (Root Mean Squared Error) is one such measure. It is used in regression problems. Gradient descent is a method that searches for model parameters that minimize loss. Machine learning algorithms have hyper-parameters. These are parameters that are set by the user rather than learned during training. An example is the learning rate in gradient descent. Logistic regression is a classification technique. Logistic regression is similar in model structure to linear regression. It is different in that it can capture non-linearities in data. As compared to regression problems, classification problems require different loss functions and measures of model performance. For example, loss can be measured using cross-entropy and model performance can be measured using accuracy. 9.2 Tutorial 9.2.1 Set-up 9.2.1.1 Import libraries We first need to import some libraries. Most of these will be familiar from previous tutorials, with the exception of keras, which uses TensorFlow as a backend. For those who may be unfamiliar with the term backend this simply refers to a program’s code or parts of a computer application that enable it to run but isn’t accessed by the user and works in the background. These packages will be explained in detail later. library(rjson) library(tidyverse) library(patchwork) library(reticulate) # use_condaenv() library(keras) library(tensorflow) library(IRdisplay) # plot size options(repr.plot.width = 10, repr.plot.height = 7) 9.2.1.2 Load your data Load the hourly data from the eddy covariance flux towers. You’ll be quite familiar with this by now. For this first example, we’ll only load one feature (PPFD_IN - incoming photosynthetic photon flux density) and our dependent variable (GPP_NT_VUT_REF - gross primary production). Use the head() command to get an initial glimpse of the first few rows of data. Notice anything strange? #path to the file file_path = &quot;./data/SLNN_I/flx_ch-lae/data.rds&quot; #read data df_data = readRDS(file_path) head(df_data) ## PPFD_IN GPP_NT_VUT_REF ## 1 -9999 -1.33329 ## 2 -9999 -2.77788 ## 3 -9999 -3.22186 ## 4 -9999 -3.24148 ## 5 -9999 -3.99006 ## 6 -9999 -4.29533 9.2.1.3 Clean your data Looks like the value of PPFD_IN has been set to -9999 in some examples. As you’ll recall from previous tutorials, this is a flag to indicate missing data. Let’s remove these examples and create a scatterplot of the remaining data. #Remove examples with PPFD_IN = -9999. They are missing values. df_data_cleaned = df_data %&gt;% na_if(-9999) %&gt;% drop_na() #Visualize the remaining data using a scatterplot df_data_cleaned %&gt;% ggplot(aes( x = PPFD_IN, y = GPP_NT_VUT_REF))+ geom_point() 9.2.1.4 Prepare your data for learning In previous lectures and tutorials, you’ve been taught the importance of splitting data into training and testing sets. Sometimes we produce a further split of the training data to include a validation set, although that won’t be necessary for the purposes of this tutorial. It is also important to shuffle your data. Why? Imagine you have temporal data that is laid out in chronological order. If you do not shuffle your data, your training set may contain examples from Spring, Summer, and Fall, whereas your testing data will contain examples from Winter. If the phenomena of interest depends on the season, then the patterns you learn in the training data may not be applicable to the patterns inherent in the test data. Let’s first shuffle the data, seeding the random number generator for reproducible results: #We can set a seed for reproducible results. set.seed(0) #Shuffle the data shuffled_id = sample(x=1:nrow(df_data_cleaned),size = nrow(df_data_cleaned),replace = FALSE) df_data_cleaned = df_data_cleaned[shuffled_id,] Next let’s split the data into training (80%) and test (20%) sets: #Make a breakpoint at 80% breakpoint =as.integer(0.8 * nrow(df_data_cleaned)) #Use this breakpoint to delineate the training data from the test data df_train = df_data_cleaned %&gt;% slice(1:breakpoint) #Past the breakpoint is the test data df_test = df_data_cleaned %&gt;% slice((breakpoint+1):nrow(df_data_cleaned)) 9.2.2 Keras for linear models Keras is a powerful API for machine learning. It provides a rich library of easy-to-use functions for building, training, and evaluating artificial neural networks. It interfaces with several backend machine learning libraries. Here we’ll use it with the TensorFlow backend, which was developed by Google Brain for deep learning applications. A fundamental object in Keras is the “layer”, which contains a number of computational units called neurons. The neurons of each layer are connected to one another in sequence by directed edges, starting from the input layer, progressing through (possibly many) hidden layers, and ending in the output layer (see image below). Each layer except has its own bias term that is connected to all neurons in the successive layer. All edges have associated weights, which we learn during training. Figure 9.1: Visualization of a neural network with four Layers and a multi-classification output. Figure from Deep Learning with Python. Below we define a function for building and training the simple model y = wx + b using gradient descent. While this task does not require a sophisticated artificial neural network, recall from lecture video 9B (nonlinear problems) that this task can be formulated using a network schematic, so the tools for training sophisticated artificial neural networks are also applicable in this simpler context. Thus, for this example, we only need a single layer of a single unit. Figure 9.2: Visualization of a neural network with one layer and one neuron. Figure from Deep Learning with Python. Notice the inputs to build_and_train_model function: The first two are for the training data. The variable x_train contains our independent variable PPFD_IN and y_train contains our dependent variable GPP_NT_VUT_REF. What about the other inputs? * learning_rate: recall from lecture 6B (loss and its minimization) that the learning rate is a hyper-parameter for gradient descent. It controls the size of the steps we take in parameter space in the opposite direction of the gradient of the loss function. Remember, we are trying to minimize loss. * num_epochs: specifies the number of passes we’ll make through the data during model training. * batch_size: specifies the number of examples used to compute the gradient. The gradient will therefore be computed len(x_train) / batch_size times per epoch. * save_path: is simply a file path for saving our results. It is worthwhile to study the innerworkings of this function to get a feel for some of the basic functionality of Keras. This will become particularly valuable in later tutorials, which use Keras to train more sophisticated models. #Let&#39;s save our output in the current working directory save_path_logs = &#39;./&#39; build_and_train_model = function(x_train,y_train,learning_rate,num_epochs,batch_size,save_path){ #Most Keras models are so-called &#39;sequential&#39; models model=keras_model_sequential() #Name the model model.name = &#39;linear_regression_&#39; #Name the log file path=file.path(save_path,&#39;logs&#39;) dir.create(path) fname=file.path(path,paste(model.name,&#39;lr_&#39;,toString(learning_rate),&#39;.json&#39;,sep=&quot;&quot;)) #The simple model y = wx + b requires only one layer of one unit. #The input shape is a scalar because we only have one independent variable x. #The linear activation function is the default setting for the function layer_dense. #We will initialize the weights with a constant value in this tutorial, to facilitate #a fair comparison across different learning rates. However, in practice, you would #randomly initialize the weights. model %&gt;% layer_dense(units = 1, input_shape = 1,activation = &#39;linear&#39;, kernel_initializer = initializer_constant(1.5), bias_initializer = initializer_constant(1)) #Print the model description summary(model) #Specify the learning rate and learning algorithm for stochastic gradient descent opt=optimizer_adam(lr = learning_rate) #Compile the model, using mean squared error to define loss model %&gt;% compile(loss = &#39;mse&#39;,optimizer = opt,metrics=list(&#39;mse&#39;)) #Create a callback function so we can monitor w and b during training, #writing these data in json format, overwriting any existing file by #the same name. This is fairly advanced Keras functionality, #so if the code is over your head, don&#39;t worry about it. if (fname %in% list.files(path,full.names = TRUE)){file.remove(fname)} json_log=file(fname) json_logging_callback = callback_lambda( on_epoch_end = function(epoch, logs) {write( toJSON(data.frame(epoch=epoch+1, loss=logs[[&#39;loss&#39;]], w=get_weights(model)[[1]], b=get_weights(model)[[2]])), file=fname,append = TRUE)}, on_train_end = function(logs) {close(json_log)}) #Fit the model y = Wx + b history = model %&gt;% fit(x_train,y_train, epochs=num_epochs, batch_size=batch_size, callbacks = json_logging_callback, ) #Extract W and b W = get_weights(model)[[1]] b = get_weights(model)[[2]] return (list(model=model,history=history,W,b)) } 9.2.2.1 Exact solution of y = wx + b The loss function for the linear regression is the Root Mean Squared Error, given by \\(L((w,b)) = \\frac{1}{n}\\sum_{i = 1}^{n} (wx_i + b-y_i)^2\\) where \\((x_i,y_i)\\) are points of the training data. Our aim is to retrieve those w and b, which minimize the loss function or equivalently to find the line which best fits our training data. In order to minimize this objective function with respect to w and b we need to solve the following system: \\[ \\begin{align} \\frac{\\partial L}{\\partial w} &amp; = 0 \\\\ \\frac{\\partial L}{\\partial b} &amp; = 0 \\end{align} \\] which gives \\(\\hat{w} = \\frac{n \\sum_{i = 1}^{n} x_iy_i -\\sum_{i = 1}^{n}x_i\\sum_{i = 1}^{n}y_i}{n\\sum_{i = 1}^{n}x_i^2 - (\\sum_{i = 1}^{n}x_i)^2}\\) and \\(\\hat{b} = \\bar{y}-\\hat{w} \\bar{x}\\). This is the so called exact solution for the linear regression problem where \\(\\hat{w}\\) and \\(\\hat{b}\\) denote the slope and the intercept of the line respectively. As we have already seen in the previous lectures we can also use the lm function in order to fit our linear model. The lm function uses the exact solution to find the optimal w and b. lm_fit = lm(GPP_NT_VUT_REF ~ PPFD_IN, data = df_train) summary(lm_fit) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ PPFD_IN, data = df_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -42.242 -2.429 -0.529 2.054 54.205 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.9372117 0.0181264 51.7 &lt;2e-16 *** ## PPFD_IN 0.0137657 0.0000327 421.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.894 on 143266 degrees of freedom ## Multiple R-squared: 0.553, Adjusted R-squared: 0.553 ## F-statistic: 1.773e+05 on 1 and 143266 DF, p-value: &lt; 2.2e-16 In the output above, the intercept corresponds to b and PPFD_IN corresponds to w. 9.2.2.2 Gradient descent solution Not all models have an exact solution. Consequently, we need another way to approximate a sufficient solution of the objective function for any model class. To this end, we use the so called “gradient descent”, which you’ll recall from lecture 6B (loss and its minimization) is the following algorithm: Initialize \\(w_0,b_0\\) For t=1,2,… repeat until convergence: \\[\\begin{align*} b_t &amp;= b_{t-1}-\\gamma \\frac{\\partial L}{\\partial b}(w_{t-1},b_{t-1}) \\\\ w_t &amp;= w_{t-1} -\\gamma \\frac{\\partial L}{\\partial w}(w_{t-1},b_{t-1}) \\end{align*}\\] where \\(\\gamma &gt;0\\) is the learning rate. The partial derivative \\(\\frac{\\partial L}{\\partial b}(w_{t-1},b_{t-1})\\) gives the gradient of the loss function with respect to \\(b\\) at the point in parameter space (\\(w_{t-1},b_{t-1}\\)). Similarly, the partial derivative \\(\\frac{\\partial L}{\\partial w}(w_{t-1},b_{t-1})\\) gives the gradient of the loss function with respect to \\(w\\) at the point in parameter space (\\(w_{t-1},b_{t-1}\\)). Checkpoint Can you apply the gradient descent algorithm in order to find the minimum of \\(f(x) = x^2 +1\\)? Use as starting point \\(x_0=10\\), learning rate 0.01 and make 1000 iterations. Hint: Recall from Calculus I, that the derivative of \\(x^2\\) is \\(2x\\). Solution ### Solution # derivative_f = 2*x x0 = 10 learning_rate = 0.01 num_iterations = 1000 x_new = x0 for (i in 1:num_iterations){ x_new = x_new - learning_rate*2*x_new } cat(&#39;Gradient descent output: x =&#39;, round(x_new,5), &quot;\\n&quot;) ## Gradient descent output: x = 0 # The true minimum of the function is given at x = 0. # As we can see the gradient descent algorithm returns a value very close # to zero after 1000 iterations Let’s use the function we wrote above, build_and_train_model, to apply gradient descent to the problem of solving y = wx + b, using a learning rate of \\(0.01\\). Note that we set the batch_size to 512, which means that each step of the algorithm will use only 512 training examples, rather than the entire training set. We set the maximum number of epochs to 20, which means that training will terminate after 20 full passes through the data. The algorithm will therefore make \\(\\frac{len(training \\ data)}{batch\\_size}\\times epochs\\) steps in total, where each step corresponds to a calculation of the gradient and an update of the model parameters. use_session_with_seed(0) #Set the number of epochs to 20 num_epochs = 20 # Build and train a model c(model,history,w,b) %&lt;-% build_and_train_model( x_train = df_train$PPFD_IN, y_train = df_train$GPP_NT_VUT_REF, learning_rate = 0.01, batch_size = 512, num_epochs = num_epochs, save_path = save_path_logs ) cat(&#39;\\nFinal solution: w =&#39;, w, &#39; b = &#39;, b, &quot;\\n&quot;) ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 2 ## Trainable params: 2 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## ## Final solution: w = 0.01373669 b = 0.7571406 Compare the solution found by gradient descent to the exact solution determined above. What do you observe? Let’s now save the model so we can retrieve it for a later use. # Create a file path for saving the model file_folder = &#39;./saved_models&#39; dir.create(file_folder) save_path = file.path(file_folder,&#39;model_lr_0.01.h5&#39;) # Save the model save_model_hdf5(model, save_path) Let’s build our intuition for how gradient descent works by visualizing the loss function versus the fitting line for each (w,b) pair derived from the gradient descent. This is an advantage of working with a two-parameter problem - we can actually visualize loss across the solution space. This is not easy to do with problems that have more parameters. In fact, it’s typically impossible. For example, try to imagine what the surface of a loss function might look like for a four-parameter problem. Not easy right? Now consider that may problems have thousands of parameters - sometimes billions. Good luck wrapping your head around that! Have a look at Figure 9.3 - what we actually observe is the fitted line to the data (right plot) for each step of gradient descent (red point of left plot). As the gradient descent update step approaches the minimum of the loss function, the line fits the data better, i.e it minimizes \\(\\sum_{i = 1}^{n} (\\hat{y}_i-y_i)^2\\). Figure 9.3: Animated gradient descent: Left plot: The gradient descent algorithm (red trace). Right plot: The fitting line for the respective step of gradient descent. Animation taken from Miro. We can also observe the convergence (gradient descent update causes tiny changes) of the algorithm by looking at either plot. Next let’s visualize the best fit inferred by gradient descent, as compared to the exact solution, in the context of the data we’re trying to fit. df_train %&gt;% ggplot(aes(x = PPFD_IN, y = GPP_NT_VUT_REF))+ #plot points geom_point()+ # plot lines ##plot the best fit for learning rate = 0.01 geom_abline(aes(intercept = b_est, slope = w_est ,color=&#39;GD Solution&#39;,linetype=&#39;GD Solution&#39;),lwd =2) + ##plot exact line geom_abline(aes(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2],color=&#39;Exact Solution&#39;,linetype=&#39;Exact Solution&#39;),lwd =2) + scale_colour_manual(values=c(&#39;GD Solution&#39; = &quot;red&quot;,&#39;Exact Solution&#39; = &#39;green&#39;))+ scale_linetype_manual(values=c(&#39;GD Solution&#39; = 1,&#39;Exact Solution&#39; = 2))+ labs(color = &quot;&quot;, linetype = &quot;&quot;) As we can see by the overlapping of the red and green lines, the gradient descent algorithm sufficiently approximates the exact solution. Although especially at the edges of the plot we can see that it is not a perfect match to the exact solution. 9.2.3 Tuning learning rate So far, we’ve only considered a single learning rate. Recall from lecture 6B (loss and its minimization) that this hyper-parameter has an important influence on the behavior of gradient descent, influencing its rate of convergence and whether it converges at all. Let’s tune the learning rate from 0.001 to 1 to see how this hyper-parameter influences model convergence. The code below calls the build_and_train_model function for four different learning rates. #Set the learning rates learning_rates=c(0.001,0.01,0.1,1.0) #Initialize some arrays to store our output loss_all = c() W_all = c() b_all = c() #Loop over the four learning rates for (i in 1:length(learning_rates)){ #Build and train a model using learning rate i c(model,history,w,b) %&lt;-% build_and_train_model( x_train = df_train$PPFD_IN, y_train = df_train$GPP_NT_VUT_REF, learning_rate = learning_rates[i] , batch_size = 512, num_epochs = num_epochs, save_path = save_path_logs ) #Store the loss and final solution for each learning rate loss_all=c(loss_all,list(history$metrics$loss)) W_all=c(W_all,w) b_all=c(W_all,b) } ## Model: &quot;sequential_1&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_1 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 2 ## Trainable params: 2 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Model: &quot;sequential_2&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_2 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 2 ## Trainable params: 2 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Model: &quot;sequential_3&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_3 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 2 ## Trainable params: 2 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Model: &quot;sequential_4&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_4 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 2 ## Trainable params: 2 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Let’s now plot the dynamics of learning, specifically how loss and the two model parameters change throughout training. Let’s start with loss. #Set the symbol labels and colors for the four different learning rates labels = c(&#39;0.001&#39;, &#39;0.01&#39;, &#39;0.1&#39;, &#39;1&#39;) colors = c(&quot;purple&quot;, &quot;red&quot;, &quot;blue&quot;,&quot;green&quot;) #Create a data frame df_loss = data.frame(epoch = rep(1:num_epochs,times=4), loss =c(loss_all[[1]],loss_all[[2]],loss_all[[3]],loss_all[[4]]), group = as.factor(rep(1:4,each=num_epochs))) #Plot loss for the four different learning rates df_loss %&gt;% ggplot(aes(x=epoch,y=loss,color = group))+ geom_line()+ geom_point(color = &#39;black&#39;,shape = 4)+ scale_color_manual(labels=labels,values = colors)+ labs(color= &#39;Learning Rate&#39;,y=&#39;Loss&#39;,title=&#39;Loss with different learning rates&#39;)+ #log scale scale_y_continuous(trans=&#39;log&#39;)+ theme_gray(base_size = 15) What do you observe? Using the learning rate 0.001, the algorithm slowly converges on a solution. Using the learning rates 0.01 and 0.1, we observe more rapid convergence. In stark contrast, using a learning rate of 1 causes divergence. Why is that? To find out, let’s have a look at the dynamics of our model parameters w and b during model training. #Get the list of file names fnames = list.files(file.path(save_path_logs,&#39;logs&#39;),full.names = T) #Allocate space for saving the model parameters w = matrix(0,nrow = num_epochs, ncol=length(fnames)) b = matrix(0,nrow = num_epochs, ncol=length(fnames)) #Loop over the four learning rates for (file_index in 1:length(fnames)){ #read the file line by line file_lines=readLines(fnames[file_index]) for (i in seq_along(file_lines)){ #convert each line to json object row_file=fromJSON(file_lines[i]) w[i,file_index]=row_file$w b[i,file_index]=row_file$b } } #plot w df_w &lt;- data.frame(loss = c(w),group = as.factor(rep(1:4,each=num_epochs)),epoch = rep(1:20,times=4)) gg1 &lt;- df_w %&gt;% ggplot(aes(x = epoch,y = loss,color = group ))+ geom_line()+ geom_point(shape=4,color = &#39;black&#39;)+ geom_hline(aes(yintercept = 0.01378, linetype=&#39;lt&#39;))+ scale_color_manual(name = &#39;Learning Rate&#39;,values = colors,labels = labels)+ scale_linetype_manual(labels =&#39;&#39;,name=&#39;exact value&#39;,values = 3)+ theme_gray(base_size = 15)+ theme(legend.position=&quot;top&quot;,legend.direction = &#39;horizontal&#39;) + ylab(&quot;Estimated w&quot;) gg1.1 &lt;- df_w %&gt;% ggplot(aes(x = epoch,y = loss,color = group ))+ geom_line()+ geom_point(shape=4,color = &#39;black&#39;)+ geom_hline(aes(yintercept = 0.01378, linetype=&#39;lt&#39;))+ scale_color_manual(name = &#39;Learning Rate&#39;,values = colors,labels = labels)+ scale_linetype_manual(labels =&#39;&#39;,name=&#39;exact value&#39;,values = 3)+ theme_gray(base_size = 15)+ theme(legend.position=&quot;top&quot;,legend.direction = &#39;horizontal&#39;) + ylab(&quot;Estimated w (zoomed)&quot;) + ylim(0, 0.04) #plot b df_b &lt;- data.frame(loss = c(b),group = as.factor(rep(1:4,each=num_epochs)), epoch = rep(1:20,times=4)) gg2 &lt;- df_b %&gt;% ggplot()+ aes(x = epoch,y = loss, color = group) + geom_line() + geom_point(shape=4,color = &#39;black&#39;) + geom_hline(aes(yintercept = 0.93987, linetype=&#39;lt&#39;)) + scale_color_manual(name = &#39;Learning Rate&#39;,values = colors,labels = labels) + scale_linetype_manual(labels =&#39;&#39;,name=&#39;exact value&#39;,values = 3) + theme_gray(base_size = 15) + theme(legend.position=&quot;top&quot;,legend.direction = &#39;horizontal&#39;) + ylab(&quot;Estimated b&quot;) gg1 + gg1.1 + gg2 + plot_spacer() + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) What do you observe? Notice when the learning rate is 1, the value of b (right figure) jumps above and below the optimal value, which we know from the exact solution to be ~0.94. Why does that happen? When using the learning rates 0.01 and 0.1, the algorithm converges (or nearly so) on the optimal solution, but when using the learning rate 0.001, the algoritm is misled from the optimal value of b, although it looks like it might recover in later epochs. What is going on here? Could it be that this solution space contains local, deceptive optima? Might that explain what’s happening with the lowest learning rate? What other explanations can you imagine? To get a better understanding of what is happening here, let’s visualize the surface of the loss function in relation to w and b. #Define a function that calculates mean squared error, our loss function MSE = function(observed,predicted) { val = mean((observed - predicted)^2) return (val) } #Get the observed PPFD_IN and GPP_NT_VUT_REF x_observed = df_train$PPFD_IN #PPFD_IN y_observed = df_train$GPP_NT_VUT_REF #GPP_NT_VUT_REF #Create 50x50 combinations of w and b numpoints = 100 w = seq(-0.2, 0.2,length.out = numpoints) b = seq(-2, 1, length.out = numpoints) #Initialize space for our measure of loss across #the 50x50 combinations of w and b loss = matrix(0,numpoints,numpoints) for (i in 1:numpoints){ for (j in 1:numpoints){ #use y = wx + b to make prediction predicted = w[i]*x_observed + b[j] #calculate loss loss[i,j] = MSE(y_observed, predicted) } } #Visualize the surface par(mfrow=c(1,1)) persp(w,b,loss,theta = 30, phi = 15, col = &quot;deepskyblue1&quot;, shade = 0.5) What features of this surface stand out to you? Notice that it does not appear to have local optima. Notice also that the value of w has a much stronger impact on loss than the value of b. To get a better sense of this, let’s look at two slices of the data. First, look at loss as a function of w for \\(b \\approx 0.76\\) . Second, look at loss as a function of b for \\(w \\approx 0.01\\) . This pair of (w,b) are the weights found when training with a learning rate of 0.01. #Create data frames for w and b df_w = data.frame(loss = loss[,92],w = w) df_b = data.frame(loss = loss[55,],b = b) # Load (w,b) found by model with lr = 0.01 model = load_model_hdf5(&quot;./data/SLNN_I/saved_models/model_lr_0.01.h5&quot;) w_est = as.numeric(get_weights(model)[1]) b_est = as.numeric(get_weights(model)[2]) #Plot loss as a function of w, when b = 0.76 gg1 = df_w %&gt;% ggplot(aes(x=w,y=loss))+ geom_point()+ geom_vline(aes(xintercept = w_est,linetype = &#39;Infered w&#39;),lwd=1,alpha = 0.6)+ scale_linetype_manual(name=&#39;&#39;,values=2)+ labs(title=paste(&quot;Loss for b = &quot;,0.76,sep=&quot; &quot; ))+ theme_gray(base_size = 20)+ theme(legend.position=&quot;top&quot;) #Plot loss as a function of b, when w = 0.01 gg2 = df_b %&gt;% ggplot(aes(x=b,y=loss))+ geom_point()+ geom_vline(aes(xintercept = b_est,linetype = &#39;Inferred b&#39;),lwd=1,alpha = 0.6)+ scale_linetype_manual(name=&#39;&#39;,values=2)+ labs(title=paste(&quot;Loss for w = &quot;,0.01,sep=&quot; &quot;))+ theme_gray(base_size = 20)+ theme(legend.position=&quot;top&quot;) gg1+gg2 Notice how little loss changes as a function of b, relative to w. Also notice that we can decrease loss further than what was found in our best model by decreasing b. Why didn’t our model find this solution? Let’s use our model to make predictions on our test data. First, let’s calculate loss on our test data for a learning rate of 0.01. # Load model with lr = 0.01 model = load_model_hdf5(&quot;./data/SLNN_I/saved_models/model_lr_0.01.h5&quot;) # Predict on test data y_predicted = model %&gt;% predict(df_test$PPFD_IN) # Compute loss on test data y_observed = df_test$GPP_NT_VUT_REF loss_test = MSE(y_observed, y_predicted) # Print loss cat(&#39;\\nLoss on test data: &#39;, loss_test, &quot;\\n&quot;) ## Loss on test data: 34.42075 How does this compare to the loss observed on the training data? Next let’s visualize the correlation between the predictions and observations for the test data and calculate its squared correlation. # Calculate squared correlation cor2=cor(df_test$GPP_NT_VUT_REF,y_predicted)^2 # Visualize observed vs. predicted GPP_NT_VUT_REF df = data.frame(Observed = df_test$GPP_NT_VUT_REF, Predicted = y_predicted) df %&gt;% ggplot(aes(x = Observed,y = Predicted))+ geom_point()+ labs(title = bquote(&#39;Observed Vs Predicted&#39; ~ cor^2 == .(round(cor2,2)))) Can you think of any problems with this approach? Notice the difference in scale of the x- and y-axis. Despite the strong(ish) positive correlation, observe that our model never predicts negative values, even though they’re present in the data. This highlights that the correlation between the observed and predicted values can be misleading, even though this correlation is often used as a measure of model performance. To hammer this point home, consider this simple example: Let the true data be 1,2,3,4 and the predicted data be 1001,1002,1003,1004. The correlation between them is 1, yet the error between the two sets of values is huge. Another common approach for measuring model performance is via comparison to a less complex model. Let’s compare our model to a null model that always predicts the mean of y_train. # Compare with a null model that always predicts the mean of y_train null_model = mean(df_train$GPP_NT_VUT_REF) # Train MSE pred_train = model %&gt;% predict(df_train$PPFD_IN) mse_lm_train = MSE(df_train$GPP_NT_VUT_REF,pred_train) mse_null_train = MSE(df_train$GPP_NT_VUT_REF,null_model) # Test MSE mse_lm_test = MSE(df_test$GPP_NT_VUT_REF,y_predicted) mse_null_test = MSE(df_test$GPP_NT_VUT_REF,null_model) # Results results = data.frame(Model = c(&#39;Null&#39;,&quot;Linear&quot;), Train_MSE = c(mse_null_train,mse_lm_train), Test_MSE = c(mse_null_test,mse_lm_test)) results What do you observe? How does the loss compare to that obtained during training? Is the linear model better than the null model? 9.2.4 Logistic Regression So far in this tutorial, we have studied gradient descent in the context of linear regression. Such a linear model is given by the following formula: \\(y = w_1x_1 +...+ w_nx_n + b\\) where y is measured on a continuous scale, \\(x_1\\) through \\(x_n\\) are the \\(n\\) features of the data and \\(w_1\\) through \\(w_n\\) are the weights you’re trying to learn. Recall from lecture 9B (nonlinear problems), that by applying a sigmoid function to the right-hand side of the above equation, you transition from a linear regression to a logistic regression. Unlike for linear regressions, the outputs or predictions from a logistic regression are not continuous. Logistic regressions can be used to model targets that lie in the range [0,1], such as probabilities. It is also commonly used to model binary targets (e.g., this image does contain a cat (presence) vs. this image does not contain a cat (absence)). Let’s consider a hypothetical binary classification example. That is, assume \\(y\\) either takes the value \\(0\\) (no cat) or \\(1\\) (cat). We are interested in modeling the relation between \\(P(Y=1|x)\\) and \\(x = (x_1,x_2,...,x_n)\\). The aforementioned relation can be defined as \\(P(Y=1|x) = \\sigma(w_1x_1 +...+ w_nx_n + b)\\), where \\(\\sigma(t) = \\frac{1}{1+e^{-t}}\\). Figure @ref(fig:log_fun) shows what such a logistic function looks like. (#fig:log_fun)Visualization of a logistic function. In a nutshell, logistic regression is just a linear model with the sigmoid activation function applied on the output, which converts the range to a probability. Any values of t below 0 will result in a probability closer to 0 (e.g no cat), while any values of t above 0 will result in a probability closer to 1 (e.g. presence of a cat). 9.2.4.1 Prediction The parameters of logistic regression can be estimated using gradient descent, just as we saw in this tutorial for linear regression, albeit with a different loss function. More on that below. Once the parameters have been estimated, the probability \\(\\hat{p} = \\sigma(w_1x_1 +...+ w_nx_n + b)\\) that an example belongs to the positive class can be predicted as follows: \\[ \\hat{y} = \\begin{cases} 1, &amp;\\quad if \\ \\hat{p} \\geq 0.5\\\\ 0, &amp;\\quad if \\ \\hat{p}&lt; 0.5 \\end{cases} \\] Here, we also have to highlight that: \\(\\hat{p}\\geq 0.5\\) when \\(w_1x_1 +...+ w_nx_n + b \\geq 0\\) \\(\\hat{p}&lt; 0.5\\) when \\(w_1x_1 +...+ w_nx_n + b &lt; 0\\) The equation \\(w_1x_1 +...+ w_nx_n + b = 0\\) is referred to as the decision boundary (the vertical black line in the figure above where t=0 on the x-axis and crossing the dotted of 0.5 on the y-axis). 9.2.4.2 Training and Loss Function The objective of training is to find a set of weights \\(w_1...w_n\\) and a bias \\(b\\), such that the model estimates high probabilities for positive instances \\((y = 1)\\) and low probabilities for negative instances \\((y=0)\\). How well the model performs this task can be quantified using the following loss function, shown here for a single training example \\(\\boldsymbol{x}=x_1...x_n\\): \\[ \\mathrm{Loss}(\\boldsymbol{(w,b)}) = \\begin{cases} -\\log{\\hat{p}}, &amp;\\quad \\mathrm{if} \\ y \\ = \\ 1 \\\\ -\\log({1-\\hat{p}}), &amp;\\quad \\mathrm{if} \\ y \\ = \\ 0 \\end{cases} \\] This function makes sense because \\(-\\log(\\hat{p})\\) grows large when \\(\\hat{p}\\) approaches 0, so the loss will be large if the model estimates a probability close to 0 for a positive instance. Similarly, \\(-\\log(1-\\hat{p})\\) grows large when \\(\\hat{p}\\) approaches 1, so the loss will be large if the model estimates a probability close to 1 for a negative instance. The loss function can be applied to the entire training set by taking the average over all training examples. It can be written in a single expression: \\[ \\mathrm{Loss}(\\boldsymbol{(w,b)}) = -\\frac{1}{n}\\sum_{i=1}^{n} \\left[ y_i\\log(\\hat{p}_i)+(1-y_i)\\log(1-\\hat{p}_i)\\right] \\] This loss function is called binary cross-entropy, as you’ll recall from lectures 6B (and will see in 10B). Unfortunately, there is no a closed form solution for this loss function, so we have to use heuristics such as the gradient descent algorithm to minimize it. 9.2.4.3 Measures of model performance for classification In this tutorial, we introduce a common classification metric called accuracy, which we will revist in lecture 10B (model performance). The accuracy of a model is defined as \\[ \\mathrm{Accuracy} = \\frac{TP + TN}{n} \\] where TP stands for ‘True Positives’ and TN stands for ‘True Negatives’, respectively, and n is the number of examples in the test set. Put simply: accuracy is the number of correct predictions divided by the total number of predictions. This measure ranges from 0 and 1, where 0 means our model never makes a correct prediction and 1 means it always makes correct predictions. This measure comes with caveats, especially for so-called class-imbalanced datasets, where the vast majority of examples have one label, whereas a small minority have another label. We’ll revisit this limitation in lecture 10B and tutorial 10. For now, let’s just ignore these caveats. 9.2.4.4 Example For this example we use the R function glm in order to fit a logistic regression model. Let’s first generate some synthetic data. For this example, our data will have only a single feature. The instances with label 0 will have their features drawn from a normal distribution with mean 2 and standard deviation 0.5. The instances with label 1 will have their features drawn from a normal distribution with mean 6 and standard deviation 1 (we also draw some instances with label 0 from this distribution so that our distributions are not totally separated). These distributions were chosen because they are clearly separable, yet have some overlap in their tails. Also the problem is not totally separable because of the noisy labels. #Seed the random number generator to ensure reproducibility set.seed(101) #We will have 150 examples with label 0 and 200 examples with label 1 n1 = 150 n2 = 200 #For examples with label 0, randomly generate features using a normal #distribution with mean 2 and standard deviation 0.5 x1 = rnorm(n1,2,0.5) #For examples with label 1, randomly generate features using a normal #distribution with mean 6 and standard deviation 1 x2 = rnorm(n2,6,1) #Concatenate the features for all examples x = c(x1,x2) #Associate labels with examples y1 = rep(0,n1) y2 = sample(x = c(0,1),size = n2 , prob = c(0.1,0.9),replace = T) # we include some noisy data here #Concatenate the labels for all examples y = c(y1,y2) #Print statistics describing the number of examples with each label cat(&quot;Proportion of examples with label 0: &quot;,100 * mean(y==0),&quot;%\\n&quot;) ## Proportion of examples with label 0: 48.57143 % cat(&quot;Proportion of examples with label 1: &quot;,100 * mean(y==1),&quot;%\\n&quot;) ## Proportion of examples with label 1: 51.42857 % Now let ’s plot the data. #create dataframe df_data = data.frame(x = x , y = y) #plot df_data %&gt;% ggplot(aes(x = x , y = y , color = as.factor(y)))+ geom_point()+ scale_color_manual(name = &#39;Label&#39;,values = c(&#39;0&#39; = &#39;red&#39;,&#39;1&#39; = &#39;blue&#39;))+ ylab(&quot;P(positive)&quot;)+ theme_grey(base_size = 15) Prepare the data for training and testing, by shuffling and using an 80/20 split, as before. #We can set a seed for reproducible results. set.seed(101) #Shuffle the data shuffled_id = sample(x=1:nrow(df_data),size = nrow(df_data),replace = FALSE) df_data = df_data[shuffled_id,] #Make a breakpoint breakpoint = as.integer(0.8 * nrow(df_data)) #Training data df_train = df_data %&gt;% slice(1:breakpoint) #Testing data df_test = df_data %&gt;% slice((breakpoint+1):nrow(df_data)) A more appropriate way to split the data would be to use the r-sample package and select stratified samples according to the labels. The reason is that we want each label to be represented in a proprotion of 80% and 20% to each of training and testing set respectively. Especially for imbalanced data we want to ensure that both labels are represented to the specified proportion on training and testing sets. In essence, what we would like to avoid is to have an over represantation of one label into the training set and an under represantaion on testing data. More on this in tutorial 10. Use the glm function to fit a logistic regression model to the training data. For a logisitic regression model the appropriate family = binomial lg_fit = glm(y ~ x , family = &#39;binomial&#39;,data = df_train) summary(lg_fit) ## ## Call: ## glm(formula = y ~ x, family = &quot;binomial&quot;, data = df_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8559 -0.2164 0.0881 0.3190 1.9610 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.1477 0.8874 -8.055 7.96e-16 *** ## x 1.6944 0.1883 8.998 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 387.00 on 279 degrees of freedom ## Residual deviance: 108.83 on 278 degrees of freedom ## AIC: 112.83 ## ## Number of Fisher Scoring iterations: 6 Note that the decision boundary is \\(-7.1477 + 1.6944 x = 0\\). This means examples with \\(x \\geq 4.22\\) are predicted to have a label of 1 (i.e., they belong to the ‘positive class’), whereas examples with \\(x &lt; 4.22\\) are predicted to have a label of 0 (i.e., they belong to the ‘negative class’). Let’s plot the learned sigmoid function as well as the decision boundary. #create a grid of values ranging from 0 to 10 grid = seq(from = 0 ,to = 10,length.out = 1000) df = data.frame(x = grid) #take predictions --&gt; it is probability of an instance to belong to the positive class pred = predict(lg_fit,newdata = df , type = &#39;response&#39;) df_pred = data.frame(grid = grid,pred = pred) df_train %&gt;% ggplot(aes(x = x,y = y , color = as.factor(y)))+ geom_point()+ geom_line(data = df_pred, aes(x = grid,y = pred,linetype = &#39;Sigmoid&#39;),color = &#39;black&#39;)+ geom_vline(xintercept = 4.22, lty = 2)+ annotate(&#39;text&#39;,x = 3.3 ,y = 0.5,label = &#39;Decision Boundary \\n x &gt;= 4.22 \\n predict 1&#39;)+ scale_color_manual(name = &#39;Class&#39;,values = c(&#39;0&#39; = &#39;red&#39;,&#39;1&#39; = &#39;blue&#39;))+ scale_linetype_manual(name = &#39;Sigmoid Activation&#39;,values = 1,labels = &#39;P(positve) &gt;= 0.5 \\n predict 1 &#39;)+ ylab(&quot;P(positive)&quot;)+ theme_grey(base_size = 15) It seems the trained model does a good job at predicting which examples should be associated with which labels, although some examples are clearly misclassified. How else might we quantify model performance here? Remember the accuracy metric we introduced? Let’s apply it to our model. #take probabilites pred_test = predict(lg_fit,newdata = df_test , type = &#39;response&#39;) #make decisions pred_y = (pred_test &gt;= 0.5)*1 #calculate accuracy acc = mean(pred_y == df_test$y) cat(&quot;Test Accuracy: &quot;,acc,&#39;\\n&#39;) ## Test Accuracy: 0.9 Wow, an accuracy of 90%. Sounds good right!? In this case it actually is, but we’ll see in the next tutorial where this metric breaks down. 9.3 Exercise Read in the data, tokenize the iris type (so that the output is a number) into a new column with name “y”. Namely, “virginica” –&gt; 1, “not_virginica” –&gt; 0 Shuffle your data and create a train and test set with proportions 80% and 20% of the given data respectively. Plot the training data and give a different color for each type. Create a Logistic Regression Model Plot the training data as in question 3 and also include the derived decision boundary (and Sigmoid output for configuration a) from the fitted model. Evaluate the accuracy of the model in the test set. Plot the testing data (use different colors for each Type), include the decision boundary from the fitted model and also use different point type for the misclassified predictions (if any). You have to solve tasks 3-7 with 2 different configurations A and B. Using only the Petal Width as predictor Using both Petal Length and Petal Width as predictors A skeleton code is provided below to help you out with the coding. IMPORTANT NOTE: READ CAREFULLY! Do not skip this part or you’ll run into issues later on! In a moment, after you’ve read the following instructions carefully, you should: run the code chunk immediately below this text (use_session_with_seed(0)). look down in the Console it asks if you want to install some packages: (“Would you like to install Miniconda? [Y/n]:”). write n and press enter. You should see the following code in the console: Would you like to install Miniconda? [Y/n]: n. if you were too eager and already pressed Y (yes) and enter, don’t panic! Just close your environment, re-open it and make sure that next time you go with n (no). ## Run this to promt miniconda installation request! use_session_with_seed(0) ## 1. Read Data data &lt;- read.csv(&#39;./data/SLNN_I/exercise/data_iris.csv&#39;) head(data) ## 2. Tokenize type # Create a new column with name y where y=1 defines virginica while y=0 defines not_virginica # &lt;ADD CODE HERE&gt; # HINT: Use the ifelse function data$y &lt;- head(data) # &lt;ADD CODE HERE: shuffle the data and make the train,test split&gt; # Shuffle data set.seed(42) shuffle &lt;- data &lt;- head(data) # Split data breakpoint &lt;- # Create train and test set df_train &lt;- df_test &lt;- # Create input and output x_train &lt;- df_train[,c(&#39;Petal_Length&#39;, &#39;Petal_Width&#39;)] y_train &lt;- df_train$y x_test &lt;- df_test[,c(&#39;Petal_Length&#39;, &#39;Petal_Width&#39;)] y_test &lt;- df_test$y Configuration A for tasks 3-5 ## A3. Plot training data # &lt;ADD CODE HERE: use only the Petal Width as input&gt; ## A4. Create model # &lt;FILL IN MISSING PARTS BELOW&gt; # HINT: Use right activation and loss functions for logistic regression use_session_with_seed(42) model_a &lt;- keras_model_sequential() model_a %&gt;% layer_dense(units = 1, activation = ) opt = optimizer_adam(lr=0.1) model_a %&gt;% compile(loss = ,optimizer = opt, metrics = &#39;accuracy&#39;) model_a %&gt;% fit(x= x_train$Petal_Width,y=y_train,epochs=20,batch_size=32) w_b &lt;- unlist(get_weights(model_a)) ## A5. Plot decision boundary # &lt;ADD CODE HERE&gt; # HINT: Decision boundary is now a vertical line ## A6. Evaluate the accuracy on test set # &lt;FILL IN MISSING PARTS BELOW&gt; # Take probabilities p_nn &lt;- predict(model_a, x_test$Petal_Width) # Make decisions pred &lt;- # Accuracy accuracy &lt;- cat(&quot;Test Accuracy: &quot;, round(accuracy, 3), &#39;/n&#39;) ## A7. Plot misclassified predictions in the test data # &lt;FILL IN MISSING PARTS BELOW&gt; # HINT: Create a new column in the df_test which indicates those points that are misclassified Configuration B for tasks 3-5 ## B3. Plot training data # &lt;ADD CODE HERE: For configuration B use both, Petal Length and Petal Width, as predictors ## B4. Create model # &lt;FILL IN MISSING PARTS BELOW&gt; # HINT: Use right activation and loss functions for logistic regression use_session_with_seed(42) model_b &lt;- keras_model_sequential() model_b %&gt;% layer_dense(units = 1, activation = ) opt = optimizer_adam(lr=0.1) model_b %&gt;% compile(loss = ,optimizer = opt, metrics = &#39;accuracy&#39;) model_b %&gt;% fit(x=as.matrix(x_train),y=y_train,epochs=30,batch_size=32) w_b &lt;- unlist(get_weights(model_b)) ## B5. Plot decision boundary # &lt;ADD CODE HERE&gt; # HINT: Decision boundary is now a line with slope and intercept ## B6. Evaluate the accuracy on test set # &lt;FILL IN MISSING PARTS BELOW&gt; # Take probabilities p_nn &lt;- predict(model_b,as.matrix(x_test)) # Make decisions pred &lt;- # Accuracy accuracy &lt;- cat(&quot;Test Accuracy: &quot;, round(accuracy, 3), &#39;/n&#39;) ## B7. Plot misclassified predictions in the test data # &lt;FILL IN MISSING PARTS BELOW&gt; # HINT: Create a new column in the df_test which indicates those points that are misclassified "],["ch-10.html", "Chapter 10 Supervised Neural Networks II 10.1 Introduction 10.2 Tutorial 10.3 Exercise", " Chapter 10 Supervised Neural Networks II 10.1 Introduction 10.1.1 Learning objectives Build and train neural networks for binary classification Observe the influence of network architecture on model performance Assess model performance using the following metrics Accuracy Precision Recall F-Score Receiver operating characteristic (ROC) curves Area under the ROC curve Account for the stochasticity of model training when assessing model performance 10.1.2 Important points from the lecture For binary classification problems, the output node often uses a sigmoid activation function. A classification threshold is applied to the output of the sigmoid activation function to determine which label is predicted. Accuracy is a popular measure of model performance, but it is not useful for class-imbalanced data. The area under the ROC curve is a measure of model performance that considers the full range of possible classification thresholds. Like accuracy, the area under the ROC curve can also be misleading in datasets with strong class imbalance Regression and classification problems require different measures of model performance. 10.2 Tutorial 10.2.1 Import libraries We first need to import some libraries. You have seen all of these in previous tutorials. Keras will play an especially prominent role in this tutorial. library(tidyverse) library(rsample) library(reticulate) # use_condaenv() library(keras) library(pROC) # plot size options(repr.plot.width = 10, repr.plot.height = 7) 10.2.2 Construct a toy dataset For this exercise we will create a toy dataset that illusrates some of the vagaries (unexpected changes that cannot be controlled but can influence a situation) of classification, as well as some of the pros and cons of model performance measures. In this toy dataset, each example has two real-valued features on the unit interval (i.e., these values are between 0 and 1). Each example has one of two labels, \\(\\color{green}{\\text{green}}\\) or \\(\\color{blue}{\\text{blue}}\\). The dataset is constructed such that those examples within a specified radius of the point (0.5, 0.5) are assigned a \\(\\color{green}{\\text{green}}\\) label and those outside this radius are assigned a \\(\\color{blue}{\\text{blue}}\\) label. # Seed the random number generator for reproducible results set.seed(41) # Number of examples N &lt;- 10000 # Square radius of inner circle radius_2 &lt;- 0.125 # Noise applied to the boundary separating green and blue examples noise &lt;- 0.025 # Place N points on the unit square. # Label those within some radius 0, and those outside 1. x1 &lt;- runif(n = N, min = 0, max = 1) x2 &lt;- runif(n = N, min = 0, max = 1) y &lt;- ((x1-0.5)**2 + (x2-0.5)**2) + rnorm(n = N,mean = 0, sd = noise) &gt; radius_2 # Create a data frame df_data &lt;- data.frame(x1 = x1, x2 = x2, y = y*1) # Split the data into training (80%) and test (20%) sets # Make a breakpoint at 80% breakpoint &lt;- as.integer(0.8 * nrow(df_data)) # training data df_train &lt;- df_data[1:breakpoint,] # testing data df_test &lt;- df_data[(breakpoint + 1):nrow(df_data),] Let’s visualize our data to get a feel for the classification task at hand. Let’s also visualize the true decision boundary used to delineate the \\(\\color{green}{\\text{green}}\\) from the \\(\\color{blue}{\\text{blue}}\\) labels, and print the number of examples per label. Notice that there are more examples labeled \\(\\color{blue}{\\text{blue}}\\) than there are examples labeled \\(\\color{green}{\\text{green}}\\). This means the data are class imbalanced (but not heavily). # visualize our data df_data %&gt;% ggplot(aes(x = x1 , y = x2, color = as.factor(y))) + geom_point() + geom_path(aes(x = 0.5 + sqrt(radius_2) * cos(seq(0, 2 * pi, length.out = N)), y = 0.5 + sqrt(radius_2) * sin(seq(0, 2 * pi, length.out = N)), linetype = &#39; True \\n Decision \\n Boundary&#39;), color = &#39;black&#39;, size = 3) + scale_color_manual(name = &#39;Class&#39;, values = c(&#39;0&#39; = &quot;green&quot;, &#39;1&#39; = &#39;blue&#39;)) + labs(linetype = &#39;&#39;) + theme_grey(base_size = 15) # print some summary statistics about these data cat(&#39;Proportion of data in class blue: &#39; , sum(df_data$y) / length(df_data$y), &#39;\\n&#39;, &#39;Proportion of data in class green: &#39; , 1 - sum(df_data$y) / length(df_data$y), &#39;\\n&#39;) ## Proportion of data in class blue: 0.6018 ## Proportion of data in class green: 0.3982 A more appropriate way to split the data into training and test sets is to take care such that each label is represented to its proportion in both sets (stratified splitting). To this end, we use the rsample package and we choose strata = y. That is crucial especially for heavily class imbalanced data sets. In essence, what we want to avoid is to have an over representation of one class in the training data set and an under representation of this class in the the test data set. # split with r sample and strata = y split &lt;- initial_split(df_data, prop = 0.8, strata = &#39;y&#39;) #train set df_train &lt;- training(split) #test set df_test &lt;- testing(split) #print statistics for each set ## training set cat(&quot;Training set: \\n&quot;, &#39;Proportion of data in class blue: &#39; , sum(df_train$y) / length(df_train$y), &#39;\\n&#39;, &#39;Proportion of data in class green: &#39; , 1 - sum(df_train$y) / length(df_train$y), &#39;\\n&#39;) ## Training set: ## Proportion of data in class blue: 0.6018252 ## Proportion of data in class green: 0.3981748 ##test set cat(&quot;\\nTest set: \\n&quot;, &#39;Proportion of data in class blue: &#39; , sum(df_test$y) / length(df_test$y), &#39;\\n&#39;, &#39;Proportion of data in class green: &#39; , 1 - sum(df_test$y) / length(df_test$y), &#39;\\n&#39;) ## ## Test set: ## Proportion of data in class blue: 0.6016992 ## Proportion of data in class green: 0.3983008 As it is obvious now, each label is represented to its initial proportion in training and test set. 10.2.3 Build and train NN As you saw in your previous tutorial, Keras is a powerful API for building, training, and evaluating artificial neural networks. In that lecture, you used Keras to construct, train, and analyze extremely simple neural networks, comprising just a single layer of a single node. Here we’ll build and consider more complex network architectures. To do so, we’ll generalize the build_and_train_model function from our previous tutorial. Specifically, we’ll change the function such that it builds and trains a network with an arbitrary number of layers and nodes per layer, such that the number of nodes per layer is the same for all layers, except the output layer, which will contain just a single node with a sigmoid activation function. Neural Networks: A closer look Before we proceed further let’s have a closer look on neural networks and what keras hides from the user. To this end, we will manually create a neural network with 1 hidden layer with 5 units and an output layer (with 1 unit). Let ’s assume that we have d features (x) and therefore \\(W_1\\)(weights) is a matrix with dimension \\(d\\times 5\\) and \\(b_1\\) has a dimension \\(5\\times 1\\)(biases). Then the output of the 1st hidden layer is \\[1st\\_hidden = xW_1 + b_1\\] The dimension of the \\(1st\\_hidden\\) is 5 (equals the number of units or more intuitively 5 different linear models output). On top of this we apply an activation function such as relu,sigmoid etc. So, \\[1st\\_hidden\\_out = activation(1st\\_hidden)\\] For the output layer we just take the values of 1st_hidden_out (now those are our new ‘features’) and we apply a matrix multiplication as we did with our features. \\[ output\\_layer = 1st\\_hidden\\_out W_2 + b_2\\] where \\(W_2\\) has dimension \\(5\\times 1\\)(equals the dimension of the previous layer output) and b_2 a dimension of 1 . Therefore \\[ y = activation(output\\_layer)\\] where the output activation for a binary classification problem is the sigmoid. In total, \\[y = activation(activation(xW_1 + b_1)*W_2 +b_2)\\] As it is obvious a neural network is a collection of connected linear models with an activation function on top of each. We can stack several hidden layers (each of those has its own number of units) following the same procedure as above. # Learning rate lr &lt;- 0.01 # Number of epochs ne &lt;- 100 # Batch size bs &lt;- 512 build_and_train_model &lt;- function (data = df_train, learning_rate = lr, num_epochs = ne, batch_size = bs, num_layers, num_units_per_layer, print_model_summary = F ){ # Most models are so-called &#39;sequential&#39; models model &lt;- keras_model_sequential() # Keras makes building neural networks as simple as adding layer upon layer with simple sequential # calls to the function &quot;layer_dense&quot;. Take a moment to appreciate how easy that makes things. # The input layer is the only layer that requires the user to specify its shape. The shape of all # subsequent layers is automatically determined based on the output of the preceding layer. Let&#39;s # use a ReLU activation function in each node in the input and hidden layers. model &lt;- model %&gt;% layer_dense(units = num_units_per_layer, input_shape = (ncol(data) - 1), activation = &quot;relu&quot;) # Add the hidden layers. Note this requires just a simple for loop that calls the function &quot;layer_dense&quot; # again and again. if (num_layers&gt;1){ for (i in 1:(num_layers-1)){ model &lt;- model %&gt;% layer_dense(units = num_units_per_layer, activation = &quot;relu&quot;) } } # Add the output layer. Note that it uses a sigmoid activation function. Make sure you know why. model &lt;- model %&gt;% layer_dense(units = 1, activation = &quot;sigmoid&quot;) # Print the model description if (print_model_summary){ summary(model) } # Specify the learning rate for stochastic gradient descent opt &lt;- optimizer_adam(lr = learning_rate) # Compile the model, using binary cross-entropy to define loss. Measure accuracy during training. # Note how easy Keras makes this. Did you have to write any functions for loss or for measuring model # performance during training? No, Keras takes care of all of this for you. model %&gt;% compile(optimizer = opt, loss =&#39;binary_crossentropy&#39;, metrics = list(&#39;accuracy&#39;)) # Fit the model history &lt;- model %&gt;% fit(x = as.matrix(data[, c(&#39;x1&#39;, &#39;x2&#39;)]), y = data$y, epochs = num_epochs, batch_size = batch_size, ) # Return the model and the training history return(list(model = model, history = history)) } Checkpoint Why does the output layer use a sigmoid activation function rather than a linear activation function? Can you think of a case where you’d want the output layer to use a linear activation function? Solution For a continuous response y then the output activation should be linear. Can we train an accurate classifier with just a single layer of a single node? Let’s try. And let’s assess model performance using a metric called Accuracy. This is simply the number of correct predictions divided by the number of predictions. In the context of this toy dataset, it’s the number of examples predicted to be blue that actually are blue plus the number of examples predicted to be green that actually are green, divided by the number of predictions. c(model, history) %&lt;-% build_and_train_model(data = df_train, learning_rate = 0.01, batch_size = 512, num_epochs = 100, num_layers = 1, num_units_per_layer = 1, print_model_summary = T) cat(&#39;Final training accuracy: &#39;, round(history$metrics$acc[ne], 3), &#39;\\n&#39;) ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense (Dense) (None, 1) 3 ## ________________________________________________________________________________ ## dense_1 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 5 ## Trainable params: 5 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Final training accuracy: 0.602 Note the final training accuracy. Let’s have a look at how accuracy and loss change during model training. plot(history) Observe how quickly accuracy converges to the final value. And note the final value - wow, we’re doing better than a coin flip (a coin flip choice would have an accuracy of 0.5) during training, using just a single layer of a single node! How is this possible? Take a moment to think about how this might be happening. Think about what you already know about these data (What is the percentage of each class? Is it possible a classifier that only predicts the largest class to seems better than a coin flip?). To see if you’re intuition is correct, let’s plot our model predictions on the test data. Note that in making these predictions, we need to choose a classification threshold. The reason is that the output layer yields a real value between 0 and 1, which needs to be discretized for the purpose of binary classification. For now, let’s just set this threshold at 0.5. We’ll see how changing the threshold impacts model performance later. # Predict on test data. Note how easy this is to do in Keras. pred &lt;- predict(model, as.matrix(df_test[, c(&#39;x1&#39;, &#39;x2&#39;)])) # Set the classification threshold to 0.5 threshold &lt;- 0.5 # Make label predictions based on the classification threshold y_pred &lt;- (pred &gt;= threshold) * 1 # Plot the predictions df_test %&gt;% ggplot(aes(x = x1, y = x2)) + geom_point(color = ifelse(y_pred == 1, &#39;blue&#39;, &#39;green&#39;)) # Evaluate and print model performance on test data c(loss, acc) %&lt;-% evaluate(model, as.matrix(df_test[,c(&#39;x1&#39;, &#39;x2&#39;)]), df_test$y) cat(&#39;Accuracy on held-out test data: &#39; , round(acc, 3), &#39;\\n&#39;) ## Accuracy on held-out test data: 0.602 Aha, the model is always predicting blue! Why is this simple trick effective? It’s because the data are class imbalanced, meaning that one class is more prevalent than the other in our dataset. Specifically, there are more blue examples than green examples. So by simply using the rule “\\(\\color{blue}{\\text{always blue}}\\)”, we can obtain an accuracy that’s better than a coin toss. This shows that accuracy has its limitations as a model performance metric and reveals the need for more nuanced metrics, such as the following: True positives: The number of positive examples predicted to be positive (+ / +) True negatives: The number of negative examples predicted to be negative (- / -) False positives: The number of negative examples predicted to be positive (+ / -) False negatives: The number of positive examples predicted to be negative (- / +) These metrics can be summarized in the so-called confusion matrix displayed in Figure 10.1. Figure 10.1: Visualization of a confusion matrix In this toy dataset, we’ll call the \\(\\color{blue}{\\text{blue}}\\) examples \\(\\color{blue}{\\text{&#39;positive&#39;}}\\) and the \\(\\color{green}{\\text{green}}\\) examples \\(\\color{green}{\\text{&#39;negative&#39;}}\\). This terminology makes more sense in more realistic classification problems, such as delineating spam email from non-spam. 10.2.4 Model performance Let’s write a function that calculates the building blocks of the confusion matrix. confusion &lt;- function(y_true, pred_out, threshold = 0.5, verbose = T){ true_positives = sum((pred_out &gt;= threshold) &amp; (y_true == 1)) true_negatives = sum((pred_out &lt; threshold) &amp; (y_true == 0)) false_positives = sum((pred_out &gt;= threshold) &amp; (y_true == 0)) false_negatives = sum((pred_out &lt; threshold) &amp; (y_true == 1)) if (verbose) { cat(&#39;true positives: &#39;, true_positives, &#39;\\n&#39;, &#39;true negatives: &#39;, true_negatives, &#39;\\n&#39;, &#39;false positives: &#39;, false_positives, &#39;\\n&#39;, &#39;false negatives: &#39;, false_negatives, &#39;\\n&#39;) } return (list(tp = true_positives, tn = true_negatives, fp = false_positives, fn = false_negatives)) } Note that we can also define accuracy in terms of true/false positives/negatives Accuracy: \\(\\frac{\\mbox{TP} + \\mbox{TN}}{\\mbox{TP + TN + FP + FN}}\\) where TP + FN is the number of positive cases (P) while TN + FP is the number of negative cases (N). Let’s apply this function to our model predictions. c(true_positives, true_negatives, false_positives, false_negatives) %&lt;-% confusion(df_test$y, pred, threshold) ## true positives: 1203 ## true negatives: 0 ## false positives: 796 ## false negatives: 0 This reveals a more nuanced picture of model performance. Sure, accuracy is better than a coin toss, but we now see that we do not correctly predict a single negative example. These four metrics are the basis for two additional, commonly used performance metrics (see Figure 10.2: Precision: \\(\\frac{\\mbox{TP}}{\\mbox{TP + FP}}\\) tells us the proportion of positive predictions that were actually correct. Recall: \\(\\frac{\\mbox{TP}}{\\mbox{TP + FN}} = \\frac{\\mbox{TP}}{\\mbox{P}}\\) tells us the proportion of positive examples that were correctly predicted to be positive. Figure 10.2: Difference of Precision and Recall in classification problems. Let’s define a function that calculates precision and recall. precision_and_recall &lt;- function(true_positives, true_negatives,false_positives, false_negatives, verbose=T){ # Protect against division by zero if ((true_positives + false_positives) &gt; 0){ precision = true_positives / (true_positives + false_positives) } else{ precision = 1 } if ((true_positives + false_negatives) &gt; 0){ recall = true_positives / (true_positives + false_negatives) } else{ recall = 0 } if (verbose){ cat(&quot;Precision: &quot;, precision, &quot;\\n&quot;, &quot;Recall: &quot;, recall, &quot;\\n&quot;) } return (list(precision = precision, recall = recall)) } Let’s apply this function to our model predictions. c(precision, recall) %&lt;-% precision_and_recall(true_positives, true_negatives, false_positives, false_negatives) ## Precision: 0.6018009 ## Recall: 1 Now we see that the model \\(\\color{blue}{\\text{always blue}}\\) does a great job at correctly predicting positive examples (recall), but many of its positive predictions are incorrect (precision). Wouldn’t it be nice if we could combine precision and recall into a single metric? Ideally, this metric would take on a high value when both precision and recall are large and a low value when either precision or recall is small. The F-score does exactly that: F-score : \\(2 \\cdot \\frac{\\mbox{precision} \\cdot \\mbox{recall}}{\\mbox{precision + recall}}\\) The F-score takes values on the interval [0,1]. An F-score of 1 indicates that both precision and recall are 1 and an F-score of 0 corresponds to a value of 0 for either precision or recall. So now let’s also define a function for the F-score and apply it to the previous example. f_score &lt;- function(precision, recall, verbose = T){ #Calculate F f = 2 * precision * recall / (precision + recall) # If the user has requested verbose output, provide it if(verbose){ cat(&quot;F-score: &quot;, f) } # Return F return(f) } f &lt;- f_score(precision, recall) ## F-score: 0.7514054 10.2.5 Influence of NN architecture Might more complex network architectures allow us to make better predictions? Let’s try a single layer of two nodes. c(model, history) %&lt;-% build_and_train_model(num_layers = 1, num_units_per_layer = 2, print_model_summary = T) cat(&#39;Final training accuracy: &#39;, round(history$metrics$acc[ne], 3), &#39;\\n&#39;) ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense (Dense) (None, 1) 3 ## ________________________________________________________________________________ ## dense_1 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 5 ## Trainable params: 5 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Final training accuracy: 0.602 Do you observe an improvement in training accuracy? If not, keep trying until you do. This may take several attempts. Note the reason you can obtain different results each time you train your model is that training is stochastic, meaning that certain steps, such as the initialization of the edge weights, depend on the generation of random numbers. Each time you construct and train a model, new random numbers are generated, and this makes each training session unique. We’ll come back to this later. Now that you’ve obtained a model with improved training accuracy, let’s see how it achieves this improvement: # Predict on test data pred &lt;- predict(model, as.matrix(df_test[, c(&#39;x1&#39;, &#39;x2&#39;)])) # Set the classification threshold threshold &lt;- 0.5 # Make label predictions based on the classification threshold y_pred &lt;- (pred &gt;= threshold) * 1 # Plot our predictions df_test %&gt;% ggplot(aes(x = x1, y = x2)) + geom_point(color = ifelse(y_pred == 1, &#39;blue&#39;, &#39;red&#39;)) # Evaluate model performance on test data c(loss,acc) %&lt;-% evaluate(model, as.matrix(df_test[, c(&#39;x1&#39;, &#39;x2&#39;)]), df_test$y) cat(&#39;Accuracy on held-out test data: &#39;, round(acc, 3), &#39;\\n&#39;) # Print precision,recall and F-score c(true_positives, true_negatives, false_positives, false_negatives) %&lt;-% confusion(df_test$y, pred, threshold, verbose = F) c(precision, recall) %&lt;-% precision_and_recall(true_positives, true_negatives, false_positives, false_negatives) f &lt;- f_score(precision, recall) ## Accuracy on held-out test data: 0.75 Precision: 0.8490566 Recall: 0.7107232 F-score: 0.7737557 What do you observe? A few points to note: * Accuracy is improved. * The improvement in accuracy comes at the cost of reduced recall, but with the benefit of increased precision. * The decision boundary learned by the model differs substantially from the true decision boundary. Now let’s consider more complex network architecture. Specifically, consider: Four layers of eight nodes To do so, simply alter the input arguments to build_and_train_model. Do we see improvements? Again, you may have to train the model several times due to the stochastic nature of the training process. c(model, history) %&lt;-% build_and_train_model(num_layers = 4, num_units_per_layer = 8, print_model_summary = T) cat(&#39;Final training accuracy: &#39;, round(history$metrics$acc[ne],3),&#39;\\n&#39;) # predict on test data pred &lt;- predict(model, as.matrix(df_test[, c(&#39;x1&#39;, &#39;x2&#39;)])) # set the classification threshold threshold &lt;- 0.5 # make predictions y_pred &lt;- (pred &gt;= threshold) * 1 # plot our predictions df_test %&gt;% ggplot(aes(x = x1, y = x2)) + geom_point(color = ifelse(y_pred == 1, &#39;blue&#39;, &#39;red&#39;)) #evaluate model performance on test data c(loss, acc) %&lt;-% evaluate(model, as.matrix(df_test[, c(&#39;x1&#39;, &#39;x2&#39;)]), df_test$y) cat(&#39;Accuracy on held-out test data: &#39;, round(acc, 3), &#39;\\n&#39;) #print precision, recall and F-score c(true_positives, true_negatives, false_positives, false_negatives) %&lt;-% confusion(df_test$y, pred, threshold, verbose = F) c(precision, recall) %&lt;-% precision_and_recall(true_positives, true_negatives, false_positives, false_negatives) f &lt;- f_score(precision, recall) ## Model: &quot;sequential_2&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_4 (Dense) (None, 8) 24 ## ________________________________________________________________________________ ## dense_5 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_6 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_7 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_8 (Dense) (None, 1) 9 ## ================================================================================ ## Total params: 249 ## Trainable params: 249 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Final training accuracy: 0.934 ## Accuracy on held-out test data: 0.936 ## Precision: 0.9453642 ## Recall: 0.9492934 ## F-score: 0.9473248 What do you observe? Does model performance improve with model complexity? If so, is this always the case? Checkpoint Compare the number of trainable parameters of this model with the previous one. Comment on this. Solution The number of trainable parameters for the 1st model is 9 while for the last is 249. This increase is huge (almost 28 times the previous one). Something we’ve glossed over so far is the dependence of model performance metrics on the classification threshold, which we’ve held fixed at 0.5. Let’s see what happens when we change this threshold. First, calculate precision and recall for thresholds between 0.05 and 0.95. # Consider 100 thresholds between 0 and 1 num_thresholds &lt;- 100 thresholds &lt;- seq(0, 1, length.out = num_thresholds) # Allocate space for the four metrics of the confusion matrix, as well as for precision and recall fp &lt;- rep(NA, num_thresholds) fn &lt;- rep(NA, num_thresholds) tp &lt;- rep(NA, num_thresholds) tn &lt;- rep(NA, num_thresholds) precision &lt;- rep(NA, num_thresholds) recall &lt;- rep(NA, num_thresholds) # Loop over the thresholds, calculate the confusion matrix as well as precision and recall for (i in 1:num_thresholds){ c(tp[i], tn[i], fp[i], fn[i]) %&lt;-% confusion(df_test$y, pred, thresholds[i], verbose = F) c(precision[i], recall[i]) %&lt;-% precision_and_recall(tp[i], tn[i], fp[i], fn[i], verbose = F) } Plot precision and recall as a function of the classification threshold. df_pre_rec &lt;- data.frame(x = thresholds, metrics = c(precision, recall), type = rep(c(&#39;precision&#39;,&#39;recall&#39;), each = num_thresholds)) df_pre_rec %&gt;% ggplot(aes(x = x, y = metrics, color = type)) + geom_line(lwd = 1.5, lty = 2) + scale_color_manual(name = &#39;&#39;, labels = c(&#39;Precision&#39;,&#39;Recall&#39;), values = c(&#39;Red&#39;,&#39;Blue&#39;)) + labs(x = &#39;Classification Threshold&#39;, y = &#39;&#39;) + theme_grey(base_size = 15) As it is obvious from the graph, precision and recall experience an opposite behaivor as the classification threshold varies. Namely, precision increases as the classification threshold gets larger while recall drops. In contrast, for a low classification threshold precision decreases and recall reaches a high value. Intuitively, this behavior was expected since if the model predict very easily the positive class (low classification threshold) then we have a lot of false positives and thus a low precision. However in this case, we identify most of the positives instances and therefore we achieve a high recall. On the other hand, if we desire our model to be very confident while predicting the positive class (high classification threshold) we expect a high precision. But in this case, we are eager to misclassify a lot of positive examples and therefore a low recall is anticipated. Checkpoint Can you think of a real world examples where it’s neccessary to have a high precision model (and potentially low recall)? Can you think of a real world example where it’s necassary to have a high recall model (and potentially low precision)? Solution Consider that you create a model for company X where the aim is to decide who is allowed to pass the company’s office entrance. The input of the model is a real time face photo and the prediction is Yes or No. You do not want to allow strangers to get inside the offices. You only want company’s related people. In this case your model should not have false positives. So you demand a high classification threshold and therefore a high precision. Let ’s say that you are working on a hospital helping doctors to predict if a patient has a specific illness. Your model takes as input a value of a test and predicts if the patient is positive or negative. In this case, you care more about to find all those patients who are indeed positives. So you want a low classification threshold and thus a high recall model. In case you have some false positives you can just inform the patient to make some other different health tests. This plot reveals a tradeoff between precision and recall, which is often the case. To understand this tradeoff, remember how precision and recall are calculated: precision: \\(\\frac{\\mbox{TP}}{\\mbox{TP + FP}}\\) recall: \\(\\frac{\\mbox{TP}}{\\mbox{TP + FN}} = \\frac{\\mbox{TP}}{\\mbox{P}}\\) All that differs between them is one term in the denominator - false positives vs. false negatives. Precision approaches 1 as the number of false positives approaches 0, and recall approaches 1 as the number of false negatives approaches 0. So why the tradeoff? Because the number of false positives and the number of false negatives depends on the classification threshold. For example, when the threshold is low, the model will often predict positive, yielding many false positives, but few false negatives. When the threshold is high, the model will often predict negative, yielding many false negatives, but few false positives. The model performance under various thresholds is often assessed by the AUC-ROC (Area Under the Receiver Operating Characteristic curve) which plots Recall as a function of False Positive Rate. False Positive Rate : \\(\\frac{\\mbox{FP}}{\\mbox{TN + FP}} = \\frac{\\mbox{FP}}{\\mbox{N}}\\) The False Positive Rate and the Precision have an opposite behavior for the various threshold. Namely, a high threshold results in low False Positive Rate, whereas a small threshold leads to large False Positive Rate Let’s write a function to calculate the ROC curve and the area under it. # Area Under Curve simple_auc &lt;- function(x, y){ # Revert order x &lt;- rev(x) y &lt;- rev(y) # Define rectangles, calculate area and add those dx &lt;- c(diff(x), 0) dy &lt;- c(diff(y), 0) sum(y * dx) + sum(dy * dx) / 2 } # ROC CURVE ROC &lt;- function(fpr, recall, viz = T){ # Calculate the area under the ROC auc = simple_auc(fpr, recall) # Visualize the ROC if (viz){ x = seq(0, 1, length.out = 100) g = ggplot() + geom_line(aes(x = fpr, y = recall, color = &#39;Model&#39;) , lty = 2, lwd = 1.2) + geom_line(aes(x = x, y = x, color = &#39;Baseline&#39;), inherit.aes = F, lty = 2, alpha = 0.8) + labs(x = &#39;False Positive Rate&#39;, y = &#39;Recall&#39;, color =&#39;&#39;, title =&#39;Area Under The curve ROC&#39;, subtitle = paste(&quot;AUC : &quot;, round(auc, 2))) + theme_grey(base_size = 20) } else{ g = NA } return (list(plot = g, value = auc)) } Now let’s apply this function to our model predictions. fpr &lt;- fp / (fp + tn) auc &lt;- ROC(fpr, recall, viz = T) cat(&quot;AUC Value: &quot;, auc$value) auc$plot ## AUC Value: 0.986246172675514 Models with ROC curves that lie on the identity line (here the ‘Baseline’ in red-ish) are no better than a random guess. As the ROC curve approaches the upper left corner of the plot, the model is improving (Total Recall!). What is crucial to understand here is that for a low classification threshold (resulting in a model that often predicts positive) the false positive rate and recall will approach 1. In contrast, for a high classification threshold (resulting in a model that rarely predicts positive), we would like to have a high value of recall, which means that our model is very confident in predicting positive cases (even with a high classification threshold). Indeed, the best model is one with perfect recall for all false positive rates. In such a case, the ROC curve would trace the left border and top border of the plot above. As we’ve seen, a simple statistic that summarizes an ROC curve is the area under the ROC curve. You can think of this as the probability that a model ranks a random positive example more highly than a random negative example. A model whose predictions are always wrong has an area under the ROC curve of 0, whereas a model whose predictions are always right has an area under the ROC curve of 1. We note here that instead of building the ROC function from scratch we can alternatively use the package pROC # ROC Curve - pROC package roc_object &lt;- roc(df_test$y ~ c(pred), plot = TRUE, print.auc = TRUE, col = &quot;blue&quot;, lwd = 4, legacy.axes = TRUE, main = &quot;ROC Curve&quot;) # AUC - pROC package auc(roc_object) ## [1] 0.9863073 Let’s put these metrics into practice by comparing different network architectures. To do so, we need to remember that model training is stochastic. Different random initializations can lead to different outcomes, and for some model architectures, some outcomes are more likely than others. For example, we’ve seen that the “\\(\\color{blue}{\\text{always blue}}\\)” outcome is particularly seducing. To account for this stochasticity, we need to train our models multiple times, and take the average performance across random initializations. Let’s compare four model architectures: * One layer with two nodes * Two layers with two nodes * Four layers with four nodes * Eight layers with eight nodes Because we’re averaging over 10 random initializations per model, this script will take a bit of time - just enough for you to go prepare a cup of tea. # Consider 100 thresholds between 0 and 1 num_thresholds &lt;- 100 thresholds &lt;- seq(0, 1, length.out = num_thresholds) # Consider 10 random initializations of the model num_initializations &lt;- 10 # Consider 4 model architectures num_layers &lt;- c(1, 2, 4, 8) num_units_per_layer &lt;- c(2, 2, 4, 8) num_models &lt;- length(num_layers) # Allocate space for model performance metrics fp &lt;- array(NA, dim = c(num_thresholds, num_initializations, num_models)) fn &lt;- array(NA, dim = c(num_thresholds, num_initializations, num_models)) tp &lt;- array(NA, dim = c(num_thresholds, num_initializations, num_models)) tn &lt;- array(NA, dim = c(num_thresholds, num_initializations, num_models)) precision &lt;- array(NA, dim = c(num_thresholds, num_initializations, num_models)) recall &lt;- array(NA, dim = c(num_thresholds, num_initializations, num_models)) # Loop over the four network architectures for (i in 1:num_models){ # Loop over the 10 random initializations for (k in 1:num_initializations){ # Print some satisfying output message(&quot;You look nice today. Model configuration &quot; , as.character(i) , &quot; initialization &quot; , as.character(k),&#39;\\n&#39;) # Build and train the model c(model, history) %&lt;-% build_and_train_model(num_layers = num_layers[i], num_units_per_layer = num_units_per_layer[i], print_model_summary = F) # Predict on test data pred = predict(model, as.matrix(df_test[,c(&#39;x1&#39;,&#39;x2&#39;)])) # Loop over thresholds to assess model performance for (j in 1:num_thresholds){ c(tp[j,k,i], tn[j,k,i],fp[j,k,i], fn[j,k,i]) %&lt;-% confusion(df_test$y, pred, thresholds[j], verbose = F) c(precision[j,k,i], recall[j,k,i]) %&lt;-% precision_and_recall(tp[j,k,i], tn[j,k,i], fp[j,k,i], fn[j,k,i], verbose = F) } } } Now let’s average our measures of model performance across the 10 random initializations. By now it should be clear why we do this, but to drive the point home we’ll say it again: Model training is stochastic, so every time you train a model with a different random initialization, you may get a different final model, even for models based with the same network architectures. # Average over the 10 random initializations average_precision &lt;- matrix(NA, nrow = num_thresholds, ncol = num_models) average_fpr &lt;- matrix(NA, nrow = num_thresholds, ncol = num_models) average_recall &lt;- matrix(NA, nrow = num_thresholds, ncol = num_models) # Allocate space for the area under the ROC curves aucs &lt;- rep(NA, num_models) # Loop over the models for (i in 1:num_models){ #Loop over the thresholds for (j in 1:num_thresholds){ average_fpr[j,i] = mean(fp[j,,i] / (fp[j,,i] + tn[j,,i])) average_recall[j,i] = mean(recall[j,,i]) } # Calculate the ROC curve and the area under it auc = ROC(average_fpr[,i], average_recall[,i]) aucs[i] = auc$value # Print the area under the ROC curve cat(&quot;Auc for model &quot;, as.character(i), &quot; is &quot;, as.character(round(auc$value, 3)), &#39;\\n&#39;) } ## Auc for model 1 is 0.677 Auc for model 2 is 0.611 Auc for model 3 is 0.929 Auc for model 4 is 0.981 What do you observe? The most complex model (8 layers, each with 8 nodes) clearly outperforms its competitors. Let’s see this in more detail by visualizing the ROC curves. # Create a vector of points between 0 and 1 x &lt;- seq(0, 1, length.out = 101) # Name the different models model_name &lt;- c(&#39;Layers: 1 , Units : 2&#39;,&#39;Layers: 2 , Units : 2&#39;,&#39;Layers: 4 , Units : 4&#39;,&#39;Layers: 8 , Units : 8&#39;) # Build a data frame df &lt;- data.frame(avg_fpr = c(average_fpr[,1], average_fpr[,2], average_fpr[,3], average_fpr[,4]), avg_recall = c(average_recall[,1], average_recall[,2], average_recall[,3], average_recall[,4]), Model = rep(model_name, each = num_thresholds)) # Plot the results df %&gt;% ggplot(aes(x = avg_fpr, y = avg_recall, color = Model)) + geom_line(lwd = 1, lty = 2) + geom_line(data = data.frame(x = x), mapping = aes(x = x, y = x), inherit.aes = F, alpha = 0.7) + labs(title=&#39;Area Under the Curve ROC&#39;, x = &#39;False Positive Rate&#39;, y = &#39;Recall&#39;) + xlim(0,1) + scale_color_discrete(labels = c(paste(model_name,&#39; AUC: &#39;, c(round(aucs[1], 3), round(aucs[2], 3), round(aucs[3], 3), round(aucs[4], 3))))) + theme_grey(base_size = 20) Again you can see the most complex model (\\(\\color{purple}{\\text{8 layers, each with 8 nodes}}\\)) clearly outperforms the less complex ones. It is important to point out that the area under the ROC curve is not always an appropriate performance metric. Specifically, this measure can be misleading when the data are heavily class-imbalanced. To see this, let’s create a toy dataset that is even more class imbalanced. # Seed the random number generator for reproducible results set.seed(41) # Number of examples N &lt;- 10000 # Square radius of inner circle radius_2 &lt;- 0.055 # Noise applied to the boundary separating green and blue examples noise &lt;- 0.045 # Place N points on the unit square. # Label those within some radius 0, and those outside 1. x1 &lt;- runif(n = N, min = 0, max = 1) x2 &lt;- runif(n = N, min = 0, max = 1) y &lt;- ((x1 - 0.5)**2 + (x2 - 0.5)**2) + rnorm(n = N, mean = 0, sd = noise) &lt;= radius_2 # Create a data frame df_new &lt;- data.frame(x1 = x1, x2 = x2, y = y*1) # split with r sample and strata = y split &lt;- initial_split(df_new, prop = 0.8, strata = &#39;y&#39;) # train set df_train_new &lt;- training(split) # test set df_test_new &lt;- testing(split) # Print some summary statistics about these new data cat(&#39;Proportion of data in class blue: &#39; , sum(df_new$y) / length(df_new$y), &#39;\\n&#39;, &#39;Proportion of data in class green: &#39; , 1 - sum(df_new$y) / length(df_new$y),&#39;\\n&#39;) ## Proportion of data in class blue: 0.1817 ## Proportion of data in class green: 0.8183 Our data now are now heavily imbalanced with 82% of examples in the negative class. Let’s build and train the most complex model considered above (8 layers, each with 8 nodes) to classify these data and calculate several performance metrics of our trained model. # Build and train the model c(model, history) %&lt;-% build_and_train_model(data = df_train_new , num_layers = 8, num_units_per_layer = 8, print_model_summary = T) # Print the final training accuracy cat(&#39;Final training accuracy: &#39;, round(history$metrics$acc[ne], 3), &#39;\\n&#39;) # Predict on test data pred = predict(model, as.matrix(df_test_new[, c(&#39;x1&#39;, &#39;x2&#39;)])) # set decisions threshold threshold = 0.5 y_pred = (pred &gt;= threshold)*1 # Print precision,recall, and F-score c(true_positives, true_negatives, false_positives, false_negatives) %&lt;-% confusion(df_test_new$y, pred, threshold, verbose = F) c(precision, recall) %&lt;-% precision_and_recall(true_positives, true_negatives, false_positives, false_negatives) f &lt;- f_score(precision, recall) ## Model: &quot;sequential_87&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_407 (Dense) (None, 8) 24 ## ________________________________________________________________________________ ## dense_408 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_409 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_410 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_411 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_412 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_413 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_414 (Dense) (None, 8) 72 ## ________________________________________________________________________________ ## dense_415 (Dense) (None, 1) 9 ## ================================================================================ ## Total params: 537 ## Trainable params: 537 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Final training accuracy: 0.887 ## Precision: 0.7365079 ## Recall: 0.6391185 ## F-score: 0.6843658 Observe that training accuracy is high, whereas precision, recall, and the F-score are all relatively low. Let’s also have a look at the confusion matrix. table(y_pred, df_test_new$y) So let’s also calculate the areas under the ROC curve. # Create 100 threhsolds between 0 and 1 num_thresholds &lt;- 100 thresholds &lt;- seq(0, 1, length.out = num_thresholds) # Allocate space for the performance metrics fp &lt;- rep(NA, num_thresholds) fn &lt;- rep(NA, num_thresholds) tp &lt;- rep(NA, num_thresholds) tn &lt;- rep(NA, num_thresholds) precision &lt;- rep(NA, num_thresholds) recall &lt;- rep(NA, num_thresholds) # Loop over the thresholds and measure model performance for (i in 1:num_thresholds){ c(tp[i], tn[i],fp[i], fn[i]) %&lt;-% confusion(df_test_new$y, pred, thresholds[i], verbose = F) c(precision[i], recall[i]) %&lt;-% precision_and_recall(tp[i], tn[i], fp[i], fn[i], verbose = F) } # Calculate and plot the ROC curve auc &lt;- ROC(fp / (fp + tn), recall, viz = T) auc$plot The area under the ROC curve indicates an adequate model, whereas the precision and recall are relatively low. This is misleading! To understand why this occurs, remember that the denominator of the false positive rate includes the number of negative examples. Because the proportion of the negative class is high (82%), the false positive rate tends to be low when recall is high, pushing the ROC curve away from the diagonal line and toward the upper left corner. As such, when the data are heavily class imbalanced, precision, recall, and the F1 score are more informative measures of model performance. 10.3 Exercise For this exercise your task is to create 3 neural network models and to compare their performance. The provided data contains 2 classes (0 and 1). Our aim is to compare those models using the majority of the metrics we have seen in tutorial 10. Tasks: Read in the data. Visualize the data and print the proportion of data belonging to each class. Split into training and test set using Stratified split. Create 3 different neural network models of your taste ranging from a simple to a very complex one. For each model calculate accuracy, precision, recall and f-score on the test data. For each model plot the decisions on the test data. For the best performing model make the ROC curve and calculate the corresponding AUC. Train the best performing model 5 times. For each time calculate the probability output on the test data. Average the probabilities over the 5 models and then make decisions. Print accuracy, precision, recall and f-score on the test data. The required libraries and some useful functions are already imported in the following chunks. For the ROC curve you can also use the pROC package. Import libraries library(tidyverse) library(reticulate) library(pROC) # use_condaenv() library(keras) library(rsample) Import functions # Area Under Curve simple_auc &lt;- function(x, y){ # Revert order x &lt;- rev(x) y &lt;- rev(y) #Define rectangles, calculate area and add those dx &lt;- c(diff(x), 0) dy &lt;- c(diff(y), 0) sum(y * dx) + sum(dy * dx) / 2 } # ROC CURVE ROC &lt;- function(fpr, recall, viz = T){ # Calculate the area under the ROC auc = simple_auc(fpr, recall) # Visualize the ROC if (viz){ x = seq(0,1,length.out = 100) g = ggplot() + geom_line(aes(x = fpr, y = recall, color = &#39;Model&#39;), lty = 2, lwd = 1.2) + geom_line(aes(x = x, y = x, color = &#39;Baseline&#39;), inherit.aes = F, lty = 2, alpha = 0.8) + labs(x = &#39;False Positive Rate&#39;, y = &#39;Recall&#39;, color = &#39;&#39;, title = &#39;Area Under The curve ROC&#39;, subtitle = paste(&quot;AUC : &quot;, round(auc, 2))) + theme_grey(base_size = 20) } else{ g = NA } return (list(plot = g, value = auc)) } confusion &lt;- function(y_true, pred_out, threshold = 0.5, verbose = T){ true_positives = sum((pred_out &gt;= threshold) &amp; (y_true == 1)) true_negatives = sum((pred_out &lt; threshold) &amp; (y_true == 0)) false_positives = sum((pred_out &gt;= threshold) &amp; (y_true == 0)) false_negatives = sum((pred_out &lt; threshold) &amp; (y_true == 1)) if (verbose) { cat(&#39;true positives: &#39;, true_positives, &#39;\\n&#39;, &#39;true negatives: &#39;, true_negatives, &#39;\\n&#39;, &#39;false positives: &#39;, false_positives, &#39;\\n&#39;, &#39;false negatives: &#39;, false_negatives, &#39;\\n&#39;) } return (list(tp = true_positives, tn = true_negatives, fp = false_positives, fn = false_negatives)) } precision_and_recall &lt;- function(true_positives, true_negatives,false_positives, false_negatives, verbose = T){ # Protect against division by zero if ((true_positives + false_positives) &gt; 0){ precision = true_positives / (true_positives + false_positives) } else{ precision = 1 } if ((true_positives + false_negatives) &gt; 0){ recall = true_positives / (true_positives + false_negatives) } else{ recall = 0 } if (verbose){ cat(&quot;Precision: &quot;, precision, &quot;\\n&quot;, &quot;Recall: &quot;, recall, &quot;\\n&quot;) } return (list(precision = precision, recall = recall)) } f_score &lt;- function(precision, recall, verbose = T){ # Calculate F f = 2 * precision * recall / (precision + recall) # If the user has requested verbose output, provide it if(verbose){ cat(&quot;F-score: &quot;,f,&quot;\\n&quot;) } # Return F return(f) } IMPORTANT NOTE: READ CAREFULLY! Do not skip this part or you’ll run into issues later on! In a moment, after you’ve read the following instructions carefully, you should: - run the code chunk immediately below this text (keras_model_sequential()). - look down in the Console it asks if you want to install some packages: (“Would you like to install Miniconda? [Y/n]:”). - write n and press enter. You should see the following code in the console: Would you like to install Miniconda? [Y/n]: n. Now, you can normally continue with the exercise. If you were too eager and already pressed Y (yes) and enter, don’t panic! Just close your environment, re-open it and make sure that next time you go with n (no). keras_model_sequential() Turn the pseudo-code into real code! # 1. Read in the data # 2. Visualize data and print proportion of data belongs to each class. # 3. Split into training and test set (Stratified split). # 4. Create models # 5. Calculate output and make decisions for each model on test set. # 6. Visualize test predictions for each model. # 7. Calculate metrics for each model on test set. # 8. Calculate ROC and AUC for the best model. # 9. ROC Curve # 10. Train 5 times the best model, take probability outputs on test set for each time. # 11. Average over the models output and then make decisions. Print Metrics. "],["ch-11.html", "Chapter 11 Application 2: Neural Networks and Hyperparameter Tuning 11.1 Introduction 11.2 Application", " Chapter 11 Application 2: Neural Networks and Hyperparameter Tuning The goal of this application is to put the concepts that we have learnt over all these weeks into practice. The task at hand is to predict the productivity of vegetation (GPP_NT_VUT_REF), using an artificial neural network. For this task we will be using the FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-3_CLEAN data, that we obtained after the data wrangling tutorial 11.1 Introduction 11.1.1 Learning Goals After this exercise session you shall be able to Create training and testing splits (80/20) Center and scale your data features (pre-processing) Use KFold cross-validation for hyper-parameter tuning Build and train a simple, 1 hidden layer NN for regression tasks Optimise the number of hidden layer units. Evaluate feature importance and its effects on the model performance (maybe as bonus?) Definitely a bonus 11.1.2 Key Points from Previous Lectures It is always worthwhile to perform an initial exploratory analysis of your data, e.g. to identify outliers, missing values, etc. To train a statistical model using machine learning, we split our data into training and testing sets. Sometimes we also include a validation set. Testing data is set aside at the initial split and not “touched” during model training. It is key to test a model’s predictive power or whether it is overfitted. Validation data is used for determining the loss during model training. The reason for distinguishing between testing and validation data is to assure we’re not misleading model training by some peculiarities of the validation data and we get an assessment of generalisability based on data that was not seen during model training. Loss is a measure of how well our trained model predicts training labels. Loss is high when predictions are poor. Loss is low when predictions are good. Model training minimises the loss. In other words, it optimises (maximises) the agreement between predicted and observed values. There are several ways to measure loss. RMSE (Root Mean Squared Error) is one such measure. It is used in regression problems. Gradient descent is a method that searches for model parameters that minimize loss. Machine learning algorithms have hyperparameters. These are parameters that are set by the user rather than learned during training. An example is the learning rate in gradient descent. In the case of Artificial Neural Networks, this can be the number of nodes per hidden layer, or the number of hidden layers. To tune a model, you can set hyperparameters that determine model structure or calibrate the coefficients. Generalisability refers to the model’s performance on data not seen during the training - the testing data. To avoid overfitting, model generalisability is desired already during model training. One method to do this is cross validation. Data leakage is when data from the testing dataset creeps into the training data. To avoid this the testing set must be left completely untouched! 11.2 Application 11.2.1 Problem Statement For this exercise we can use the reduced and cleaned dataset with half-hourly data from Chapter 2. The task is to predict the productivity of vegetation (GPP), using all the other features as predictor variables. GPP_NT_VUT_REF from the dataset is the target variable and rest of the columns can be used as the input predictor variables. For the sake of limiting the task complexity, we assume the data to be IID generated, and treat each row in the dataset as an independent observation. Use these inputs to create a neural network with just one hidden layer. Use cross validation to optimise hyper-parameters such as number of hidden nodes, choice of activation function etc. Use mean squared error (MSE) loss as a metric for monitoring the performance, plot the MSE loss as a function of the number of epochs. Plot the MSE loss as a function of the hyperparameters used. The list of tasks mentioned here are not exhaustive, you’ll be guided through the application and asked questions along the way! 11.2.2 Data preparation ## Loading the required libraries library(keras) library(reticulate) library(caret) library(tidyverse) library(recipes) # use_condaenv() ### Read in the .csv file as a dataframe df &lt;- read.csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-3_CLEAN.csv&quot;) ### Things to check when preprocessing # Look at the structure and shape of the data (print it out using str()) # Convert time-stamp columns using as.POSIXct() to convert them from a character to date-time object # As we treat the observations as IID, we don&#39;t really need the time-stamp values as a feature variable, but we still retain them as they will be used for some time series plots later. dim(df) ## [1] 192864 20 str(df) ## &#39;data.frame&#39;: 192864 obs. of 20 variables: ## $ TIMESTAMP_START: chr &quot;2004-01-01T00:00:00Z&quot; &quot;2004-01-01T00:30:00Z&quot; &quot;2004-01-01T01:00:00Z&quot; &quot;2004-01-01T01:30:00Z&quot; ... ## $ TIMESTAMP_END : chr &quot;2004-01-01T00:30:00Z&quot; &quot;2004-01-01T01:00:00Z&quot; &quot;2004-01-01T01:30:00Z&quot; &quot;2004-01-01T02:00:00Z&quot; ... ## $ TA_F : num -2.22 -2.25 -2.28 -2.5 -2.72 ... ## $ SW_IN_F : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LW_IN_F : num 304 304 281 281 281 ... ## $ VPD_F : num 0.562 0.56 0.558 0.565 0.571 0.577 0.584 0.59 0.596 0.606 ... ## $ PA_F : num 93.3 93.3 93.3 93.3 93.3 ... ## $ P_F : num 0.014 0.014 0 0 0 0 0 0 0 0 ... ## $ WS_F : num 2.2 2.13 2.06 1.96 1.85 ... ## $ CO2_F_MDS : num NA NA NA NA NA NA NA NA NA NA ... ## $ PPFD_IN : num NA NA NA NA NA NA NA NA NA NA ... ## $ GPP_NT_VUT_REF : num NA NA NA NA NA NA NA NA NA NA ... ## $ SWC_F_MDS_1 : num NA NA NA NA NA NA NA NA NA NA ... ## $ SWC_F_MDS_2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ SWC_F_MDS_3 : num NA NA NA NA NA NA NA NA NA NA ... ## $ WS : num NA NA NA NA NA NA NA NA NA NA ... ## $ WD : num NA NA NA NA NA NA NA NA NA NA ... ## $ RH : num NA NA NA NA NA NA NA NA NA NA ... ## $ NIGHT : int 1 1 1 1 1 1 1 1 1 1 ... ## $ NEE_VUT_REF_QC : int 3 3 3 3 3 3 3 3 3 3 ... df$TIMESTAMP_START &lt;- as.POSIXct(df$TIMESTAMP_START, format = &quot;%Y-%m-%dT%TZ&quot;) df$TIMESTAMP_END &lt;- as.POSIXct(df$TIMESTAMP_END, format = &quot;%Y-%m-%dT%TZ&quot;) ### Check the structure of your data again (print it out using str()) str(df) ## &#39;data.frame&#39;: 192864 obs. of 20 variables: ## $ TIMESTAMP_START: POSIXct, format: &quot;2004-01-01 00:00:00&quot; &quot;2004-01-01 00:30:00&quot; ... ## $ TIMESTAMP_END : POSIXct, format: &quot;2004-01-01 00:30:00&quot; &quot;2004-01-01 01:00:00&quot; ... ## $ TA_F : num -2.22 -2.25 -2.28 -2.5 -2.72 ... ## $ SW_IN_F : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LW_IN_F : num 304 304 281 281 281 ... ## $ VPD_F : num 0.562 0.56 0.558 0.565 0.571 0.577 0.584 0.59 0.596 0.606 ... ## $ PA_F : num 93.3 93.3 93.3 93.3 93.3 ... ## $ P_F : num 0.014 0.014 0 0 0 0 0 0 0 0 ... ## $ WS_F : num 2.2 2.13 2.06 1.96 1.85 ... ## $ CO2_F_MDS : num NA NA NA NA NA NA NA NA NA NA ... ## $ PPFD_IN : num NA NA NA NA NA NA NA NA NA NA ... ## $ GPP_NT_VUT_REF : num NA NA NA NA NA NA NA NA NA NA ... ## $ SWC_F_MDS_1 : num NA NA NA NA NA NA NA NA NA NA ... ## $ SWC_F_MDS_2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ SWC_F_MDS_3 : num NA NA NA NA NA NA NA NA NA NA ... ## $ WS : num NA NA NA NA NA NA NA NA NA NA ... ## $ WD : num NA NA NA NA NA NA NA NA NA NA ... ## $ RH : num NA NA NA NA NA NA NA NA NA NA ... ## $ NIGHT : int 1 1 1 1 1 1 1 1 1 1 ... ## $ NEE_VUT_REF_QC : int 3 3 3 3 3 3 3 3 3 3 ... Recall the tutorial on data wrangling. We already performed some data-preprocessing there. We had replaced the missing value flag -9999 by NAs for the target variable GPP_NT_VUT_REF. We had used the categorical variable NEE_VUT_REF_QC, representing the quality control, to further edit some of the values of the target variable GPP_NT_VUT_REF. For all those rows where \\(NEE\\_VUT\\_REF\\_QC \\in \\{3,4\\}\\), i.e. poor quality of measurements, we have already replaced the target variable GPP_NT_VUT_REF with NAs. Thus we remove all the rows where the target variable is NA. As the target GPP_NT_VUT_REF is missing, we cannot learn or test against this data, so we remove all the rows where the target variable is missing. After we do this, the information that we previously encoded in NEE_VUT_REF_QC has already been used to filter out the rows with poor quality control measures. The variable NEE_VUT_REF_QC does not carry any additional information that would be helpful in predicting the target variable. Consequently, we can discard this variable. Note that NEE_VUT_REF_QC was a categorical feature in our dataset, and by discarding it we got lucky. Neural networks in keras require input data in the form of matrices, and if we had any other categorical variables present in our data, we would have to encode these e.g. One-Hot Encoding vectors. But indeed we have one remaining: NIGHT (Check unique(df$NIGHT)). In order to avoid further complications we remove Night from our dataset as well. ## Drop rows with NAs for GPP_NT_VUT_REF, and discard the columns NEE_VUT_REF_QC and NIGHT df &lt;- df %&gt;% drop_na(any_of(&quot;GPP_NT_VUT_REF&quot;)) %&gt;% select(-c(&quot;NEE_VUT_REF_QC&quot;,&quot;NIGHT&quot;)) ## Print the dimensionality of the data frame now to check how many rows have been deleted, and the num of columns remaining dim(df) ## [1] 176132 18 ## Print the summary() of your dataframe for some more information about your variables summary(df) ## TIMESTAMP_START TIMESTAMP_END TA_F ## Min. :2004-03-31 11:00:00 Min. :2004-03-31 11:30:00 Min. :-17.200 ## 1st Qu.:2007-02-04 11:52:30 1st Qu.:2007-02-04 12:22:30 1st Qu.: 1.520 ## Median :2009-10-01 17:15:00 Median :2009-10-01 17:45:00 Median : 8.010 ## Mean :2009-09-26 02:34:05 Mean :2009-09-26 03:04:06 Mean : 7.808 ## 3rd Qu.:2012-05-19 19:37:30 3rd Qu.:2012-05-19 20:07:30 3rd Qu.: 13.850 ## Max. :2014-12-31 23:30:00 Max. :2015-01-01 00:00:00 Max. : 31.820 ## NA&#39;s :20 NA&#39;s :20 ## SW_IN_F LW_IN_F VPD_F PA_F ## Min. : 0.000 Min. :135.4 Min. : 0.000 Min. :89.57 ## 1st Qu.: 0.000 1st Qu.:275.4 1st Qu.: 0.203 1st Qu.:92.86 ## Median : 2.139 Median :310.5 Median : 1.737 Median :93.34 ## Mean : 138.904 Mean :304.2 Mean : 3.313 Mean :93.26 ## 3rd Qu.: 182.627 3rd Qu.:337.2 3rd Qu.: 4.877 3rd Qu.:93.75 ## Max. :1074.410 Max. :423.9 Max. :34.391 Max. :95.32 ## ## P_F WS_F CO2_F_MDS PPFD_IN ## Min. :0.0000 Min. : 0.004 Min. :210.8 Min. : 3.399 ## 1st Qu.:0.0000 1st Qu.: 1.180 1st Qu.:370.7 1st Qu.: 4.114 ## Median :0.0000 Median : 2.065 Median :386.5 Median : 6.831 ## Mean :0.0656 Mean : 2.499 Mean :395.9 Mean : 281.226 ## 3rd Qu.:0.0590 3rd Qu.: 3.366 3rd Qu.:401.6 3rd Qu.: 341.000 ## Max. :3.5510 Max. :13.249 Max. :999.4 Max. :2170.000 ## NA&#39;s :5172 ## GPP_NT_VUT_REF SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 ## Min. :-35.4686 Min. : 7.70 Min. : 6.532 Min. : 7.32 ## 1st Qu.: -0.3901 1st Qu.:18.46 1st Qu.:17.340 1st Qu.:18.61 ## Median : 1.6599 Median :21.17 Median :22.050 Median :21.55 ## Mean : 4.8678 Mean :21.47 Mean :21.213 Mean :21.24 ## 3rd Qu.: 7.7467 3rd Qu.:25.03 3rd Qu.:24.690 3rd Qu.:24.01 ## Max. : 61.1749 Max. :32.74 Max. :32.086 Max. :31.79 ## NA&#39;s :15112 NA&#39;s :8344 NA&#39;s :8344 ## WS WD RH ## Min. : 0.004 Min. : 0.062 Min. : 17.09 ## 1st Qu.: 1.174 1st Qu.:101.198 1st Qu.: 65.74 ## Median : 2.065 Median :227.899 Median : 81.80 ## Mean : 2.502 Mean :188.123 Mean : 78.89 ## 3rd Qu.: 3.377 3rd Qu.:259.823 3rd Qu.: 97.10 ## Max. :13.249 Max. :359.987 Max. :100.00 ## NA&#39;s :4039 NA&#39;s :3442 NA&#39;s :6017 We see that there are still a few variables with NAs. Let’s see how many NAs each column has and what would be the number of rows in the resulting dataframe if we drop all rows with NAs ## Compute how many rows have NA values for each column, and report the result for each column, print the result # e.g. # TIMESTAMP_START 0 # TIMESTAMP_START0TIMESTAMP_END 0 # TA_F abc # . # . # . # RH xyz colSums(is.na(df)) ## TIMESTAMP_START TIMESTAMP_END TA_F SW_IN_F LW_IN_F ## 20 20 0 0 0 ## VPD_F PA_F P_F WS_F CO2_F_MDS ## 0 0 0 0 0 ## PPFD_IN GPP_NT_VUT_REF SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 ## 5172 0 15112 8344 8344 ## WS WD RH ## 4039 3442 6017 ## Compute the rows we will be left with, if we drop all rows where even 1 column contains NA, print the result nrow(df %&gt;% drop_na()) ## [1] 155451 The number of rows with NAs are not that large compared to the current size of the dataset. So we can afford to discard these rows without reducing the size of our dataset by much, and we still have enough data to carry out analysis. Sidenote: It can happen in some cases that by doing so we lose a majority of our dataset, because each row has at least 1 column with NA. If the number of rows reduced significantly by this operation, we would use some data imputation technique to fill-in these NA values. But also note that by doing so, we are introducing some bias to our model, by the data imputation technique we choose. ## drop all the rows with NAs df &lt;- df %&gt;% drop_na() str(df) ## &#39;data.frame&#39;: 155451 obs. of 18 variables: ## $ TIMESTAMP_START: POSIXct, format: &quot;2004-09-20 10:30:00&quot; &quot;2004-09-20 11:00:00&quot; ... ## $ TIMESTAMP_END : POSIXct, format: &quot;2004-09-20 11:00:00&quot; &quot;2004-09-20 11:30:00&quot; ... ## $ TA_F : num 13.3 13.4 13.8 14.5 14 ... ## $ SW_IN_F : num 561 462 536 672 390 ... ## $ LW_IN_F : num 346 346 346 346 346 ... ## $ VPD_F : num 2.87 3.35 3.71 4.58 4.25 ... ## $ PA_F : num 93.6 93.6 93.6 93.6 93.6 ... ## $ P_F : num 0.019 0.019 0.019 0.019 0.019 0 0 0 0 0 ... ## $ WS_F : num 2.29 2.86 2.42 2.91 2.87 ... ## $ CO2_F_MDS : num 356 355 354 353 352 ... ## $ PPFD_IN : num 1078 889 1030 1292 750 ... ## $ GPP_NT_VUT_REF : num 21.1 19.4 22.4 30.9 16.9 ... ## $ SWC_F_MDS_1 : num 16.4 16.4 16.4 16.4 16.4 ... ## $ SWC_F_MDS_2 : num 14.4 14.4 14.4 14.4 14.4 ... ## $ SWC_F_MDS_3 : num 16.7 16.7 16.7 16.7 16.7 ... ## $ WS : num 2.29 2.86 2.42 2.91 2.87 ... ## $ WD : num 244 252 248 249 256 ... ## $ RH : num 81.2 78.2 76.5 72.2 73.4 ... ## Here we create a few variables to reference different columns that might make preprocessing a little easier for us ## Feel free to use or not use them in the subsequent steps; If you wish not to use them, just put a &quot;#&quot; before the lines below time_cols &lt;- c(&quot;TIMESTAMP_START&quot;,&quot;TIMESTAMP_END&quot;) target_variable &lt;- c(&quot;GPP_NT_VUT_REF&quot;) column_names &lt;- colnames(df) predictors &lt;- column_names[!column_names %in% c(target_variable, time_cols)] ## time stamp columns and the target variables are not used as predictors Next, we create indices to split the entire dataset into train and test (80/20) split. The 80% of our data will be used to train the models and the rest 20% will be our held-out test set to evaluate the performance of the model. ## set seed for reproducibility set.seed(2021) ## First shuffle the dataset (Hint: sample() ) df &lt;- df[sample(nrow(df)),] ## get indices for train_data into train and test splits (Hint: sample() ) ind &lt;- sample(2, nrow(df), replace=TRUE, prob = c(0.8,0.2)) ## Use the indicies to get test and train splits train_split &lt;- df[ind == 1, ] ## include all columns test_split &lt;- df[ind == 2, ] ## include all columns Next we separate the target variable from the predictors. ## Save time stamps for the test and train splits, as a separate data frame for time-series plots train_data_time &lt;- train_split$TIMESTAMP_START test_data_time &lt;- test_split$TIMESTAMP_START ## Separate the splits to get train_data, train_target, test_data and test_target. After this you should have 4 corresponding dataframes. Also drop the time stamp columns from the train data and test data as we treat the observations as IID. ( we have stored them separately for plots ) train_target &lt;- train_split %&gt;% select(target_variable) test_target &lt;- test_split %&gt;% select(target_variable) train_data &lt;- train_split %&gt;% select(-one_of(c(target_variable, time_cols))) test_data &lt;- test_split %&gt;% select(-one_of(c(target_variable, time_cols))) head(test_data) ## TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F CO2_F_MDS PPFD_IN ## 94853 4.473 116.437 263.402 2.854 92.573 0.000 4.911 391.322 230.800 ## 119494 6.142 132.001 341.487 0.123 94.599 0.201 1.483 489.318 279.600 ## 91493 13.550 446.310 340.507 6.240 93.764 0.026 0.625 372.174 938.000 ## 65225 9.200 0.000 346.635 3.556 93.191 0.000 6.901 371.297 4.106 ## 119406 14.450 320.703 297.376 5.043 93.937 0.000 0.365 393.235 706.000 ## 41390 12.480 0.000 276.947 6.894 93.491 0.000 0.859 358.149 4.262 ## SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 WS WD RH ## 94853 27.760 24.550 24.140 4.911 225.587 66.06 ## 119494 28.861 28.542 28.722 1.483 105.247 98.70 ## 91493 27.620 24.990 26.960 0.625 222.012 59.82 ## 65225 15.780 17.330 16.660 6.901 252.697 69.45 ## 119406 27.528 26.817 27.282 0.365 198.350 69.37 ## 41390 18.420 20.860 21.390 0.859 137.811 52.39 11.2.3 Center and scale Take care to extract the centering and scaling parameters from the training set and use them to center and scale your test data. If you use the entire data set to get the centering and scaling parameters, we actually use information from the test-data, which is something we don’t have access to in real life. Thus doing so results in information leak from the test data, and we may get results more optimistic than our model’s true predictions. Follow the steps below to carry out centering and scaling in a proper way: Extract normalisation parameters from train data for numeric predictors Normalize train data using these parameters Normalize test data using the parameters extracted from train data Generally we only normalize the numeric variables and not the factors ## Make use of recipe() or any other function you wish, to scale and center each of the columns train_data_stat &lt;- preProcess(train_data, method = c(&quot;center&quot;,&quot;scale&quot;)) # get the statistics (mean, variance, etc) of numeric cols train_data[, predictors] &lt;- predict(train_data_stat, train_data) # transform the train data to center and scale it test_data[, predictors] &lt;- predict(train_data_stat, test_data) # transform the test data to center and scale it # OR # scale_and_center&lt;- recipe(train_data) %&gt;% # step_center(all_numeric()) %&gt;% # step_scale(all_numeric()) # prep_recipe &lt;- prep(scale_and_center, training = train_data) # train_data &lt;- bake(prep_recipe, new_data = train_data) # test_data &lt;- bake(prep_recipe, new_data = test_data) ## Display the summary of train data and test data summary(train_data) ## TA_F SW_IN_F LW_IN_F VPD_F ## Min. :-3.07087 Min. :-0.5797 Min. :-3.5688 Min. :-0.7826 ## 1st Qu.:-0.78187 1st Qu.:-0.5797 1st Qu.:-0.6240 1st Qu.:-0.7360 ## Median : 0.00522 Median :-0.5706 Median : 0.1365 Median :-0.3832 ## Mean : 0.00000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.75652 3rd Qu.: 0.1359 3rd Qu.: 0.7074 3rd Qu.: 0.3563 ## Max. : 2.91712 Max. : 4.1269 Max. : 2.5027 Max. : 7.6424 ## PA_F P_F WS_F CO2_F_MDS ## Min. :-4.9809 Min. :-0.4291 Min. :-1.3776 Min. :-3.29786 ## 1st Qu.:-0.5438 1st Qu.:-0.4291 1st Qu.:-0.7414 1st Qu.:-0.46095 ## Median : 0.1137 Median :-0.4291 Median :-0.2446 Median :-0.17433 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.6697 3rd Qu.:-0.0453 3rd Qu.: 0.4881 3rd Qu.: 0.09763 ## Max. : 2.7963 Max. :22.6680 Max. : 5.9249 Max. :10.73269 ## PPFD_IN SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 ## Min. :-0.5767 Min. :-3.06114 Min. :-2.7664 Min. :-2.85365 ## 1st Qu.:-0.5752 1st Qu.:-0.66958 1st Qu.:-0.9207 1st Qu.:-0.73046 ## Median :-0.5700 Median :-0.06612 Median : 0.1963 Median : 0.09601 ## Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.1117 3rd Qu.: 0.78896 3rd Qu.: 0.8196 3rd Qu.: 0.77429 ## Max. : 3.9776 Max. : 2.51471 Max. : 2.5913 Max. : 3.02886 ## WS WD RH ## Min. :-1.3776 Min. :-2.2447 Min. :-3.3769 ## 1st Qu.:-0.7414 1st Qu.:-1.0371 1st Qu.:-0.7124 ## Median :-0.2446 Median : 0.4799 Median : 0.1570 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.4881 3rd Qu.: 0.8422 3rd Qu.: 0.9816 ## Max. : 5.9249 Max. : 2.0263 Max. : 1.1509 summary(test_data) ## TA_F SW_IN_F LW_IN_F ## Min. :-3.0758483 Min. :-0.579662 Min. :-3.51786 ## 1st Qu.:-0.7919767 1st Qu.:-0.579662 1st Qu.:-0.64789 ## Median :-0.0009991 Median :-0.570660 Median : 0.11998 ## Mean :-0.0034195 Mean : 0.009953 Mean :-0.01008 ## 3rd Qu.: 0.7527867 3rd Qu.: 0.155803 3rd Qu.: 0.70732 ## Max. : 2.8885133 Max. : 4.014066 Max. : 2.47299 ## VPD_F PA_F P_F WS_F ## Min. :-0.782558 Min. :-4.55211 Min. :-0.42906 Min. :-1.37378 ## 1st Qu.:-0.734788 1st Qu.:-0.52891 1st Qu.:-0.42906 1st Qu.:-0.74359 ## Median :-0.383248 Median : 0.12314 Median :-0.42906 Median :-0.25731 ## Mean : 0.001133 Mean : 0.01085 Mean :-0.01593 Mean :-0.01063 ## 3rd Qu.: 0.357802 3rd Qu.: 0.67373 3rd Qu.:-0.07132 3rd Qu.: 0.47322 ## Max. : 7.332737 Max. : 2.78408 Max. :22.66796 Max. : 5.47831 ## CO2_F_MDS PPFD_IN SWC_F_MDS_1 SWC_F_MDS_2 ## Min. :-3.11425 Min. :-0.576680 Min. :-3.050001 Min. :-2.775959 ## 1st Qu.:-0.46494 1st Qu.:-0.575222 1st Qu.:-0.667353 1st Qu.:-0.942087 ## Median :-0.17863 Median :-0.570717 Median :-0.075031 Median : 0.192294 ## Mean :-0.01124 Mean : 0.008878 Mean :-0.005281 Mean :-0.009313 ## 3rd Qu.: 0.09051 3rd Qu.: 0.133568 3rd Qu.: 0.777824 3rd Qu.: 0.806045 ## Max. :10.27260 Max. : 3.824155 Max. : 2.481306 Max. : 2.529647 ## SWC_F_MDS_3 WS WD RH ## Min. :-2.850802 Min. :-1.37378 Min. :-2.242558 Min. :-3.357215 ## 1st Qu.:-0.721914 1st Qu.:-0.74359 1st Qu.:-1.042955 1st Qu.:-0.710242 ## Median : 0.090312 Median :-0.25731 Median : 0.466688 Median : 0.151513 ## Mean :-0.002816 Mean :-0.01063 Mean :-0.005619 Mean :-0.000871 ## 3rd Qu.: 0.754343 3rd Qu.: 0.47322 3rd Qu.: 0.842417 3rd Qu.: 0.970671 ## Max. : 2.845898 Max. : 5.47831 Max. : 2.022990 Max. : 1.150886 train_data_stat &lt;- preProcess(train_data, method = c(&quot;center&quot;, &quot;scale&quot;)) #initialize of same dimension train_data_cs &lt;- train_data test_data_cs &lt;- test_data for (i in 1:ncol(train_data)){ #center and scale training data set train_data_cs[,i] &lt;- (train_data[,i] - train_data_stat$mean[i])/train_data_stat$std[i] #center and scale testing data set test_data_cs[,i] &lt;- (test_data[,i] - train_data_stat$mean[i])/train_data_stat$std[i] } summary(train_data_cs) ## TA_F SW_IN_F LW_IN_F VPD_F ## Min. :-3.07087 Min. :-0.5797 Min. :-3.5688 Min. :-0.7826 ## 1st Qu.:-0.78187 1st Qu.:-0.5797 1st Qu.:-0.6240 1st Qu.:-0.7360 ## Median : 0.00522 Median :-0.5706 Median : 0.1365 Median :-0.3832 ## Mean : 0.00000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.75652 3rd Qu.: 0.1359 3rd Qu.: 0.7074 3rd Qu.: 0.3563 ## Max. : 2.91712 Max. : 4.1269 Max. : 2.5027 Max. : 7.6424 ## PA_F P_F WS_F CO2_F_MDS ## Min. :-4.9809 Min. :-0.4291 Min. :-1.3776 Min. :-3.29786 ## 1st Qu.:-0.5438 1st Qu.:-0.4291 1st Qu.:-0.7414 1st Qu.:-0.46095 ## Median : 0.1137 Median :-0.4291 Median :-0.2446 Median :-0.17433 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.6697 3rd Qu.:-0.0453 3rd Qu.: 0.4881 3rd Qu.: 0.09763 ## Max. : 2.7963 Max. :22.6680 Max. : 5.9249 Max. :10.73269 ## PPFD_IN SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 ## Min. :-0.5767 Min. :-3.06114 Min. :-2.7664 Min. :-2.85365 ## 1st Qu.:-0.5752 1st Qu.:-0.66958 1st Qu.:-0.9207 1st Qu.:-0.73046 ## Median :-0.5700 Median :-0.06612 Median : 0.1963 Median : 0.09601 ## Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.1117 3rd Qu.: 0.78896 3rd Qu.: 0.8196 3rd Qu.: 0.77429 ## Max. : 3.9776 Max. : 2.51471 Max. : 2.5913 Max. : 3.02886 ## WS WD RH ## Min. :-1.3776 Min. :-2.2447 Min. :-3.3769 ## 1st Qu.:-0.7414 1st Qu.:-1.0371 1st Qu.:-0.7124 ## Median :-0.2446 Median : 0.4799 Median : 0.1570 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.4881 3rd Qu.: 0.8422 3rd Qu.: 0.9816 ## Max. : 5.9249 Max. : 2.0263 Max. : 1.1509 summary(test_data_cs) ## TA_F SW_IN_F LW_IN_F ## Min. :-3.0758483 Min. :-0.579662 Min. :-3.51786 ## 1st Qu.:-0.7919767 1st Qu.:-0.579662 1st Qu.:-0.64789 ## Median :-0.0009991 Median :-0.570660 Median : 0.11998 ## Mean :-0.0034195 Mean : 0.009953 Mean :-0.01008 ## 3rd Qu.: 0.7527867 3rd Qu.: 0.155803 3rd Qu.: 0.70732 ## Max. : 2.8885133 Max. : 4.014066 Max. : 2.47299 ## VPD_F PA_F P_F WS_F ## Min. :-0.782558 Min. :-4.55211 Min. :-0.42906 Min. :-1.37378 ## 1st Qu.:-0.734788 1st Qu.:-0.52891 1st Qu.:-0.42906 1st Qu.:-0.74359 ## Median :-0.383248 Median : 0.12314 Median :-0.42906 Median :-0.25731 ## Mean : 0.001133 Mean : 0.01085 Mean :-0.01593 Mean :-0.01063 ## 3rd Qu.: 0.357802 3rd Qu.: 0.67373 3rd Qu.:-0.07132 3rd Qu.: 0.47322 ## Max. : 7.332737 Max. : 2.78408 Max. :22.66796 Max. : 5.47831 ## CO2_F_MDS PPFD_IN SWC_F_MDS_1 SWC_F_MDS_2 ## Min. :-3.11425 Min. :-0.576680 Min. :-3.050001 Min. :-2.775959 ## 1st Qu.:-0.46494 1st Qu.:-0.575222 1st Qu.:-0.667353 1st Qu.:-0.942087 ## Median :-0.17863 Median :-0.570717 Median :-0.075031 Median : 0.192294 ## Mean :-0.01124 Mean : 0.008878 Mean :-0.005281 Mean :-0.009313 ## 3rd Qu.: 0.09051 3rd Qu.: 0.133568 3rd Qu.: 0.777824 3rd Qu.: 0.806045 ## Max. :10.27260 Max. : 3.824155 Max. : 2.481306 Max. : 2.529647 ## SWC_F_MDS_3 WS WD RH ## Min. :-2.850802 Min. :-1.37378 Min. :-2.242558 Min. :-3.357215 ## 1st Qu.:-0.721914 1st Qu.:-0.74359 1st Qu.:-1.042955 1st Qu.:-0.710242 ## Median : 0.090312 Median :-0.25731 Median : 0.466688 Median : 0.151513 ## Mean :-0.002816 Mean :-0.01063 Mean :-0.005619 Mean :-0.000871 ## 3rd Qu.: 0.754343 3rd Qu.: 0.47322 3rd Qu.: 0.842417 3rd Qu.: 0.970671 ## Max. : 2.845898 Max. : 5.47831 Max. : 2.022990 Max. : 1.150886 Are all of the columns in the training set are centered perfectly with a mean = 0.0000? Yes! Are all of the columns in the testing set are centered perfectly with a mean = 0.0000? No! Explanation: This is because the mean of the training set is used to center it, thus that is centered perfectly. Using the statistics of the train data, we centered the test data, thus we can see the data is not exactly centered, but it is normalized enough to bring all features to 1 scale and to not affect the predictions at test time. 11.2.4 Building a simple model with keras ( SubTask 1) To train keras models, the input data needs to be as a R matrix or arrays. Since all our features are numeric, we can safely convert the train and test datasets to matrices. ## Convert the &quot;train_data&quot;, &quot;train_targets&quot;, &quot;test_data&quot; and &quot;test_targets&quot;, to matrix using as.matrix() train_data &lt;- as.matrix(train_data) train_target &lt;- as.matrix(train_target) test_data &lt;- as.matrix(test_data) test_target &lt;- as.matrix(test_target) Now we can build a sequential model with keras. Define a model with 1 hidden layer with 20 units, and 1 output unit, for the target variable. Remember to specify a non-linear activation such as Sigmoid or ReLU for the hidden layer, and a linear activation function for the output unit (as it is a regression task). If we don’t specify a non-linear activation we will just be training a linear mapping! When compiling the model take care to use appropriate optimiser, loss and evaluation metric. Use mean squared error as the loss, and mean absolute error as the evaluation metric Side Note: Since our model is not very deep in this application (only 1 hidden layer), we can use any of the non-linear activations (tanh, sigmoid, or ReLu), but in practice when we train deep neural networks (with a large number of hidden layers) we prefer to use ReLU as the activation function, as it does not suffer from the problem of Vanishing or Exploding gradients (which happens if we use sigmoid or tanh). In a nutshell : In a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large then the gradient will increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of exploding gradient. Alternatively, if the derivatives are small then the gradient will decrease exponentially as we propagate through the model until it eventually vanishes, and this is the vanishing gradient problem. [Source for the interested] (https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11). ## Define and compile the model model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 20, activation = &#39;relu&#39;, input_shape = ncol(train_data)) %&gt;% layer_dense(units = 1) %&gt;% compile( optimizer = optimizer_adam(lr=0.001), loss = &#39;mse&#39;, metrics = list(&#39;mean_absolute_error&#39;)) ## Print the summary of the model using summary() summary(model) ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense (Dense) (None, 20) 320 ## ________________________________________________________________________________ ## dense_1 (Dense) (None, 1) 21 ## ================================================================================ ## Total params: 341 ## Trainable params: 341 ## Non-trainable params: 0 ## ________________________________________________________________________________ How many training parameters are needed? Can you account for the total number of parameters at each layer? The network needs 341 parameters. These are split as Layer 1: 320 parameters – (20x15) weight and (20x1) bias matrix, and Layer 2: 21 parameters – (20x1) weight matrix and (1x1) bias unit. ## Fit the model and store the training history. Use a reasonable batch_size, epochs, and validation_split ## The validation split here is a portion of the training data that is kept apart to evaluate the loss and evaluation metric at the end of each epoch. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This is useful for stopping training after a certain number of epochs if we see the validation error increasing with training. We can also program the model to do so, this is called Early Stopping. ## This can be done by using a training callback for early stopping: callback_early_stopping() ## More info on training callbacks: https://keras.rstudio.com/articles/training_callbacks.html ## More info on the fit function https://keras.rstudio.com/reference/fit.html ## You can choose the number of epochs to train, manually by monitoring the validation loss while training and selecting the number of epochs where the validation loss starts to increase. Alternatively you can use early stopping to pause the model training at an optimal point for you. Choose whatever me(thod you feel is best! history &lt;- fit( object = model, x = train_data, y = train_target, batch_size = 128, validation_split = 0.15, epochs = 50, shuffle = TRUE, callbacks = list(callback_early_stopping( monitor = &quot;val_loss&quot;, patience = 0)) ) ## Patience : number of epochs with no improvement after which training will be stopped. Now that the model training is complete we can look at the history, plot the training history, and look at our trained weights. Pay attention to the trained weight matrices and try to recognise the transformation weights and the bias units. # print(history) # plot history # plot(history, metrics = c(&quot;loss&quot;), smooth = getOption(&quot;keras.plot.history.smooth&quot;, TRUE)) plot(history) # Get weights of model get_weights(model) ## [[1]] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2.869463682 2.19894099 -2.60152483 0.51017755 8.1782951 -0.1264951 ## [2,] -0.257749677 -6.38722610 -0.26673290 0.12389217 1.9635506 5.5812244 ## [3,] -0.810692906 -0.22325119 3.41061616 1.36443913 1.3101629 9.2959471 ## [4,] -3.332208872 -3.74554729 0.69165123 -2.23075891 1.1527030 -7.5465598 ## [5,] 0.009921599 -0.57942855 0.06560050 0.35118413 1.8717020 0.5662153 ## [6,] -0.639508903 -0.51970196 -7.03948593 -0.35604978 -1.0026512 0.5032421 ## [7,] 0.117622830 0.18488406 -0.07820407 -0.74191588 0.4872454 1.5795393 ## [8,] -1.078623652 0.32196170 -0.07609604 -0.92650974 -2.3898470 1.1969419 ## [9,] -0.291276693 -7.54775238 -0.15960379 -0.36234933 2.0654554 -0.3033638 ## [10,] -0.354794204 0.27889904 -1.32088995 -0.26132381 6.1111360 -0.0110246 ## [11,] -0.075735129 -0.51960874 -1.08324683 1.22804534 -9.2768269 -0.3711588 ## [12,] 0.058648713 0.01862233 1.23881853 0.08873922 0.6291698 -0.2280734 ## [13,] -0.219447121 0.06975523 -0.28102785 0.84798932 0.1717782 5.7479606 ## [14,] -0.777138412 -2.31531525 7.50002480 -2.60044813 7.6696396 -6.5305896 ## [15,] -1.614494205 -0.05201649 -0.51413023 -0.94068855 -2.0706074 -3.8512673 ## [,7] [,8] [,9] [,10] [,11] ## [1,] 0.04682195 -0.052829422 1.172855377 6.30979824 0.52859932 ## [2,] -0.22194999 -0.243081853 -2.264952421 3.04037237 -0.95039535 ## [3,] 0.14613368 0.332655936 0.491766244 4.90739346 -0.50222224 ## [4,] 0.05983398 -0.335039854 -2.523161650 -4.82348061 -0.73482287 ## [5,] 0.20005836 -0.089846745 0.515249848 -0.01903337 0.29309896 ## [6,] -0.76276135 -0.172726154 0.191874042 1.01086140 -0.50880861 ## [7,] -0.27699345 0.031146886 -0.068503618 -1.35167754 -0.09551523 ## [8,] -0.19921121 -0.050250128 -1.608266592 -1.96596658 -0.53051108 ## [9,] -0.29272005 -0.153516173 -1.579962850 1.97364700 -1.06721187 ## [10,] -0.35709265 -0.048273247 0.995545089 -1.33178234 0.56702429 ## [11,] -0.38769460 0.195704937 1.311706424 1.23969197 0.12406404 ## [12,] 0.48858991 -0.010217057 0.085440978 -0.37758657 0.27877322 ## [13,] -0.09290090 0.005608748 0.484116018 -1.63200808 0.39846551 ## [14,] 0.25080982 -0.011659722 -1.215049624 -1.07155561 -0.32186502 ## [15,] -0.06362370 0.235450491 0.007811341 -8.73015308 -0.52522039 ## [,12] [,13] [,14] [,15] [,16] [,17] ## [1,] -0.711098015 -0.155946359 -5.3655915 7.3770199 0.293657303 -0.19214493 ## [2,] -0.365851760 0.089299537 1.3707904 -5.9960160 -0.127565414 -0.53482211 ## [3,] 0.055914585 -0.007191619 -2.1241577 4.7871284 -0.239663601 -0.07870594 ## [4,] -0.827158570 0.168920189 -4.2775497 -7.5697961 0.079768963 -0.42832005 ## [5,] -0.786235392 -0.258505225 -0.2865027 -0.2799671 -0.104162373 -0.08785495 ## [6,] -0.007873034 -0.371807247 -4.7030549 0.5258003 -0.060212884 -0.01263460 ## [7,] 0.003547309 -0.341258347 5.6166472 -1.9062901 -0.028141027 0.04434934 ## [8,] -0.368607908 -0.188798785 -2.9444225 -1.5065690 0.002863772 -0.20875025 ## [9,] -0.495464146 0.092135318 2.9440861 -4.1805649 -0.266763061 -0.20316155 ## [10,] -0.042366728 -0.102412179 0.4786149 -0.9693843 0.136674106 -0.17017232 ## [11,] -0.495398074 -0.109507836 1.6707268 1.7287951 -0.145421311 0.10051851 ## [12,] -0.254084796 0.439668804 2.7467921 -1.3010908 0.075597413 0.29256424 ## [13,] -0.634631693 0.172714919 1.5466242 -1.5283233 0.020010771 -0.13009861 ## [14,] 0.059612643 0.105869599 -6.3877649 -1.9179984 -0.045273066 -0.40067214 ## [15,] 0.231342480 -0.376736462 -1.6059383 -10.0494280 -0.094165772 0.14342698 ## [,18] [,19] [,20] ## [1,] 5.5032167 -0.46716672 -0.6539110 ## [2,] 1.9781926 -0.42978939 0.3046646 ## [3,] 0.1271483 0.47976965 -0.2256196 ## [4,] -12.0853949 -0.80036938 0.4287973 ## [5,] 0.9761505 -0.22349076 0.2876348 ## [6,] -2.0184972 -0.84300071 -0.2434829 ## [7,] -2.3686037 0.22000718 -0.4334196 ## [8,] -6.2144308 -0.28150293 -0.5173919 ## [9,] 0.8613948 -0.59269583 0.3008582 ## [10,] 0.1340202 0.15566082 0.1325917 ## [11,] 0.9494711 -0.08530131 0.1096366 ## [12,] 0.4856019 0.75556695 0.5051430 ## [13,] -1.7481350 -0.40033895 0.7129067 ## [14,] -5.9514213 -0.21660900 0.1049089 ## [15,] -2.7778580 0.19276462 -0.3107617 ## ## [[2]] ## [1] -9.184246 1.047771 -20.209112 -8.611093 -19.685486 -16.133827 ## [7] -2.834236 -1.757904 -15.453846 -6.230545 -7.671953 -6.257191 ## [13] -2.829708 -21.390947 -14.927919 -1.536069 -2.516061 -11.184190 ## [19] -5.883048 -4.925455 ## ## [[3]] ## [,1] ## [1,] 0.47884747 ## [2,] -0.44409966 ## [3,] 0.72907656 ## [4,] 0.12370997 ## [5,] -0.52531523 ## [6,] 0.34080574 ## [7,] 0.41197014 ## [8,] -0.07848634 ## [9,] -2.98452473 ## [10,] 0.82209224 ## [11,] -0.90803719 ## [12,] -0.10138759 ## [13,] -0.12153122 ## [14,] -0.31301230 ## [15,] -0.70200449 ## [16,] 0.04088609 ## [17,] 0.11272940 ## [18,] 0.19971827 ## [19,] 0.26930034 ## [20,] -0.10159779 ## ## [[4]] ## [1] 4.201512 Now we can evaluate our model predictions on the held-out testing set. ## make predictions and evaluate using test set test_data_predictions &lt;- predict(model, test_data) eval_result &lt;- model %&gt;% evaluate(test_data, test_target) print(eval_result) ## $loss ## [1] 26.00271 ## ## $mean_absolute_error ## [1] 3.288165 Next we can plot the model predictions with along with the ground truth for the test and training dataset. Check if the model predictions actually follow the trend in the data. ## plot the test data observations along with the test data predictions. Use color red for predictions, and blue for the observed data. plt_1 &lt;- ggplot() + geom_line(data = test_split, aes(x = test_data_time, y = test_data_predictions), col = alpha(&quot;red&quot;, 0.5)) + geom_line(data = test_split, aes(x = test_data_time, y = test_target),col=alpha(&quot;blue&quot;,0.3))+ xlab(&quot;Time of observation&quot;)+ ylab(&quot;Prediction and observation&quot;)+ labs(title=&quot;Test data&quot;) plt_1 ## plot the train data observations along with the train data predictions. Use color red for predictions, and blue for the observed data. train_data_predictions &lt;- predict(model, train_data) plt_2 &lt;- ggplot() + geom_line(data = train_split, aes(x = train_data_time, y = train_data_predictions), col = alpha(&quot;red&quot;, 0.5)) + geom_line(data = train_split, aes(x = train_data_time, y = train_target),col=alpha(&quot;blue&quot;,0.15))+ xlab(&quot;Time of observation&quot;)+ ylab(&quot;Prediction and observation&quot;)+ labs(title=&quot;Train data&quot;) plt_2 ## Hint: For both the plots above, edit the alpha value such that the test data observations and the test data predictions both can be seen even with overlap Nice! From our domain knowledge we know that the target varible GPP_NT_VUT_REF cannot be negative, but our dataset contains some negative observations because of noisy measurements. From the plot we see that the model learns the trends in the data, without learning the noise. Finally let’s make a plot of our predictions v/s observed data. In an ideal world this should be a straight line y=x, giving us a Pearson’s correlation coefficient of 1, but because of the noise in data we can expect some deviations from the ideal result. ## plot test set predictions v/s test set observations (from the test dataset) plt3 &lt;- ggplot(test_split, aes(x = test_target, y=test_data_predictions)) + geom_point() plt3 What is the Pearson’s correlation coefficient obtained for this? ## compute the correlation coefficient r2 &lt;- function (p, q) {cor(p, q)^2} r2(test_target, test_data_predictions) ## [,1] ## GPP_NT_VUT_REF 0.6658682 Having done this we have an idea of how well our model generalises against unseen data. But, to optimise the hyperparameters of the network we should not use the performance on the test set to select these hyperparameters. If we tune our model hyperparameters using results from the test set, then the performance of our best model on the test set underestimates the true risk of our model. In the following steps we do the same things as we have discusses until now, but with 5-fold cross validation. We will do cross validation “by-hand”, as we try to avoid using caret here, and doing so by hand gives us a more fine grain control. Let’s put the model creating and training in a function which takes the different hyper-parameters as inputs. This will make it easier to build a model from inside a cross validation loop. Write a function, that does all the above steps, and returns a trained model. This will come in handy when we perform cross-validation. Use mean squared error as your loss and evaluation metric. Don’t forget to use early stopping (callback_early_stopping()). # Function name: build_model() # Inputs: Training data, Training target, number of hidden units, activation type, number of epochs, batch size and validation size # Output: List of 2 objects --&gt; trained model, and the training history ## We also return the training history in case we want to make plots of the training process for each of the folds build_model &lt;- function(X_train, y_train, num_units, activation_type, num_epochs, batch_size, val_size){ model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = num_units, activation = activation_type, input_shape = ncol(X_train)) %&gt;% layer_dense(units = 1) %&gt;% compile( optimizer = optimizer_adam(lr=0.001), loss = &#39;mean_squared_error&#39;, metrics = &#39;mean_squared_error&#39;) history &lt;- fit( object = model, x = X_train, y = y_train, batch_size = batch_size, epochs = num_epochs, validation_split = val_size, shuffle = TRUE, callbacks = list(callback_early_stopping( monitor = &quot;val_loss&quot;, patience = 0)) ) trained_model &lt;- list(&quot;model&quot; = model, &quot;history&quot; = history) return(trained_model) } 11.2.5 Cross validation In this section you will perform 5-Fold CV, “by - hand” # We prepare our cross validation splits. First we randomly shuffle the initial train set. # Note: Use the data that we obtained from the train split, from before we centered and scaled the data. # Using the already centered and scaled data, will introduce information leak into out cross-validation folds cv_data &lt;- train_split cv_data &lt;- cv_data[sample(nrow(cv_data)),] #Create 5 equally size folds ## hint: folds &lt;- cut(seq(1,nrow(cv_data)),breaks=5,labels=FALSE) folds &lt;- cut(seq(1,nrow(cv_data)),breaks=5,labels=FALSE) ## create a list, to store the results across different folds history_list &lt;- list() cv_performance_list &lt;- list() #Perform 5 fold cross validation for(i in 1:5){ cat(sprintf(&quot;CV Fold --&gt; %i/5\\n&quot;, i)) #Segment the data by fold ## hint : indices &lt;- which(folds==i,arr.ind=TRUE) indices &lt;- which(folds==i,arr.ind=TRUE) ## indices is a vector with TRUE for the rows where the folds == i; and FALSE for the rest of the row ## indices which are TRUE for a particular loop form the cross-validation test set for that loop ## Segment data as cv_test_data and cv_train data using the indices. ## Now we just repeat the steps we did previously ## Seperate out the cv_test_data, cv_test_targets, cv_train_data, cv_train_targets. We won&#39;t be making any time series plots when cross-validating, so we can drop the timestamp columns cv_test_data &lt;- cv_data[indices, ] cv_test_target &lt;- cv_test_data %&gt;% select(target_variable) cv_test_data &lt;- cv_test_data %&gt;% select(-one_of(c(target_variable, time_cols))) cv_train_data &lt;- cv_data[-indices, ] cv_train_target &lt;- cv_train_data %&gt;% select(target_variable) cv_train_data &lt;- cv_train_data %&gt;% select(-one_of(c(target_variable, time_cols))) # scale and center using cv_train_data # get the statistics (mean, variance, etc) of numeric cols cv_train_data_stat &lt;- preProcess(cv_train_data, method = c(&quot;center&quot;,&quot;scale&quot;)) # transform the cv_train_data and cv_test_data to center and scale it cv_train_data[, predictors] &lt;- predict(cv_train_data_stat, cv_train_data) cv_test_data[, predictors] &lt;- predict(cv_train_data_stat, cv_test_data) # OR # scale_and_center&lt;- recipe(cv_train_data) %&gt;% # step_center(all_numeric()) %&gt;% # step_scale(all_numeric()) # prep_recipe &lt;- prep(scale_and_center, training = cv_train_data) # cv_train_data &lt;- bake(prep_recipe, new_data = cv_train_data) # cv_test_data &lt;- bake(prep_recipe, new_data = cv_test_data) ## Convert data using as.matrix() cv_train_data &lt;- as.matrix(cv_train_data) cv_train_target &lt;- as.matrix(cv_train_target) cv_test_data &lt;- as.matrix(cv_test_data) cv_test_target &lt;- as.matrix(cv_test_target) ## train a model using the build_model() function we wrote previously # cv_model &lt;- build_model() cv_model &lt;- build_model(X_train = cv_train_data, y_train = cv_train_target, num_units = 20, activation_type = &quot;relu&quot;, num_epochs = 30, batch_size = 128, val_size = 0.15) # get the evaluation error on the test folds eval_result &lt;- cv_model$model %&gt;% evaluate(cv_test_data, cv_test_target) # Store the results in the lists we made outside the loop cv_performance_list[[i]] &lt;- eval_result history_list[[i]] &lt;- cv_model$history cat(&quot;Evaluation error on the held out fold: &quot; , eval_result$mean_squared_error,&quot;\\n&quot;) } ## CV Fold --&gt; 1/5 ## Model: &quot;sequential_1&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_2 (Dense) (None, 20) 320 ## ________________________________________________________________________________ ## dense_3 (Dense) (None, 1) 21 ## ================================================================================ ## Total params: 341 ## Trainable params: 341 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 23.76635 ## CV Fold --&gt; 2/5 ## Model: &quot;sequential_2&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_4 (Dense) (None, 20) 320 ## ________________________________________________________________________________ ## dense_5 (Dense) (None, 1) 21 ## ================================================================================ ## Total params: 341 ## Trainable params: 341 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.21327 ## CV Fold --&gt; 3/5 ## Model: &quot;sequential_3&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_6 (Dense) (None, 20) 320 ## ________________________________________________________________________________ ## dense_7 (Dense) (None, 1) 21 ## ================================================================================ ## Total params: 341 ## Trainable params: 341 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.25686 ## CV Fold --&gt; 4/5 ## Model: &quot;sequential_4&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_8 (Dense) (None, 20) 320 ## ________________________________________________________________________________ ## dense_9 (Dense) (None, 1) 21 ## ================================================================================ ## Total params: 341 ## Trainable params: 341 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.77271 ## CV Fold --&gt; 5/5 ## Model: &quot;sequential_5&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_10 (Dense) (None, 20) 320 ## ________________________________________________________________________________ ## dense_11 (Dense) (None, 1) 21 ## ================================================================================ ## Total params: 341 ## Trainable params: 341 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.60175 history_list &lt;- readRDS(&quot;./data/history_list_11.rds&quot;) cv_performance_list &lt;- readRDS(&quot;./data/cv_performance_list.rds&quot;) After running this loop we would have trained 5 different models, on our cross validation folds, and logged the evaluation results in “cv_performance_list”. Next we want to plot the MSE (mean square error), across the 5 cross validation test folds. Use the list containing these results to make the plot. Taking an average of the cross validation test score (MSE) for all the different folds, we get the average cross validation loss, which can then be used in model tuning. ## Plot the training history for all the 5 folds for (i in 1:5){ plt&lt;-plot(history_list[[i]]) print(plt) } ## plot MSE across cross validation test folds loss_list &lt;- rep(NA, 5) for (i in 1:5){ loss_list[i] &lt;- cv_performance_list[[i]]$mean_squared_error } ggplot() + geom_point(aes(c(1:5), loss_list), size = 4) + labs(title=&quot;MSE on left out folds&quot;, x =&quot;Cross-Validation Folds&quot;, y = &quot;Mean Squared Error&quot;) ## compute the average MSE across folds avg_cv_loss &lt;- mean(loss_list) avg_cv_loss What is the average cross validation error obtained for this configuration of the network ? 11.2.6 Parameter Tuning Here we will write a function that does the cross-validation for parameter tuning. The content of the cross-validation code remains the same as we have seen before. This is the same code written as a function that also takes the hyperparameter to be tuned as an argument. Your task is to optimise the number of hidden units. First we will organise all the steps done until now to write the function. # Function Name: cross_validate_model() # Inputs: Training split, number of hidden units, and number of epochs # Output: The average cross validation loss across all 5 folds. ## Hint: All you have to do is organise the steps we followed in the crossvalidation section as a function to make it reproducible cross_validate_model &lt;- function(train_split, num_units, num_epochs){ #Randomly shuffle the data cv_data &lt;- train_split cv_data &lt;- cv_data[sample(nrow(cv_data)),] #Create 5 equally size folds folds &lt;- cut(seq(1,nrow(cv_data)),breaks=5,labels=FALSE) history_list &lt;- list() cv_performance_list &lt;- list() #Perform 5 fold cross validation for(i in 1:5){ cat(sprintf(&quot;CV Fold --&gt; %i/5\\n&quot;, i)) #Segement the data by fold using the which() function indices &lt;- which(folds==i,arr.ind=TRUE) ## indices is a vector with TRUE for the rows where the folds == i; and FALSE for the rest of the row ## indices which are TRUE for a particular loop form the cross-validation test set for that loop cv_test_data &lt;- cv_data[indices, ] cv_test_target &lt;- cv_test_data %&gt;% select(target_variable) cv_test_data &lt;- cv_test_data %&gt;% select(-one_of(c(target_variable, time_cols))) cv_train_data &lt;- cv_data[-indices, ] cv_train_target &lt;- cv_train_data %&gt;% select(target_variable) cv_train_data &lt;- cv_train_data %&gt;% select(-one_of(c(target_variable, time_cols))) # scale and center using cv_train set # get the statistics (mean, variance, etc) of numeric cols cv_train_data_stat &lt;- preProcess(cv_train_data, method = c(&quot;center&quot;,&quot;scale&quot;)) # transform the train data to center and scale it cv_train_data &lt;- predict(cv_train_data_stat, cv_train_data) cv_test_data &lt;- predict(cv_train_data_stat, cv_test_data) # OR # scale_and_center&lt;- recipe(cv_train_data) %&gt;% # step_center(all_numeric()) %&gt;% # step_scale(all_numeric()) # prep_recipe &lt;- prep(scale_and_center, training = cv_train_data) # cv_train_data &lt;- bake(prep_recipe, new_data = cv_train_data) # cv_test_data &lt;- bake(prep_recipe, new_data = cv_test_data) # if(!missing(subset_predictors)){ # ## take a subset of predictors # cv_train_data &lt;- cv_train_data %&gt;% select(subset_predictors) # cv_test_data &lt;- cv_test_data %&gt;% select(subset_predictors) # } cv_train_data &lt;- as.matrix(cv_train_data) cv_train_target &lt;- as.matrix(cv_train_target) cv_test_data &lt;- as.matrix(cv_test_data) cv_test_target &lt;- as.matrix(cv_test_target) cv_model &lt;- build_model(X_train = cv_train_data, y_train = cv_train_target, num_units = num_units, activation_type = &quot;relu&quot;, num_epochs = num_epochs, batch_size = 128, val_size = 0.15) eval_result &lt;- cv_model$model %&gt;% evaluate(cv_test_data, cv_test_target) cv_performance_list[[i]] &lt;- eval_result cat(&quot;Evaluation error on the held out fold: &quot; , eval_result$mean_squared_error,&quot;\\n&quot;) } return(cv_performance_list) } Next you need to write a function to iterate over the list of hyper-parameters. In each iteration we iterate over a list of the possible number of hidden units, and compute the average cross-validation error. # Function Name: tune_num_units() # Inputs: training split, num_units --&gt; a vector containing the possible values of hidden units # Output: List / vector containing the average cross validation loss for each possible value of hidden units tune_num_units &lt;- function(train_split, num_units){ ## create an empty list to store the average loss for each value in num_units avg_loss_list&lt;- rep(NA, length(num_units)) ## write a loop to iterate over num_units and store the average cross validation loss p &lt;- 1 for (k in num_units){ performance&lt;- cross_validate_model(train_split = train_split, num_units = k, num_epochs = 30) loss &lt;- rep(NA,5) for (i in 1:5) { loss[i] &lt;- performance[[i]]$mean_squared_error } avg_loss_list[p] &lt;- mean(loss) p &lt;- p+1 } print(avg_loss_list) return(avg_loss_list) } Now let’s consider the given vector “num_units” for tuning the number of hidden units # num_units &lt;- c(1,3,10,30,100) num_units &lt;- c(1,3,10,30,100) ## compute the average cross validation loss for each element in num_units using tune_num_units() # Output is shown at bottom of this chapter due to its length. tune_results &lt;- tune_num_units(train_split = train_split, num_units = num_units) ## Print out the results print(tune_results) ## [1] 31.07678 25.69231 24.78246 24.29657 23.89560 ## Plot the average cross validation loss, as a function of the num_units plot(num_units, tune_results, type = &quot;b&quot;) ## What is the number of hidden units that gives the minimum loss ? min_loss &lt;- min(tune_results) min_loss_index &lt;- which.min(tune_results) num_units_best &lt;- num_units[min_loss_index] sprintf(&quot;Number of units for minimal average cross validation error: %i&quot;, num_units_best) ## [1] &quot;Number of units for minimal average cross validation error: 100&quot; What is the most practical number of hidden units in your opinion? (Consider the tradeoff between the reduction in MSE loss with the increase in hidden units and the increase in training time and model complexity, and number of parameters to be learnt) As one might suspect network with 100 hidden units gives the minimum loss, but it is only marginally better than a network with 30 hidden units. So, one might consider using 30 hidden units as it gives a reasonable tradeoff between model complexity and the cross validation loss. Also note that if we decrease the model complexity too much, the loss on cross-validation increases significantly. Closing remarks: Finally, in practice, when you arrive at the optimal model parameters after cross-validation, you train a new model, using the entire training set, and use the initial held-out test set (20%) to evaluate the model’s performance on new data. # Output of tune_results &lt;- tune_num_units(train_split = train_split, num_units = num_units) # ## CV Fold --&gt; 1/5 ## Model: &quot;sequential_6&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_12 (Dense) (None, 1) 16 ## ________________________________________________________________________________ ## dense_13 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 18 ## Trainable params: 18 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 34.77391 ## CV Fold --&gt; 2/5 ## Model: &quot;sequential_7&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_14 (Dense) (None, 1) 16 ## ________________________________________________________________________________ ## dense_15 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 18 ## Trainable params: 18 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 29.33935 ## CV Fold --&gt; 3/5 ## Model: &quot;sequential_8&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_16 (Dense) (None, 1) 16 ## ________________________________________________________________________________ ## dense_17 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 18 ## Trainable params: 18 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 28.22049 ## CV Fold --&gt; 4/5 ## Model: &quot;sequential_9&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_18 (Dense) (None, 1) 16 ## ________________________________________________________________________________ ## dense_19 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 18 ## Trainable params: 18 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 28.60611 ## CV Fold --&gt; 5/5 ## Model: &quot;sequential_10&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_20 (Dense) (None, 1) 16 ## ________________________________________________________________________________ ## dense_21 (Dense) (None, 1) 2 ## ================================================================================ ## Total params: 18 ## Trainable params: 18 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 34.44407 ## CV Fold --&gt; 1/5 ## Model: &quot;sequential_11&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_22 (Dense) (None, 3) 48 ## ________________________________________________________________________________ ## dense_23 (Dense) (None, 1) 4 ## ================================================================================ ## Total params: 52 ## Trainable params: 52 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.88199 ## CV Fold --&gt; 2/5 ## Model: &quot;sequential_12&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_24 (Dense) (None, 3) 48 ## ________________________________________________________________________________ ## dense_25 (Dense) (None, 1) 4 ## ================================================================================ ## Total params: 52 ## Trainable params: 52 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.98859 ## CV Fold --&gt; 3/5 ## Model: &quot;sequential_13&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_26 (Dense) (None, 3) 48 ## ________________________________________________________________________________ ## dense_27 (Dense) (None, 1) 4 ## ================================================================================ ## Total params: 52 ## Trainable params: 52 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.33782 ## CV Fold --&gt; 4/5 ## Model: &quot;sequential_14&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_28 (Dense) (None, 3) 48 ## ________________________________________________________________________________ ## dense_29 (Dense) (None, 1) 4 ## ================================================================================ ## Total params: 52 ## Trainable params: 52 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.62691 ## CV Fold --&gt; 5/5 ## Model: &quot;sequential_15&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_30 (Dense) (None, 3) 48 ## ________________________________________________________________________________ ## dense_31 (Dense) (None, 1) 4 ## ================================================================================ ## Total params: 52 ## Trainable params: 52 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.62623 ## CV Fold --&gt; 1/5 ## Model: &quot;sequential_16&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_32 (Dense) (None, 10) 160 ## ________________________________________________________________________________ ## dense_33 (Dense) (None, 1) 11 ## ================================================================================ ## Total params: 171 ## Trainable params: 171 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.59519 ## CV Fold --&gt; 2/5 ## Model: &quot;sequential_17&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_34 (Dense) (None, 10) 160 ## ________________________________________________________________________________ ## dense_35 (Dense) (None, 1) 11 ## ================================================================================ ## Total params: 171 ## Trainable params: 171 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.03098 ## CV Fold --&gt; 3/5 ## Model: &quot;sequential_18&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_36 (Dense) (None, 10) 160 ## ________________________________________________________________________________ ## dense_37 (Dense) (None, 1) 11 ## ================================================================================ ## Total params: 171 ## Trainable params: 171 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.29626 ## CV Fold --&gt; 4/5 ## Model: &quot;sequential_19&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_38 (Dense) (None, 10) 160 ## ________________________________________________________________________________ ## dense_39 (Dense) (None, 1) 11 ## ================================================================================ ## Total params: 171 ## Trainable params: 171 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.55116 ## CV Fold --&gt; 5/5 ## Model: &quot;sequential_20&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_40 (Dense) (None, 10) 160 ## ________________________________________________________________________________ ## dense_41 (Dense) (None, 1) 11 ## ================================================================================ ## Total params: 171 ## Trainable params: 171 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.43872 ## CV Fold --&gt; 1/5 ## Model: &quot;sequential_21&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_42 (Dense) (None, 30) 480 ## ________________________________________________________________________________ ## dense_43 (Dense) (None, 1) 31 ## ================================================================================ ## Total params: 511 ## Trainable params: 511 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 23.64743 ## CV Fold --&gt; 2/5 ## Model: &quot;sequential_22&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_44 (Dense) (None, 30) 480 ## ________________________________________________________________________________ ## dense_45 (Dense) (None, 1) 31 ## ================================================================================ ## Total params: 511 ## Trainable params: 511 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.02392 ## CV Fold --&gt; 3/5 ## Model: &quot;sequential_23&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_46 (Dense) (None, 30) 480 ## ________________________________________________________________________________ ## dense_47 (Dense) (None, 1) 31 ## ================================================================================ ## Total params: 511 ## Trainable params: 511 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 25.02493 ## CV Fold --&gt; 4/5 ## Model: &quot;sequential_24&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_48 (Dense) (None, 30) 480 ## ________________________________________________________________________________ ## dense_49 (Dense) (None, 1) 31 ## ================================================================================ ## Total params: 511 ## Trainable params: 511 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.03987 ## CV Fold --&gt; 5/5 ## Model: &quot;sequential_25&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_50 (Dense) (None, 30) 480 ## ________________________________________________________________________________ ## dense_51 (Dense) (None, 1) 31 ## ================================================================================ ## Total params: 511 ## Trainable params: 511 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.74669 ## CV Fold --&gt; 1/5 ## Model: &quot;sequential_26&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_52 (Dense) (None, 100) 1600 ## ________________________________________________________________________________ ## dense_53 (Dense) (None, 1) 101 ## ================================================================================ ## Total params: 1,701 ## Trainable params: 1,701 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 23.98048 ## CV Fold --&gt; 2/5 ## Model: &quot;sequential_27&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_54 (Dense) (None, 100) 1600 ## ________________________________________________________________________________ ## dense_55 (Dense) (None, 1) 101 ## ================================================================================ ## Total params: 1,701 ## Trainable params: 1,701 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 23.41279 ## CV Fold --&gt; 3/5 ## Model: &quot;sequential_28&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_56 (Dense) (None, 100) 1600 ## ________________________________________________________________________________ ## dense_57 (Dense) (None, 1) 101 ## ================================================================================ ## Total params: 1,701 ## Trainable params: 1,701 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 24.43465 ## CV Fold --&gt; 4/5 ## Model: &quot;sequential_29&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_58 (Dense) (None, 100) 1600 ## ________________________________________________________________________________ ## dense_59 (Dense) (None, 1) 101 ## ================================================================================ ## Total params: 1,701 ## Trainable params: 1,701 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 23.77185 ## CV Fold --&gt; 5/5 ## Model: &quot;sequential_30&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_60 (Dense) (None, 100) 1600 ## ________________________________________________________________________________ ## dense_61 (Dense) (None, 1) 101 ## ================================================================================ ## Total params: 1,701 ## Trainable params: 1,701 ## Non-trainable params: 0 ## ________________________________________________________________________________ ## Evaluation error on the held out fold: 23.87822 ## [1] 31.07678 25.69231 24.78246 24.29657 23.89560 "],["ch-12.html", "Chapter 12 Supervised Deep Learning I 12.1 Introduction 12.2 Tutorial 12.3 Exercise", " Chapter 12 Supervised Deep Learning I 12.1 Introduction In this tutorial, we’ll use convolutional neural networks for image classification. The dataset we’ll work with comes from a 2013 Kaggle competition, which you can read about here. Kaggle is an online community for data scientists and machine learning engineers, where they can explore datasets, build models and enter contests to attempt solving data science hurdles. The dataset contains 25,000 images of dogs and cats. We’ll use just a subset of these - 6,000 images to be precise. These we’ll break down into a training set of 4,000 images, a validation set of 1000 images, and a testing set of 1000 images. This tutorial is based on Chapter 5 of Deep learning with Python by F. Chollet - highly recommended reading! 12.1.1 Learning objectives Use Keras to implement a deep convolutional neural network for image classification Avoid overfitting using weight regularization, dropout and data augmentation Visualize what convolutional neural networks learn via the visualization of intermediate activations Important points from the lecture - Convolutional neural networks are a powerful kind of machine learning for image classification - They comprise a series of convolution and max pooling layers, the output of which is then fed into one or more dense layers - The lower convolution layers learn local patterns in small windows, like edges and textures - A spatial hierarchy of local patterns are then learned by the higher convolution layers - The final dense layer then learns global patterns in this spatial hierarchy of lower-level patterns - Convolutional neural networks are partially “white box”, because you can visualize much of what the network learns, for example by visualizing intermediate activations. 12.2 Tutorial 12.2.1 Building Blocks of CNNs You probably already know this or at least guessed it but CNN stands for convolutional neural network. CNNs built for classification tasks typically make use the following types of layers (displayed in Figure 12.1: Convolutional Layers: Layers implementing the actual convolution. Their outputs are feature maps which are then passed through an activation function in order to introduce non-linearities into the system. Convolutional layers can be seen as extracting features that are passed on deeper into the model thus enabling the model to learn higher-level features that make the classification task easier. Pooling Layers: Downsampling or pooling layers concentrate the information so that deeper layers focus more on abstract/high-level patterns. A common choice is max-pooling, where only the maximum value occurring in a certain region is propagated to the output. Dense Layers: A dense or fully-connected layer connects every node in the input to every node in the output. This is the type of layer you already used in the previous tutorial. If the input dimension is large, the amount of learnable parameters introduced by using a dense layer can quickly explode. Hence, dense layers are usually added on deeper levels of the model, where the pooling operations have already reduced the dimensionality of the data. Typically, the dense layers are added last in a classification model, performing the actual classification on the features extracted by the convolutional layers. Figure 12.1: Building blocks of a CNN. Figures taken from Chapter 5 of Deep learning with Python. Left: Features extraction from a CNN (convolutional neural network). Right: Instance classification. You can think of a convolution as sliding a window, also called kernel or filter, over each pixel of an image and computing a dot product between the filter’s values and the image’s values that the window (or filter) is covering. This operation produces one output value for every location on the image over which we slide the filter. In Figure 12.2, in blue is the input image and in shaded blue the 3-by-3 filter that we are convolving the input image with and green is the output image. After the convolutional layer usually a max-pooling layer is applied which downsamples the feature map. Through this only certain aspects of information in our original image are extracted. Figure 12.2: Animation on the left displays the sliding filter across the input feature map (source). The image on the right shows a pooling layer where the highest value of a quadrant is taken as value in a reduced feature map (source). We can stack several convolutional-pooling layers. The final pooling layer output is given to a feed forward neural network (i.e dense layers) which is in charge to make the prediction. Let’s dive into our example for a practical application of this. # import libraries library(reticulate) # use_condaenv() library(keras) library(tensorflow) library(tidyverse) library(patchwork) We’ll start by creating some useful functions which will be used later in the tutorial. # a function that specifies the size of the plot fig_size &lt;- function(width, height){ options(repr.plot.width = width, repr.plot.height = height) } # a function that visualize the corresponding RGB image derived from a tensor plotRGB = function(img_tensor){ #take dimension of image width = dim(img_tensor)[1] height = dim(img_tensor)[2] #take color channels red = as.numeric(img_tensor[,,1]) green = as.numeric(img_tensor[,,2]) blue = as.numeric(img_tensor[,,3]) #create dataframe img = data.frame(x = rep(1:height,each = width), y = rep(height:1,times = width) , r = red , g = green, b = blue) #plot RGB image img %&gt;% ggplot(aes(x=x, y=y, fill=rgb(r,g,b))) + geom_raster()+ scale_y_continuous()+ scale_x_continuous()+ scale_fill_identity()+ theme_void() } # a function that plot the output of the given filter activation plot_filter = function(filter_activation){ #create dataframe height = dim(filter_activation)[1] width = dim(filter_activation)[2] filter = data.frame(x = rep(1:width,each = height), y = rep(height:1,times = width) , fill = as.numeric(filter_activation)) #plot filter p = filter %&gt;% ggplot(aes(x = x,y = y))+ geom_raster(aes(fill = fill),show.legend = FALSE)+ scale_y_continuous()+ scale_x_continuous()+ scale_fill_viridis_c()+ theme_void() return(p) } # data directory base_dir = &#39;./data/SDL_I/dogs_v_cats_small&#39; # train data directory train_dir = file.path(base_dir,&#39;train&#39;) #validation data directory validation_dir = file.path(base_dir,&#39;validation&#39;) #test data directory test_dir = file.path(base_dir,&#39;test&#39;) To get an overview of our data, we’ll start by printing the number cat vs dog images in each given data set (training, validation &amp; testing). #train set cat(&#39;total training cat images:&#39;, length(list.files(file.path(train_dir,&#39;cats&#39;))),&quot;\\n&quot;, &#39;total training dog images:&#39;, length(list.files(file.path(train_dir,&#39;dogs&#39;))),&quot;\\n&quot;) ## total training cat images: 2000 ## total training dog images: 2000 #validation set cat(&#39;total validation cat images:&#39;, length(list.files(file.path(validation_dir,&#39;cats&#39;))),&quot;\\n&quot;, &#39;total validation dog images:&#39;, length(list.files(file.path(validation_dir,&#39;dogs&#39;))),&quot;\\n&quot;) ## total validation cat images: 500 ## total validation dog images: 500 #test set cat(&#39;total test cat images:&#39;, length(list.files(file.path(test_dir,&#39;cats&#39;))),&quot;\\n&quot;, &#39;total test dog images:&#39;, length(list.files(file.path(test_dir,&#39;dogs&#39;))),&quot;\\n&quot;) ## total test cat images: 500 ## total test dog images: 500 Note that this data set is class balanced, meaning that accuracy is a useful measure of model performance. A sample image from each class: Keras has built-in functionality for feeding batches of training and validation data to the optimization function. This is useful if, for example, you want to make random changes to your training and validation data on the fly, in order to avoid overfitting. You’ll get to read more about this below. This functionality comes in the form of the ImageDataGenerator function, which you can read about here. Let’s use ImageDataGenerator to generate batches of training and validation data. # Specify image size IMAGE_HEIGHT = 150 IMAGE_WIDTH = 150 IMAGE_CHANNELS = 3 IMAGE_SIZE = c(IMAGE_HEIGHT,IMAGE_WIDTH) # Specify batch size for each set N_TRAIN = length(list.files(train_dir,recursive = T)) N_VALIDATION = length(list.files(validation_dir,recursive = T)) N_TEST = length(list.files(test_dir,recursive = T)) BATCH_SIZE_TRAIN = 32 BATCH_SIZE_VALIDATION = 50 BATCH_SIZE_TEST = 50 #Rescale images by 1/255 to get numbers between 0 and 1 train_datagen = image_data_generator(rescale = 1/255) test_datagen = image_data_generator(rescale= 1/255) #Create a generator for the training data train_generator = flow_images_from_directory( directory = train_dir, generator = train_datagen, target_size = IMAGE_SIZE, batch_size = BATCH_SIZE_TRAIN, class_mode = &quot;binary&quot; ) #Create a generator for the validation data validation_generator = flow_images_from_directory( directory = validation_dir, generator = test_datagen, target_size = IMAGE_SIZE, batch_size = BATCH_SIZE_TEST, class_mode = &quot;binary&quot; ) #Create a generator for the test data test_generator = flow_images_from_directory( directory = test_dir, generator = test_datagen, target_size = IMAGE_SIZE, batch_size = BATCH_SIZE_TEST, class_mode = &quot;binary&quot; ) #Let&#39;s have a look at one batch of data from train_generator: batch = iter_next(train_generator) cat(&#39;data batch shape:&#39;, dim(batch[[1]]),&quot;\\n&quot;) cat(&#39;labels batch shape:&#39;, dim(batch[[2]]),&quot;\\n&quot;) Notice the dimensions of the tensor data_batch: - The first dimension is the number of images in the batch. - The second and third dimensions are the dimensions of each image. - The fourth dimension is 3 because these are color images - one for each RGB (red-blue-green) channel. 12.2.2 Build the model Let’s write a function to build a deep convolutional neural network with the following specifications: 4 convolution layers with 32, 64, 128, and 128 filters per layer. Each with a ReLU activation function. A max pooling layer after each convolution layer that halves each feature map. A dense layer with 512 neurons, each with a ReLU activation function. A single output neuron with a sigmoid activation function. The function takes two input parameters: (1) the L2 regularization rate and (2) the dropout rate. We’ll look into these in detail later. build_model = function(L2_rate = 0, drop_rate = 0){ #Name the model model.name = paste(&#39;convnet_L2_&#39;,L2_rate,&#39;_dropout_&#39;,drop_rate,sep=&quot;&quot;) #sequential model model = keras_model_sequential(name = model.name) model = model %&gt;% #Add the first convolution layer, with 32 3x3 filters and ReLU activation #Note that you only need to provide the input dimensions for the 1st layer. #Note also that we don&#39;t add regularization to this layer. The reason is it has fewer parameters #than subsequent layers and is therefore less likely to be a source of overfitting. However, you #could add regularization here too, if you&#39;d like. layer_conv_2d(filter = 32 , kernel_size = c(3,3),activation = &#39;relu&#39;,input_shape = c(IMAGE_SIZE,IMAGE_CHANNELS)) %&gt;% #Add a max pooling layer with 2x2 windows. This will halve the dimension of data. layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% #Add a convolution layer with 64 3x3 filters and ReLU activation layer_conv_2d(filters = 64,kernel_size = c(3,3),activation = &#39;relu&#39;) %&gt;% #Add a max pooling layer with 2x2 windows. This will halve the dimension of data. layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% #Add a convolution layer with 128 3x3 filters and ReLU activation #add L2 regularization layer_conv_2d(filter = 128, kernel_size = c(3,3),activation = &#39;relu&#39;,kernel_regularizer = regularizer_l2(l = L2_rate)) %&gt;% #Add a max pooling layer with 2x2 windows. This will halve the dimension of data. layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% #add a dropout layer layer_dropout(rate = drop_rate)%&gt;% #Add a convolution layer with 128 3x3 filters and ReLU activation #add L2 regularization layer_conv_2d(filter = 128, kernel_size = c(3,3),activation = &#39;relu&#39;,kernel_regularizer = regularizer_l2(l = L2_rate)) %&gt;% #Add a max pooling layer with 2x2 windows. This will halve the dimension of data. layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% #Squash the output of the final max pooling layer into a 1D vector layer_flatten()%&gt;% #add a dropout layer here layer_dropout(rate = drop_rate)%&gt;% #Use this 1D vector as input to a single dense layer with 512 neurons #add L2 regularization layer_dense(units = 512,activation = &#39;relu&#39;,kernel_regularizer = regularizer_l2(l = L2_rate)) %&gt;% #add a dropout layer layer_dropout(rate = drop_rate)%&gt;% #Finally, add a single neuron with a sigmoid activation function layer_dense(units = 1,activation = &#39;sigmoid&#39;) #Print out a model summary summary(model) return (model) } 12.2.3 Compile and train the model Now let’s write a function to compile and train our model, using the data generators created above to feed the images into the training function. We’ll measure loss using binary cross-entropy and calculate accuracy during training as a measure of model performance. Remember, this is a useful measure for this dataset because there are an equal number of dog and cat photos. There is however something new in this function: namely a callback function. As used here, this callback function monitors validation loss during training. If it does not improve over specified number of epochs (usually 5), training is terminated. This way, if training has stopped improving, you don’t have to wait for the specified maximum number of epochs for training to stop. We will use this functionality later on when we train our final model. You can read more about this callback function here. compile_and_train = function(model, learning_rate, max_epochs, early_stopping = F){ #Compile the model model %&gt;% compile(loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_adam(lr = learning_rate), metrics = list(&#39;acc&#39;) ) #create callback object callbacks_cnn = list() #If early stopping is requested if (early_stopping){ #create callback that reduces the learning rate when no improvement is observed after 3 epochs callbacks_cnn = append(callbacks_cnn,callback_reduce_lr_on_plateau(monitor = &quot;val_loss&quot;,patience = 3 ,factor = 0.1)) #create callback for early stopping callbacks_cnn = append(callbacks_cnn,callback_early_stopping(monitor=&#39;val_loss&#39;,patience = 5,mode = &#39;min&#39;)) #create folder to save the models dir.create(&quot;./saved_models&quot;) #create callbacks to save the best model when the training process terminates callbacks_cnn = append(callbacks_cnn, callback_model_checkpoint(file.path(&quot;./&quot;,&#39;saved_models&#39;,paste(model$name,&quot;.h5&quot;,sep=&quot;&quot;)), monitor=&#39;val_loss&#39;,save_best_only = T,mode = &#39;min&#39;)) } #Train the model, using the data generators created above history = model %&gt;% fit_generator( train_generator, steps_per_epoch = as.integer(N_TRAIN/BATCH_SIZE_TRAIN), epochs = max_epochs, validation_data = validation_generator, validation_steps = as.integer(N_VALIDATION/BATCH_SIZE_VALIDATION), callbacks = callbacks_cnn ) return (history) } Now let’s compile and train our model, using a learning rate of 0.001, a maximum number of 20 epochs, but for now without early stopping, regularization, or dropout. This will take some time. Go make a cup of tea, coffee or even hot chocolate. model_baseline = build_model(0, 0) history_baseline = compile_and_train(model_baseline, 1e-3, 20, F) Checkpoint Take some time to digest this model summary. Make sure you understand and can answer the following questions: A: Why is the dimensionality of the output of the first convolution layer (148, 148, 32) rather than (150, 150, 32)? B: Why does the first convolution layer have 896 free parameters? C: Why don’t the max-pooling layers have any free parameters? D: Why does the last layer contain only a single neuron with a sigmoid activation function? Solutions A: Because the size of the filter is 3 x 3 B: There are 3x3 = 9 parameters for each filter, times 3 channels (RGB), plus one bias term per filter. Thus: 32 x (9x3+1) C: There are no parameters associated with these layers. The max operation is hard-coded. D: This is a classification problem. Let’s have a look at loss and accuracy during training. Both are measured on the training data and the validation data. #load history of trained models history = readRDS(&#39;./data/SDL_I/saved_models/models_history.rds&#39;) fig_size(8,8) plot(history$baseline) + theme_grey(base_size = 20) What do you observe? Notice that toward the end of model training, training accuracy approaches 1, whereas validation accuracy plateaus at a lower value, around 0.75. Similarly, training loss approaches 0, yet after around 8 epochs, validation loss begins to increase. 12.2.4 Reduce Overfitting These are classic symptoms of overfitting. In the lectures, we’ve learned about some techniques to avoid overfitting. Specifically, early stopping, regularization, and dropout. In the function compile_and_train_model, you can turn on early stopping by setting the parameter early_stopping to 1. This will force early stopping if validation loss does not decrease for 5 epochs. You can also add regularization and dropout using the first two input parameters. Below, we’ll also use another approach that is particularly amenable to image classification, namely data augmentation, and to which dropout can be applied to further improve model performance (this will be our final trained model). 12.2.4.1 Weight Regularization Given some training data and a network architecture, multiple sets of weight values (multiple models) could explain the data. Simpler models are less likely to overfit than complex ones. A simple model in this context is a model with fewer parameters. Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called weight regularization, and it’s done by adding to the loss function of the network a cost associated with having large weights. For this course, we will focus on L2 regularization. L2 regularization: The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights). L2 regularization is also called weight decay in the context of neural networks. The L2 rate specifies how much we desire to shrink the weights. A value of 0 indicates no regularization while a large rate value (heavy regularization) can cause the weights to take a value very small (close to 0 which can lead to underfitting). Therefore we should choose the L2 rate very carefully. A quick note here: weight regularization can be applied to every model we have seen in this course so far (it is not only related to neural networks as dropout). Let’s now create a model with an L2 rate that equals 0.005. model_L2 = build_model(L2_rate = 0.005) history_L2_reg = compile_and_train(model_L2, 1e-3, 20, F) ## Model: &quot;convnet_L2_0.005_dropout_0&quot; ## ______________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================== ## conv2d (Conv2D) (None, 148, 148, 32) 896 ## ______________________________________________________________________________________________________________ ## max_pooling2d (MaxPooling2D) (None, 74, 74, 32) 0 ## ______________________________________________________________________________________________________________ ## conv2d_1 (Conv2D) (None, 72, 72, 64) 18496 ## ______________________________________________________________________________________________________________ ## max_pooling2d_1 (MaxPooling2D) (None, 36, 36, 64) 0 ## ______________________________________________________________________________________________________________ ## conv2d_2 (Conv2D) (None, 34, 34, 128) 73856 ## ______________________________________________________________________________________________________________ ## max_pooling2d_2 (MaxPooling2D) (None, 17, 17, 128) 0 ## ______________________________________________________________________________________________________________ ## dropout (Dropout) (None, 17, 17, 128) 0 ## ______________________________________________________________________________________________________________ ## conv2d_3 (Conv2D) (None, 15, 15, 128) 147584 ## ______________________________________________________________________________________________________________ ## max_pooling2d_3 (MaxPooling2D) (None, 7, 7, 128) 0 ## ______________________________________________________________________________________________________________ ## flatten (Flatten) (None, 6272) 0 ## ______________________________________________________________________________________________________________ ## dropout_1 (Dropout) (None, 6272) 0 ## ______________________________________________________________________________________________________________ ## dense (Dense) (None, 512) 3211776 ## ______________________________________________________________________________________________________________ ## dropout_2 (Dropout) (None, 512) 0 ## ______________________________________________________________________________________________________________ ## dense_1 (Dense) (None, 1) 513 ## ============================================================================================================== ## Total params: 3,453,121 ## Trainable params: 3,453,121 ## Non-trainable params: 0 ## ______________________________________________________________________________________________________________ Let’s have again a look at loss and accuracy during training, both measured on the training data and the validation data. fig_size(8,8) plot(history$L2_reg) + theme_grey(base_size = 20) It seems that overfitting has been prevented with L2 regularization since validation loss keeps decreasing as long as training loss also decreases. 12.2.4.2 Dropout Dropout is a simple and effective way to reduce overfitting. The idea behind this method is to add noise to the output of certain layers during training, thereby reducing the chance they memorize spurious (false or fake) correlations in the training data. In practice, dropout works by simply changing a certain fraction of outputs in the specified layers to zero, thus ‘dropping them out’. This fraction is specified by the dropout rate, which is typically set between 0.2 and 0.5. Exactly which neurons get dropped out randomly changes for each example. To compensate for the decreased number of active neurons in layers with dropout, the layer’s outputs are scaled by the inverse of the (1-dropout rate). This is shown graphically in Figure 12.3 with the dropout rate being 0.5. Figure 12.3: Visualization of dropout using a 0.5 dropout rate. Dropout applied to an activation matrix at training time, with rescaling happening during training. At test time, the activation matrix is unchanged. model_dropout = build_model(0, 0.5) history_dropout = compile_and_train(model_dropout, 1e-3, 20, F) fig_size(8,8) plot(history$drop) + theme_grey(base_size = 20) What do you observe? Compare this to the figures above using L2 regularization. 12.2.4.3 Data augmentation Overfitting commonly occurs on small training sets. For image data, that we are considering here, one approach to avoid such overfitting is data augmentation. In this approach, you generate new images via a random transformation of the training images. You thus augment your training set by generating new, believable looking images. By doing so, your model will never see the same image twice during training. Keras makes this easy with the class ImageDataGenerator. Just by tweaking a few lines of our train_datagen function, we can augment our dataset during training. Let’s take a look at this. # Image generatos for data augmentation train_datagen = image_data_generator( rescale = 1./255, rotation_range = 15, width_shift_range = 0.1, height_shift_range = 0.1, shear_range = 0.1, zoom_range = 0.2, horizontal_flip = T, fill_mode = &#39;nearest&#39; ) # Create a generator for the training data train_generator = flow_images_from_directory( directory = train_dir, generator = train_datagen, target_size = IMAGE_SIZE, batch_size = BATCH_SIZE_TRAIN, class_mode = &quot;binary&quot; ) Let’s have a closer look at the parameters of this function. rescale: You’ve seen this in our first use of ‘ImageDataGenerator’. This simply rescales the RGB values to be between 0 and 1. rotation_range: is the range in degrees of permitted image rotations. width_shift_range: shifts the image to the left or to the right by the fraction of the total width specified (i.e., 0.1 in this example). height_shift_range: shifts the image up or down by the fraction of the total height specified (i.e., 0.1 in this example). shear_range: distorts the image along an axis, in effect approximating a change in the position of the photographer. zoom_range: zooms in and out of the image. horizontal_flip: randomly flips the image along the horizontal axis. fill_mode: This defines how pixels are to be filled, for example when the image is shifted such that some pixels become empty. You can read more details about this function here. If you’d like to see more examples of how the parameters of this function affect images, see this blog post. To get your own feel for what this function does, and to enjoy some funny cat pictures, execute the code cell below with different cats by just changing the index to the list fnames on line 5. #Get the file names of the test images for cats fnames = list.files(file.path(train_dir,&#39;cats&#39;),full.names = T) #Choose a cat cat = 1000 img_path= fnames[cat] #Read, resize, and rescale the image img = image_load(img_path, target_size = IMAGE_SIZE) img_tensor = image_to_array(img) x = array_reshape(img_tensor,dim = c(-1,IMAGE_SIZE,IMAGE_CHANNELS)) #Visualize the image fig_size(10,6) i = 6 plots = list() for(j in 1:i){ batch = iter_next(flow_images_from_data(generator = train_datagen,x = x,batch_size = 1)) plots[[j]]=plotRGB(batch[1,,,]) } p = plots[[1]] for (j in 1:(i-1)){ p = p + plots[[j+1]] } p + plot_layout(nrow = 2) Get the idea? Data augmentation stretches, moves, rotates, shears, and inverts your raw data images to produce new images, with the goal of never showing your model the same image twice during training. Now let’s put our new train_datagen function to work. model_data_augment = build_model(0, 0) history_data_augment = compile_and_train(model_data_augment, 1e-3, 20, F) ## Model: &quot;convnet_L2_0_dropout_0&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## conv2d_12 (Conv2D) (None, 148, 148, 32) 896 ## ________________________________________________________________________________ ## max_pooling2d_12 (MaxPooling2D) (None, 74, 74, 32) 0 ## ________________________________________________________________________________ ## conv2d_13 (Conv2D) (None, 72, 72, 64) 18496 ## ________________________________________________________________________________ ## max_pooling2d_13 (MaxPooling2D) (None, 36, 36, 64) 0 ## ________________________________________________________________________________ ## conv2d_14 (Conv2D) (None, 34, 34, 128) 73856 ## ________________________________________________________________________________ ## max_pooling2d_14 (MaxPooling2D) (None, 17, 17, 128) 0 ## ________________________________________________________________________________ ## dropout_9 (Dropout) (None, 17, 17, 128) 0 ## ________________________________________________________________________________ ## conv2d_15 (Conv2D) (None, 15, 15, 128) 147584 ## ________________________________________________________________________________ ## max_pooling2d_15 (MaxPooling2D) (None, 7, 7, 128) 0 ## ________________________________________________________________________________ ## flatten_3 (Flatten) (None, 6272) 0 ## ________________________________________________________________________________ ## dropout_10 (Dropout) (None, 6272) 0 ## ________________________________________________________________________________ ## dense_6 (Dense) (None, 512) 3211776 ## ________________________________________________________________________________ ## dropout_11 (Dropout) (None, 512) 0 ## ________________________________________________________________________________ ## dense_7 (Dense) (None, 1) 513 ## ================================================================================ ## Total params: 3,453,121 ## Trainable params: 3,453,121 ## Non-trainable params: 0 ## ________________________________________________________________________________ Let’s again plot the learning curves fig_size(8,8) plot(history$augment) + theme_grey(base_size = 20) As you can see clearly above, overfitting is avoided by using the data augmentation technique. This was a lot of information and different techniques of how to avoid overfitting. So, to avoid complete confusion and provide a visual comparison, we will summarize all the different models below. This way you can see how the different techniques for reducing overfitting can change the behaviour of training and validation metrics. p_base = plot(history$baseline) + labs(title = &#39;Baseline&#39;) +theme_gray(base_size = 15) + theme(plot.title = element_text(size=17,hjust = 0.5)) p_L2 = plot(history$L2_reg) + labs(title = &#39;L2 Regularization&#39;) +theme_gray(base_size = 15) + theme(plot.title = element_text(size=17,hjust = 0.5)) p_dropout = plot(history$drop) + labs(title = &#39;Dropout&#39;) +theme_gray(base_size = 15) + theme(plot.title = element_text(size=17,hjust = 0.5)) p_data_augment = plot(history$augment) + labs(title = &#39;Data Augmentation&#39;) +theme_gray(base_size = 15) + theme(plot.title = element_text(size=17,hjust = 0.5)) p_base + p_L2 + p_dropout + p_data_augment + plot_layout(nrow = 2) Now, it’s time to train our final model so it can be used in the next section. We will be using data augmentation and dropout. Further, we’ll use early stopping and a maximum of 100 epochs. The best model will be saved automatically with the callback callback_model_checkpoint model_final = build_model(0, 0.5) history_final = compile_and_train(model_final, 1e-3, 100, T) ## Model: &quot;convnet_L2_0_dropout_0.5&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## conv2d_16 (Conv2D) (None, 148, 148, 32) 896 ## ________________________________________________________________________________ ## max_pooling2d_16 (MaxPooling2D) (None, 74, 74, 32) 0 ## ________________________________________________________________________________ ## conv2d_17 (Conv2D) (None, 72, 72, 64) 18496 ## ________________________________________________________________________________ ## max_pooling2d_17 (MaxPooling2D) (None, 36, 36, 64) 0 ## ________________________________________________________________________________ ## conv2d_18 (Conv2D) (None, 34, 34, 128) 73856 ## ________________________________________________________________________________ ## max_pooling2d_18 (MaxPooling2D) (None, 17, 17, 128) 0 ## ________________________________________________________________________________ ## dropout_12 (Dropout) (None, 17, 17, 128) 0 ## ________________________________________________________________________________ ## conv2d_19 (Conv2D) (None, 15, 15, 128) 147584 ## ________________________________________________________________________________ ## max_pooling2d_19 (MaxPooling2D) (None, 7, 7, 128) 0 ## ________________________________________________________________________________ ## flatten_4 (Flatten) (None, 6272) 0 ## ________________________________________________________________________________ ## dropout_13 (Dropout) (None, 6272) 0 ## ________________________________________________________________________________ ## dense_8 (Dense) (None, 512) 3211776 ## ________________________________________________________________________________ ## dropout_14 (Dropout) (None, 512) 0 ## ________________________________________________________________________________ ## dense_9 (Dense) (None, 1) 513 ## ================================================================================ ## Total params: 3,453,121 ## Trainable params: 3,453,121 ## Non-trainable params: 0 ## ________________________________________________________________________________ model = load_model_hdf5(&#39;./data/SDL_I/saved_models/final_model.h5&#39;) Let’s also evaluate the perfomance on the test set. validation_eval = evaluate_generator(model, validation_generator,steps = as.integer(N_VALIDATION/BATCH_SIZE_VALIDATION)) test_eval = evaluate_generator(model, test_generator,steps = as.integer(N_TEST/BATCH_SIZE_TEST)) # Evaluate model on validation and test data cat(&quot;Validation accuracy: &quot;,validation_eval$acc,&quot;\\n&quot;) cat(&quot;Test accuracy: &quot;,test_eval$acc,&quot;\\n&quot;) ## Validation accuracy: 0.846 ## Test accuracy: 0.849 Great, we have a well performing model. Now let’s interpret what the CNN learns. 12.2.5 Visualizing a CNN There are multiple ways to visualize what convolutional neural networks learn, here are three examples: Visualizing intermediate activations - this shows you how successive convolutional layers transform their input. Visualizing convolutional filters - this shows you what visual pattern or concept a filter responds to. Visualizing heatmaps of class activation in an image - this shows you which parts of an image were identified as belonging to a particular class. The activation of a layer is the output of the activation function for that layer, given some input. This shows how an input is decomposed by the different filters learned by a convolutional neural network. Let’s visualize the intermediate activations of the last convolutional neural network you trained - the one with data augmentation and dropout. We’ll start by loading the model and, as a reminder, printing its summary. Now let’s grab a random image of a cat from the test set, get it in the right format, and visualize it. # Get the file names of the test images for cats fnames = list.files(file.path(test_dir,&#39;dogs&#39;),full.names = T) # Choose a cat cat = 50 img_path= fnames[cat] # Read, resize, and rescale the image img = image_load(img_path, target_size = IMAGE_SIZE) img_tensor = image_to_array(img) img_tensor = img_tensor * 1/255 # Visualize the image fig_size(7,7) plotRGB(img_tensor) To visualize the intermediate activations, you’ll create a Keras model that takes an image as input and outputs the activations of all the convolution layers. This is simply a model that shows you how the neurons in each layer of your trained network respond to a particular image. #Extract the outputs of the top eight convolution layers n_layers = 8 layers_outputs = list() for ( i in 1:n_layers){ layers_outputs[[i]] = get_layer(model,index = i)$output } #Create a model that returns the activation of the top eight convolution layers, #given an input image activation_model = keras_model(inputs = model$input, outputs = layers_outputs) #Get the activations of the top eight convolution layers, given the cat #image selected above activations = predict(activation_model,array_reshape(img_tensor,dim = c(-1,IMAGE_SIZE,IMAGE_CHANNELS))) #Get the first layer activations first_layer_activation = activations[[1]] #Get filter i from the first layer i = 3 fig_size(7,7) plot_filter(first_layer_activation[1,,,i]) What do you observe? It appears this 3rd filter encodes an edge detector. How about other filters? Rerun the code cell for different filters. You can do this by changing the value of i, currently i=3, on line 5 of the code cell above to any number between 1 and 32. See anything else that interests you? Let’s go ahead and visualize the intermediate activations of all of the filters of all of the convolution layers. Go and choose pic 50 from dogs. What do you observe? Can you see how the filter works as a leash detector? This makes a lot of sense considering that dogs are seldom without a leash and cats always! That can be a very powerful feature for our classifier. #Get the layer names layer_names = list() for ( i in 1:n_layers){ layer_names[[i]] = get_layer(model,index = i)$name } #Set the number of images per row images_per_row = 16 plots = list() for (i in 1:n_layers){ #Number of features in this feature map n_features = dim(activations[[i]])[4] #The feature map has the shape (I, size, size, n_features) size = dim(activations[[i]])[2] #Number of columns for display n_rows = as.integer(n_features/images_per_row) display_grid = matrix(NA,nrow = size * n_rows,ncol = images_per_row * size) for(row in 1:n_rows){ for(col in 1:images_per_row){ #Get the activation response filter_image = activations[[i]][1,,,(row-1) * images_per_row + col] #Do some post-processing to make the image easier to see filter_image = filter_image - mean(filter_image) filter_image = filter_image*64 filter_image = filter_image + 128 filter_image = pmax(pmin(filter_image, 255), 0) #Add to the grid of images display_grid[((row-1)*size+1):(row*size),((col-1)*size+1):(col*size)] = filter_image } } #Only visualize the intermediate activations of the convolutional layers if(length(grep(&#39;conv&#39;,layer_names[[i]]))!=0){ conv_layers = c(1,3,5,7) plots[[which(i == conv_layers)]] = plot_filter(display_grid) } } # Plot all the filters for each layer # 1st layer n_filters = dim(activations[[1]])[4] n_rows = as.integer(n_filters/images_per_row) fig_size(images_per_row +1,n_rows +1) plots[[1]]+labs(title = &quot;1st CNN Layer&quot;)+ theme(plot.title = element_text(size=22,hjust = 0.5)) # 2nd layer n_filters = dim(activations[[3]])[4] n_rows = as.integer(n_filters/images_per_row) fig_size(images_per_row +1,n_rows+1) plots[[2]]+labs(title = &quot;2nd CNN Layer&quot;)+ theme(plot.title = element_text(size=22,hjust = 0.5)) # 3rd layer n_filters = dim(activations[[5]])[4] n_rows = as.integer(n_filters/images_per_row) fig_size(images_per_row +1,n_rows+1) plots[[3]]+labs(title = &quot;3rd CNN Layer&quot;)+ theme(plot.title = element_text(size=22,hjust = 0.5)) # 4th layer n_filters = dim(activations[[7]])[4] n_rows = as.integer(n_filters/images_per_row) fig_size(images_per_row +1,n_rows+1) plots[[4]]+labs(title = &quot;4th CNN Layer&quot;)+ theme(plot.title = element_text(size=22,hjust = 0.5)) To guide you a little, here are a few things to note: The first layer is a collection of edge detectors. The activations maintain most of the information present in the original image. They still look like cats. As you go to deeper layers, the activations become increasingly abstract and harder to interpret. They encode higher-level concepts, like “cat ear”. These activations carry increasingly less information about the raw content of the image and increasingly more information about the class of the image. As you go into even deeper layers, there is a greater proportion of empty activations. This means the patterns encoded by these higher-level layers are not found in this particular image. These three points relate to a universal characteristic of data representation in convolutional neural networks. As you go deeper into the network, the features extracted by each layer become increasingly abstract. They carry less information about the specific input image and more about its label. In this way, convolutional neural networks act as information distillation pipelines. Go back a few cells and see what happens with other cats. It’s really fun to see what these filters are picking up! 12.3 Exercise The purpose of this exercise is to create a CNN model for a multiclass classification problem. So far, a binary classification problem was considered. However, in practice, there are several cases where more than 2 classes have to be considered. For example, the mnist dataset consists of handwritten digit images and in total contains 10 different classes (number 0 to number 9). The goal of this exercise is to create a model that takes the handwritten image as input and predicts the number which is written in this image. Regarding the modelling part of multiclass classification problem the only thing that changes, compared to a binary classification problem, is the number of nodes and the activation function on the output layer. Also, a different loss function has to be considered (‘categorical_crossentropy’). In other words, the number of nodes of the output layer should equal the number of discrete classes (10 in our case). Furthermore, the activation function of the output layer is the ‘softmax’ activation which converts each output node to a probability of the corresponding class. Therefore, for a prediction, we choose the class where its node gives the maximum probability. Furthermore, a skeleton code is provided which does the preprocessing and creates a baseline model. Your task is to create a CNN architecture that outperforms the baseline model. The data are class balanced so accuracy can be used in this case. You should use dropout and/or regularization to avoid overfitting. Also, comment on the number of trainable parameters for each model. 12.3.1 Import libraries and data library(reticulate) # use_condaenv() library(keras) library(tensorflow) library(tidyverse) library(rsample) IMPORTANT NOTE: READ CAREFULLY! Do not skip this part or you’ll run into issues later on! In a moment, after you’ve read the following instructions carefully, you should: - run the code chunk immediately below this text (keras_model_sequential()). - look down in the Console it asks if you want to install some packages: (“Would you like to install Miniconda? [Y/n]:”). - write n and press enter. You should see the following code in the console: Would you like to install Miniconda? [Y/n]: n. Now, you can normally continue with the exercise. If you were too eager and already pressed Y (yes) and enter, don’t panic! Just close your environment, re-open it and make sure that next time you go with n (no). 12.3.2 Tasks keras_model_sequential() Load mnist dataset: mnist = dataset_mnist() # take train and test set train = mnist$train test = mnist$test Plot an image index_image = 5 ## change this index to see different image. input_matrix = train$x[index_image,,] output_matrix &lt;- apply(input_matrix, 2, rev) output_matrix &lt;- t(output_matrix) image(output_matrix, col=gray.colors(256), xlab=paste(&#39;Image for digit &#39;, train$y[index_image]), ylab=&quot;&quot;) Specify image size and number of classes img_size = dim(train$x)[2:3] img_channels = 1 n_classes = length(unique(train$y)) cat(&#39;Image size: &#39;,c(img_size,img_channels),&quot;\\n&quot;) cat(&#39;Total classes: &#39;,n_classes) Create a validation set using stratified split and rescale input to [0,1] #make stratified split split = initial_split(data.frame(&#39;y&#39;=train$y),prop = 0.8,strata = &#39;y&#39;) #train set x_train = train$x[split$in_id,,] ##rescale to [0,1] x_train = x_train/255 y_train = train$y[split$in_id] #validation set x_val = train$x[-split$in_id,,] ##rescale to [0,1] x_val = x_val/255 y_val = train$y[-split$in_id] #test set x_test = test$x ##rescale to [0,1] x_test = x_test/255 y_test = test$y Encode classes to one-hot vectors y_train = to_categorical(y_train, n_classes) y_val = to_categorical(y_val,n_classes) y_test = to_categorical(y_test, n_classes) head(y_train) Create and train the baseline model A simple approach would be to create a vector (flatten the 2d input matrix) from the input image and then to apply a feed-forward neural network (i.e. dense layers). You are already provided with a trained model (because it takes a long time to train it from scratch), so you do not have to run the fit function. You just have to load the model. You don’t have to run the following chunk but have a look at this model nevertheless. ffnn_model = keras_model_sequential() ffnn_model %&gt;% layer_flatten(input_shape = img_size) %&gt;% layer_dense(units = 1024,activation = &#39;relu&#39;)%&gt;% layer_dense(units = 512,activation = &#39;relu&#39;)%&gt;% layer_dense(units = 256,activation = &#39;relu&#39;) %&gt;% layer_dense(units = 128,activation = &#39;relu&#39;) %&gt;% layer_dense(units = 10,activation = &#39;softmax&#39;) ffnn_model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_adam(lr = 0.001), metrics = c(&#39;accuracy&#39;) ) dir.create(file.path(&#39;saved_models&#39;)) save_ffnn = file.path(&#39;saved_models&#39;,&#39;baseline.h5&#39;) callbacks_ffnn = list(callback_early_stopping(monitor=&#39;val_loss&#39;,patience = 5,mode = &#39;min&#39;), callback_model_checkpoint(save_ffnn,monitor=&#39;val_loss&#39;,save_best_only = T,mode = &#39;min&#39;)) #history_ffnn = fit(ffnn_model,x_train, y_train, epochs = 20, batch_size = 128, # validation_data = list(x_val,y_val),callbacks = callbacks_ffnn) Load the saved Baseline model and evaluate the performance on validation and test set model_ffnn = load_model_hdf5(&quot;./data/SDL_I/saved_models/baseline.h5&quot;) #evalute accuracy val_acc = evaluate(model_ffnn,x_val,y_val)[[2]] test_acc = evaluate(model_ffnn,x_test,y_test)[[2]] cat(&#39;Validation Accuracy: &#39;, val_acc,&#39;\\n&#39;) cat(&#39;Test Accuracy: &#39;,test_acc,&#39;\\n&#39;) Next steps to code: Create a CNN architecture Train the model Hint: reshape your inputs so that they match the desired dimension of the input_shape in layer_conv_2d. Use callbacks for early stopping and model checkpoint (look at the baseline model). Load the saved model, evaluate performance on validation and test set Print summary of both models and comment on the number of parameters "],["ch-13.html", "Chapter 13 Supervised Deep Learning II - A Scenario of Environmental Systems 13.1 Introduction 13.2 Tutorial 13.3 Exercise", " Chapter 13 Supervised Deep Learning II - A Scenario of Environmental Systems 13.1 Introduction In this tutorial, we will put into practice and apply some of the models seen earlier in the course, in order to achieve a simple goal: efficiently extrapolate an environmental data set. For this, we will use satellite, meteorological, environmental data (that is measured scarcely by expensive physical devices (towers)) as an accurate and cheap predictors of a specific environmental variable (GPP - Gross primary production). Sit down and relax. This tutorial invites you to take a step back, and reflect on all the knowledge you have acquired on modeling. 13.1.1 Learning objectives Understand the link between research objective, data type and machine learning methods. Develop critical judgment, assess the relevance of a method for a particular modeling objective. Be able to cite examples of possible applications of various machine learning methods to problems in environmental systems science. Build intuitions for future machine learning applications in own projects. Understand the difference between forward Neural Networks, CNN. Appreciate similarities and differences between simpler statistical models and NN. Important points from the lecture Several types of models are commonly used in the Environmental Sciences. Those include linear models, general linear models, random forests, and neural networks. Linear models are suited for obtaining simple relationships between predictors and variables (for those with a normally distributed response variable) \\[\\begin{equation} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N} (0,\\sigma^2) \\end{equation}\\] Generalized linear models are similar to linear models. However, they allow for less restrictive assumptions on the distribution of the predicted variable (e.g., can be binomial for predicting presence/absence). Randoms forests can be used for classification or label predictions. Neural Networks can be used for capturing more complex relationships between predictors and variable. This comes at the cost of interpretability. Convolutional Neural Networks are a special type of neural network which is suited to for example capturing the effect of the spatial structure of the predictors. Neural Networks can be combined with mechanistic models to reduce the dimensionality of a problem. 13.1.2 Content &amp; Operations Description of the modeling task Preprocessing Old World: Linear models New World: Neural networks Feed Forward Neural Network Convolutional Neural Network Comparisons 13.2 Tutorial 13.2.1 Description of the modelling task Ecosystem-atmosphere exchange fluxes of water vapour and CO2 are continuously measured at several hundred of sites, distributed across the globe. The oldest running sites have been recording data since over twenty years. Thanks to the international FLUXNET initiative, these time series data are made openly accessible from over hundred sites and provided in a standardized format and complemented with measurements of several meteorological variables, plus soil temperature and moisture, measured in parallel. These data provide an opportunity for understanding ecosystem fluxes and how they are affected by environmental covariates. The challenge is to build models that are sufficiently generalisable in space. That is, temporally resolved relationships learned from one subset of sites should be used effectively to predict time series, given environmental covariates, at new sites (spatial upscaling). This is a challenge as previous research has shown that relatively powerful site-specific models can be trained, but predictions to new sites have been found wanting. This may be due to site-specific characteristics (e.g. vegetation type) that have thus far not been satisfactorily encoded in models. In other words, factors that would typically be regarded as random factors in mixed effects modeling, continue to undermine effective learning in machine learning models. 13.2.2 Goal of the tutorial Our aim is to build a model that can estimate the GPP - gross primary production of a given place, extending the range of measurements from around the towers to the entire Earth system. We will compare different approaches to find the function \\(f\\) that best captures the relationship between the explanatory variables and the GPP - gross primary production (\\(y_{GPP}\\)). \\[\\begin{equation} y_{GPP} = f(x_{features},...) \\end{equation}\\] 13.2.3 Dataset As a first step of any modeling problem, we need to get and process the data to be used. Those steps have been detailed in Tutorial 4: Data scraping. 13.2.3.1 FLUXNET dataset We will once again be using the FLUXNET data set, that you have gotten familiar with in the previous tutorials. We want to try to predict the variable y = GPP_NT_VUT_REF, which expresses the monthly gross primary production of the corresponding region. Here is the map showing the location of the 71 towers available in our dataset: 13.2.3.2 Explanatory variables fpar_loess: fraction of absorbed photosynthetically active radiation, interpolated to monthly values using LOESS TA_F: Air temperature SW_IN_F: Shortwave incoming radiation LW_IN_F: Longwave incoming radiation VPD_F: Vapour pressure deficit (relates to the humidity of the air) PA_F: Atmospheric pressure P_F: Precipitation WS_F: Wind speed CO2_F_MDS: CO2 concentration NDVI (Landsat 7 product): The Normalised Difference Vegetation Index (NDVI) is a measure for live green foliage and is strongly related to the fraction of absorbed photosynthetically active radiation. In other words, it should scale (more or less linearly) with GPP. NDVI is easy to obtain at a fine spatial resolution, through satellite imagery. The spatial resolution of a pixel is 30m and the temporal resolution between 2 consecutive dates is 16 days.In this tutorial, we keep the maximum available date per month. More details about the product can be found here. For each area of interest (i.e tower location) and date, a square with side 6km is considered. The center of this square is the tower location. The extracted NDVI pixels are the area of the square. Therefore, in total for a specific (tower location, date) pair we have a square with a side of [6km / 30m(per pixel)] = 200 pixels and thus 200 x 200 = 40000 pixels per pair. Each Landsat product gives a tile with the desired information. From this tile a region around each tower is extracted. The following picture shows you an example of such a tile and the area around the CH Oerlikon tower. Extracted Region (black square) around the Tower of CH Oerlikon (red cross) for the given Landsat tile 13.2.4 Let’s dive into the code First we need to load the libraries required for our modeling task. We’ll be using the Keras library for the neural networks. library(tidyverse) library(lubridate) library(stringr) library(imputeTS) # Library for Imputation library(reticulate) # use_condaenv(&#39;r-reticulate&#39;) library(keras) # Python library for deep learning library(tensorflow) # Google API for Machine Learning 13.2.5 Read in the Data We have two data sources. The FLUXNET data set averaged on monthly valued which contains the target variable gpp as well as the following explanatory variables: fpar_loess, TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, CO2_F_MDS. Next, we have the Landsat 7 NDVI product on monthly NDVI values. path = &quot;./data/&quot; # read fluxnet data flux_data = read.csv(file.path(path, &quot;flux_dataset.csv&quot;)) # have a look head(flux_data) ## X sitename year month y fpar_loess_per_month TA_F_per_month ## 1 1 AR-Vir 2010 2 18.155930 0.7175273 27.90639 ## 2 2 AR-Vir 2010 3 16.566188 0.8062719 25.88332 ## 3 3 AR-Vir 2010 4 11.883871 0.8253712 21.08527 ## 4 4 AR-Vir 2010 5 7.550117 0.7912010 17.02619 ## 5 5 AR-Vir 2010 9 12.171233 0.6176354 19.77360 ## 6 6 AR-Vir 2010 10 12.463795 0.7283527 20.05271 ## SW_IN_F_per_month LW_IN_F_per_month VPD_F_per_month PA_F_per_month ## 1 230.0684 412.0384 13.867857 99.57232 ## 2 212.2040 391.1563 11.892645 99.74226 ## 3 168.1141 359.2931 9.315033 100.19970 ## 4 109.3910 349.3595 5.616355 100.30897 ## 5 176.7665 349.7738 6.618600 100.01453 ## 6 236.9492 348.2435 6.848935 100.02084 ## P_F_per_month WS_F_per_month CO2_F_MDS_per_month ## 1 4.796929 2.778643 424.0304 ## 2 6.909194 2.407871 430.5104 ## 3 4.921133 2.639733 420.3460 ## 4 3.307871 2.397226 432.3273 ## 5 5.868133 2.339433 426.6124 ## 6 3.709258 2.558194 417.1832 # read ndvi file path ndvi_files = list.files(file.path(path, &quot;ndvi/&quot;), full.names = T) # look one file cat(ndvi_files[1]) ## ./data//ndvi//AR-Vir_2010-02-24_ndvi_pixel.rds Let’s now look at some statistics of the data. # Number of samples cat(paste(&quot;The dataset contain&quot;, nrow(flux_data), &quot;samples&quot;, &quot;\\n \\n&quot;)) ## The dataset contain 5617 samples ## # Number of distinct towers cat(paste(&quot;The dataset contain&quot;, length(unique(flux_data$sitename)), &quot;different tower sites&quot;,&quot;\\n&quot;)) ## The dataset contain 71 different tower sites We start now the modeling part 13.2.6 Naive models (Old World): Linear Models We first build a linear model, which assumes a linear relationship between label (output) and feature (input) \\[y=f(x)+ϵ\\]. That is, f is of the form \\[f(x)=β_1x+β_0\\] with \\(β_0,β_1 \\in R\\). For this first model we will use only the NDVI value as our predictor. However, it seems reasonable to simply average the NDVI values per image, and use it as a predictor for the GPP. Later on we will extract more statistics. avg_values_ndvi = c() for(j in 1:length(ndvi_files)){ # read ndvi file ndvi = readRDS(ndvi_files[j]) # keep only healthy pixels ndvi = ndvi[ndvi &lt;= 10000] # take average ndvi avg_values_ndvi = c(avg_values_ndvi, mean(ndvi, na.rm = T)) } The built-in function lm can be used to estimate \\(f\\). # create dataframe df_data = data.frame(y = flux_data$y, avg_ndvi = avg_values_ndvi) # drop null values df_data = na.omit(df_data) head(df_data) ## y avg_ndvi ## 1 18.155930 1060.115 ## 2 16.566188 7133.001 ## 3 11.883871 7486.476 ## 4 7.550117 7195.569 ## 5 12.171233 5060.266 ## 6 12.463795 5682.808 # fit the model model_ndvi = lm(y ~ avg_ndvi, data = df_data) summary(model_ndvi) ## ## Call: ## lm(formula = y ~ avg_ndvi, data = df_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.8873 -2.1929 -0.7164 1.7530 16.0698 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.183e+00 7.238e-02 16.34 &lt;2e-16 *** ## avg_ndvi 8.523e-04 1.758e-05 48.47 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.077 on 5330 degrees of freedom ## Multiple R-squared: 0.3059, Adjusted R-squared: 0.3058 ## F-statistic: 2349 on 1 and 5330 DF, p-value: &lt; 2.2e-16 Although the \\(R^2 = 0.31\\) is low, the NDVI predictor can be considered significant (\\(p_{value} &lt; 0.05\\)) for the prediction of GPP. Let’s also create a plot of the linear model df_data %&gt;% ggplot(aes(x = avg_ndvi, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, lwd = 2, se = F) + theme_gray(base_size = 20) + labs(x = &#39;Average NDVI&#39;, y = &quot;gpp&quot; ) Next, we fit again a linear model but at this time we incorporate all the available predictors of fluxnet data set along with the extracted average NDVI values. colnames(flux_data) ## [1] &quot;X&quot; &quot;sitename&quot; &quot;year&quot; ## [4] &quot;month&quot; &quot;y&quot; &quot;fpar_loess_per_month&quot; ## [7] &quot;TA_F_per_month&quot; &quot;SW_IN_F_per_month&quot; &quot;LW_IN_F_per_month&quot; ## [10] &quot;VPD_F_per_month&quot; &quot;PA_F_per_month&quot; &quot;P_F_per_month&quot; ## [13] &quot;WS_F_per_month&quot; &quot;CO2_F_MDS_per_month&quot; # create dataframe df_data_all = data.frame(flux_data[,-c(1,3:4)], avg_ndvi = avg_values_ndvi) head(df_data_all) ## sitename y fpar_loess_per_month TA_F_per_month SW_IN_F_per_month ## 1 AR-Vir 18.155930 0.7175273 27.90639 230.0684 ## 2 AR-Vir 16.566188 0.8062719 25.88332 212.2040 ## 3 AR-Vir 11.883871 0.8253712 21.08527 168.1141 ## 4 AR-Vir 7.550117 0.7912010 17.02619 109.3910 ## 5 AR-Vir 12.171233 0.6176354 19.77360 176.7665 ## 6 AR-Vir 12.463795 0.7283527 20.05271 236.9492 ## LW_IN_F_per_month VPD_F_per_month PA_F_per_month P_F_per_month WS_F_per_month ## 1 412.0384 13.867857 99.57232 4.796929 2.778643 ## 2 391.1563 11.892645 99.74226 6.909194 2.407871 ## 3 359.2931 9.315033 100.19970 4.921133 2.639733 ## 4 349.3595 5.616355 100.30897 3.307871 2.397226 ## 5 349.7738 6.618600 100.01453 5.868133 2.339433 ## 6 348.2435 6.848935 100.02084 3.709258 2.558194 ## CO2_F_MDS_per_month avg_ndvi ## 1 424.0304 1060.115 ## 2 430.5104 7133.001 ## 3 420.3460 7486.476 ## 4 432.3273 7195.569 ## 5 426.6124 5060.266 ## 6 417.1832 5682.808 # drop null values df_data_all = na.omit(df_data_all) # fit the model model_all = lm(y ~ fpar_loess_per_month + TA_F_per_month + SW_IN_F_per_month + LW_IN_F_per_month + VPD_F_per_month + PA_F_per_month + P_F_per_month + WS_F_per_month + CO2_F_MDS_per_month + avg_ndvi, data = df_data_all) summary(model_all) ## ## Call: ## lm(formula = y ~ fpar_loess_per_month + TA_F_per_month + SW_IN_F_per_month + ## LW_IN_F_per_month + VPD_F_per_month + PA_F_per_month + P_F_per_month + ## WS_F_per_month + CO2_F_MDS_per_month + avg_ndvi, data = df_data_all) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.3155 -1.1650 -0.1044 0.9828 10.8688 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.103e+00 8.849e-01 -8.027 1.26e-15 *** ## fpar_loess_per_month 8.820e+00 2.155e-01 40.923 &lt; 2e-16 *** ## TA_F_per_month -2.676e-04 1.207e-02 -0.022 0.982313 ## SW_IN_F_per_month 2.084e-02 6.371e-04 32.700 &lt; 2e-16 *** ## LW_IN_F_per_month 2.282e-02 1.648e-03 13.845 &lt; 2e-16 *** ## VPD_F_per_month -2.460e-01 1.103e-02 -22.302 &lt; 2e-16 *** ## PA_F_per_month -3.698e-02 5.178e-03 -7.141 1.07e-12 *** ## P_F_per_month 5.612e-02 1.490e-02 3.765 0.000168 *** ## WS_F_per_month 7.624e-02 3.123e-02 2.441 0.014692 * ## CO2_F_MDS_per_month 8.748e-04 1.732e-03 0.505 0.613560 ## avg_ndvi 1.066e-04 1.544e-05 6.902 5.81e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.917 on 4566 degrees of freedom ## Multiple R-squared: 0.7322, Adjusted R-squared: 0.7316 ## F-statistic: 1248 on 10 and 4566 DF, p-value: &lt; 2.2e-16 As you can see now, by using all the available predictors we have a magnificent improvement of the \\(R^2 = 0.73\\) We may also perform a cross validation approach for the mse of our model. Our goal is to generalize to unseen tower sites. Therefore the test fold of the cv should be unseen tower sites. Here, we choose to leave one tower site out (similar to LooCV apporach) for the test fold n_tower_test = 1 unique_towers = unique(df_data_all$sitename) n_breaks = as.integer(length(unique_towers)/n_tower_test) folds = cut(1:length(unique_towers), breaks = n_breaks,labels = F) mse_per_fold = c() for(i in 1:n_breaks){ train_towers = unique(df_data_all$sitename)[folds!=i] test_towers = unique(df_data_all$sitename)[folds==i] train_ind = df_data_all$sitename %in% train_towers test_ind = !train_ind x_train = df_data_all[train_ind,-c(1:2)] x_test = df_data_all[test_ind,-c(1:2)] y_train = df_data_all$y[train_ind] y_test = df_data_all$y[test_ind] df = data.frame(y_train = y_train,x_train) head(df) fit = lm(y_train ~ . ,data = df) pred = predict(fit, x_test) mse = mean((pred-y_test)^2,na.rm=T) mse_per_fold = c(mse_per_fold,mse) cat(paste(test_towers, &#39;mse:&#39;, mse,&quot;\\n&quot;)) } ## AR-Vir mse: 25.5165721158572 ## AU-Ade mse: 0.996198031367656 ## AU-ASM mse: 1.47301824854867 ## AU-DaP mse: 4.37935258968633 ## AU-DaS mse: 1.59165837894441 ## AU-Dry mse: 1.61280472268077 ## AU-Fog mse: 9.3672008641652 ## AU-Gin mse: 1.02531653623349 ## AU-How mse: 2.16895221336887 ## AU-Stp mse: 1.7753231861928 ## AU-Whr mse: 0.2204273751078 ## AU-Wom mse: 1.83977950254792 ## BE-Bra mse: 1.91301758761868 ## BE-Vie mse: 2.89262264258649 ## CH-Fru mse: 3.11655794961346 ## CH-Lae mse: 2.30191269258554 ## CH-Oe1 mse: 5.27251470602645 ## CN-Cng mse: 4.86463105435891 ## CZ-wet mse: 4.54993627632313 ## DE-Akm mse: 7.72342722209925 ## DE-Geb mse: 10.5842119696382 ## DE-Gri mse: 3.84948495477265 ## DE-Hai mse: 6.38586262324752 ## DE-Kli mse: 9.67131843905939 ## DE-Obe mse: 5.9036191598398 ## DE-RuR mse: 2.87756815212698 ## DE-Spw mse: 4.23880705859011 ## DE-Tha mse: 5.29920782081775 ## DK-NuF mse: 4.16811410507384 ## DK-Sor mse: 20.1741981838768 ## FI-Hyy mse: 2.33538347040779 ## FI-Sod mse: 1.62960603843157 ## FR-Fon mse: 7.42811535539279 ## FR-LBr mse: 1.50301196281801 ## FR-Pue mse: 5.13653390418094 ## IT-Col mse: 3.0317868888794 ## IT-Cp2 mse: 2.65216532171752 ## IT-Cpz mse: 3.17238775179885 ## IT-Isp mse: 10.3873664088935 ## IT-Lav mse: 3.010691028071 ## IT-MBo mse: 2.8785260822475 ## IT-Noe mse: 7.35344799088317 ## IT-PT1 mse: 7.87607679174638 ## IT-Ren mse: 1.23982585828515 ## IT-Ro1 mse: 5.29258867461421 ## IT-SR2 mse: 1.65807484316668 ## IT-SRo mse: 2.15785071766904 ## IT-Tor mse: 2.37668631080789 ## JP-SMF mse: 2.27771030753627 ## NL-Hor mse: 2.79257242006782 ## NL-Loo mse: 1.81894119626356 ## RU-Fyo mse: 3.46941802687701 ## SD-Dem mse: 14.8223454673033 ## US-GLE mse: 2.89794145838108 ## US-Ha1 mse: 4.41953069773198 ## US-Los mse: 6.60932420721002 ## US-Me2 mse: 2.61506241914978 ## US-MMS mse: 3.64287833864999 ## US-PFa mse: 5.22700946992549 ## US-SRG mse: 0.723577410407293 ## US-SRM mse: 0.512748570189335 ## US-Syv mse: 3.21731260411245 ## US-Ton mse: 0.824709832111569 ## US-UMB mse: 1.87149312696677 ## US-UMd mse: 2.05593059595968 ## US-Var mse: 5.90022888855451 ## US-WCr mse: 3.78983794424176 ## US-Wi4 mse: 2.31229320696217 ## ZM-Mon mse: 0.58085100015533 cat(paste(&quot;\\n&quot;, &quot;CV MSE:&quot;, mean(mse_per_fold))) ## ## CV MSE: 4.36602114422789 13.2.7 Neural Networks (New World) 13.2.7.1 Feed Forward Neural Network Instead of a Linear model, let’s see what happens when we approximate \\(f\\) using a Feed Forward Neural Network with two hidden layers. We use again all the available predictors as well as the ndvi features (we also extract more statistics such maximum, minimum and standard deviation). avg_values_ndvi = c() max_values_ndvi = c() min_values_ndvi = c() std_values_ndvi = c() for(j in 1:length(ndvi_files)){ # read ndvi file ndvi = readRDS(ndvi_files[j]) # keep only healthy pixels ndvi = ndvi[ndvi &lt;= 10000] # take average ndvi avg_values_ndvi = c(avg_values_ndvi, mean(ndvi,na.rm = T)) # take max ndvi max_values_ndvi = c(max_values_ndvi, max(ndvi,na.rm = T)) # take min ndvi min_values_ndvi = c(min_values_ndvi, min(ndvi,na.rm = T)) # take std ndvi std_values_ndvi = c(std_values_ndvi, sd(ndvi,na.rm = T)) } # create dataframe df_data_all_nn = data.frame(flux_data[,-c(1,3:4)], avg_ndvi = avg_values_ndvi, max_ndvi = max_values_ndvi, min_ndvi = min_values_ndvi, std_ndvi = std_values_ndvi) head(df_data_all_nn) ## sitename y fpar_loess_per_month TA_F_per_month SW_IN_F_per_month ## 1 AR-Vir 18.155930 0.7175273 27.90639 230.0684 ## 2 AR-Vir 16.566188 0.8062719 25.88332 212.2040 ## 3 AR-Vir 11.883871 0.8253712 21.08527 168.1141 ## 4 AR-Vir 7.550117 0.7912010 17.02619 109.3910 ## 5 AR-Vir 12.171233 0.6176354 19.77360 176.7665 ## 6 AR-Vir 12.463795 0.7283527 20.05271 236.9492 ## LW_IN_F_per_month VPD_F_per_month PA_F_per_month P_F_per_month WS_F_per_month ## 1 412.0384 13.867857 99.57232 4.796929 2.778643 ## 2 391.1563 11.892645 99.74226 6.909194 2.407871 ## 3 359.2931 9.315033 100.19970 4.921133 2.639733 ## 4 349.3595 5.616355 100.30897 3.307871 2.397226 ## 5 349.7738 6.618600 100.01453 5.868133 2.339433 ## 6 348.2435 6.848935 100.02084 3.709258 2.558194 ## CO2_F_MDS_per_month avg_ndvi max_ndvi min_ndvi std_ndvi ## 1 424.0304 1060.115 4302 -256 367.8867 ## 2 430.5104 7133.001 10000 -2028 520.4147 ## 3 420.3460 7486.476 9483 -3780 1197.5095 ## 4 432.3273 7195.569 10000 -3990 1363.8775 ## 5 426.6124 5060.266 6845 -282 866.6186 ## 6 417.1832 5682.808 8490 -1336 1345.6937 # drop null values df_data_all_nn = na.omit(df_data_all_nn) y = df_data_all_nn$y x = df_data_all_nn[,-c(1,2)] The first step and key step as always is preprocessing! # normalize x_scaled = scale(x) head(x_scaled) ## fpar_loess_per_month TA_F_per_month SW_IN_F_per_month LW_IN_F_per_month ## 1 1.103840 1.6416076 0.66121806 2.0251698 ## 2 1.546659 1.4284386 0.44404475 1.5772096 ## 3 1.641961 0.9228722 -0.09194708 0.8936821 ## 4 1.471458 0.4951717 -0.80583171 0.6805892 ## 5 0.605397 0.7846632 0.01323877 0.6894764 ## 6 1.157856 0.8140727 0.74486658 0.6566475 ## VPD_F_per_month PA_F_per_month P_F_per_month WS_F_per_month ## 1 1.130090936 0.6155796 1.0972644 0.27527617 ## 2 0.820142810 0.6403129 2.0123121 -0.10299988 ## 3 0.415666754 0.7068910 1.1510708 0.13355522 ## 4 -0.164725871 0.7227943 0.4521944 -0.11386049 ## 5 -0.007454645 0.6799410 1.5613176 -0.17282264 ## 6 0.028689349 0.6808587 0.6260781 0.05036513 ## CO2_F_MDS_per_month avg_ndvi max_ndvi min_ndvi std_ndvi ## 1 2.030934 -0.9914477 -0.7223423 0.1833766 -0.87171195 ## 2 2.405037 1.5677285 1.1334298 -0.5466108 -0.71461192 ## 3 1.818223 1.7166866 0.9650489 -1.2683591 -0.01722132 ## 4 2.509927 1.5940955 1.1334298 -1.3548700 0.15413353 ## 5 2.179998 0.6942571 0.1058831 0.1726657 -0.35803060 ## 6 1.635632 0.9566023 0.6416404 -0.2615367 0.13540472 Then we define the model (our function f), as follows with: an input layer, with dimensions the number of features x a hidden layer, with relu activation, with a size of 10 a hidden layer, with relu activation, with a size of 5 an ouput layer, of size 1 with linear activation #FFNN model = keras_model_sequential() model %&gt;% layer_dense(units = 10, activation = &#39;relu&#39;, input_shape = ncol(x)) %&gt;% layer_dense(units = 5, activation = &#39;relu&#39;) %&gt;% layer_dense(units=1) We can also have a look at the number of trainable parameters of the model. summary(model) ## Model: &quot;sequential&quot; ## __________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================== ## dense_2 (Dense) (None, 10) 140 ## __________________________________________________________________________ ## dense_1 (Dense) (None, 5) 55 ## __________________________________________________________________________ ## dense (Dense) (None, 1) 6 ## ========================================================================== ## Total params: 201 ## Trainable params: 201 ## Non-trainable params: 0 ## __________________________________________________________________________ In this setting we make a CV approach while keeping 10 tower sites for the test fold (in order to save time). n_tower_test = 10 unique_towers = unique(df_data_all_nn$sitename) n_breaks = as.integer(length(unique_towers)/n_tower_test) folds = cut(1:length(unique_towers), breaks = n_breaks,labels = F) mse_per_fold_nn = c() for(i in 1:n_breaks){ train_towers = unique(df_data_all_nn$sitename)[folds!=i] test_towers = unique(df_data_all_nn$sitename)[folds==i] train_ind = df_data_all_nn$sitename %in% train_towers test_ind = !train_ind model = keras_model_sequential() model %&gt;% layer_dense(units = 10, activation = &#39;relu&#39;, input_shape = ncol(x)) %&gt;% layer_dense(units = 5, activation = &#39;relu&#39;) %&gt;% layer_dense(units=1) #optimizer opt=optimizer_adam(lr=0.01) #compile compile(model, loss = &#39;mse&#39;, optimizer = opt, metrics=list(&#39;mse&#39;)) history_ffnn = fit(model, x = data.matrix(x_scaled[train_ind,]), y = y[train_ind], batch_size = 512, epochs = 50, shuffle = T) pred = predict(model, data.matrix(x_scaled[test_ind,])) mse = mean((pred-y[test_ind])^2, na.rm=T) mse_per_fold_nn = c(mse_per_fold_nn,mse) cat(paste(test_towers, &#39;mse:&#39;, mse,&quot;\\n&quot;)) } cat(paste(&quot;\\n&quot;, &quot;CV MSE:&quot;, mean(mse_per_fold_nn))) ## CV MSE: 3.78162750720368 As you can see, the FFNN improves the out of training mean square error (sometimes this is not the case due to randomization during training). 13.2.8 Convolutional Neural Network In the previous section we extracted statistics of NDVI images and then we used those features together with the FLUXNET features as input to the feed forward neural network. Now, we will use the image of the NDVI and we allow a CNN model to extract relevant features. Then we will feed those features together with the FLUXNET features to a feed forward neural network. As it mentioned in the beginning the size of each NDVI image is 200 x 200 pixel. To avoid memory issues and facilitate fast training we crop the images to 50 x 50 pixel keeping the same center. Before we proceed, we remind here the basic building blocks of the CNN model. Building Blocks of a CNN Convolutional Layers: Layers implementing the actual convolution. Their outputs are feature maps which are then passed through an activation function in order to introduce non-linearities into the system. Convolutional layers can be seen as extracting features that are passed on deeper into the model thus enabling the model to learn higher-level features that make the classification task easier. Pooling Layers: Downsampling or pooling layers concentrate the information so that deeper layers focus more on abstract/high-level patterns. A common choice is max-pooling, where only the maximum value occurring in a certain region is propagated to the output. Dense Layers: A dense or fully-connected layer connects every node in the input to every node in the output. This is the type of layer you already used in the previous tutorial. If the input dimension is large, the amount of learnable parameters introduced by using a dense layer can quickly explode. Hence, dense layers are usually added on deeper levels of the model, where the pooling operations have already reduced the dimensionality of the data. Typically, the dense layers are added last in a predictive model, performing the actual prediction on the features extracted by the convolutional layers. data_cnn = data.frame(flux_data, ndvi_file = ndvi_files) head(data_cnn) ## X sitename year month y fpar_loess_per_month TA_F_per_month ## 1 1 AR-Vir 2010 2 18.155930 0.7175273 27.90639 ## 2 2 AR-Vir 2010 3 16.566188 0.8062719 25.88332 ## 3 3 AR-Vir 2010 4 11.883871 0.8253712 21.08527 ## 4 4 AR-Vir 2010 5 7.550117 0.7912010 17.02619 ## 5 5 AR-Vir 2010 9 12.171233 0.6176354 19.77360 ## 6 6 AR-Vir 2010 10 12.463795 0.7283527 20.05271 ## SW_IN_F_per_month LW_IN_F_per_month VPD_F_per_month PA_F_per_month ## 1 230.0684 412.0384 13.867857 99.57232 ## 2 212.2040 391.1563 11.892645 99.74226 ## 3 168.1141 359.2931 9.315033 100.19970 ## 4 109.3910 349.3595 5.616355 100.30897 ## 5 176.7665 349.7738 6.618600 100.01453 ## 6 236.9492 348.2435 6.848935 100.02084 ## P_F_per_month WS_F_per_month CO2_F_MDS_per_month ## 1 4.796929 2.778643 424.0304 ## 2 6.909194 2.407871 430.5104 ## 3 4.921133 2.639733 420.3460 ## 4 3.307871 2.397226 432.3273 ## 5 5.868133 2.339433 426.6124 ## 6 3.709258 2.558194 417.1832 ## ndvi_file ## 1 ./data//ndvi//AR-Vir_2010-02-24_ndvi_pixel.rds ## 2 ./data//ndvi//AR-Vir_2010-03-28_ndvi_pixel.rds ## 3 ./data//ndvi//AR-Vir_2010-04-29_ndvi_pixel.rds ## 4 ./data//ndvi//AR-Vir_2010-05-31_ndvi_pixel.rds ## 5 ./data//ndvi//AR-Vir_2010-09-20_ndvi_pixel.rds ## 6 ./data//ndvi//AR-Vir_2010-10-06_ndvi_pixel.rds Here we remove the images with more than 40% missing NDVI values. # remove rows with NAs data_cnn = na.omit(data_cnn) # remove rows with more than 40% missing NDVI values dim = 200 to_remove = c() for(j in 1:length(data_cnn$ndvi_file)){ # read ndvi file, extract window of 50x50 ndvi = matrix(readRDS(data_cnn$ndvi_file[j]), nrow=dim, byrow = T)[75:124,75:124] # check percentage of missing values percentage_missing = mean(is.na(ndvi) | ndvi&gt;10000) if (percentage_missing &gt; 0.4 | length(ndvi)==0){ to_remove = c(to_remove, j) } } data_cnn_complete = data_cnn[-to_remove,] Now it is time to scale the inputs. dim = 50 ndvi = array(NA,dim =c(length(data_cnn_complete$ndvi_file), dim, dim)) for(j in 1:length(data_cnn_complete$ndvi_file)){ ndvi_values = readRDS(data_cnn_complete$ndvi_file[j]) #reshape ndvi[j,,] = matrix(ndvi_values, nrow=200, byrow = T)[75:124,75:124] } #specify image size IMAGE_WIDTH = dim IMAGE_HEIGHT = dim IMAGE_CHANNELS = 1 IMAGE_SIZE = c(IMAGE_WIDTH,IMAGE_HEIGHT,IMAGE_CHANNELS) #fill missing values , rescale images to [0,1] , reshape to be a valid input for NN preprocess_images = function(ndvi){ min_ndvi = -10000 max_ndvi = 10000 #fill missing values nd = apply(ndvi, c(2,3), function(i) na_interpolation(i)) #rescale to [0,1] nd = (nd-min_ndvi)/(max_ndvi-min_ndvi) #reshape adding an extra dimension nd = array_reshape(nd, dim = c(-1,IMAGE_SIZE)) return (nd) } ndvi_pr = preprocess_images(ndvi) x_flux = data_cnn_complete[,-c(1:5,15)] x_flux_scaled = scale(x_flux) y = data_cnn_complete$y Here we create the CNN model. We use only one CNN layer with 4 filters and kernel size of 5. We also use a pooling layer. create_cnn = function(){ #input --&gt; ndvi images input_1 = layer_input(shape=IMAGE_SIZE) # cnn layer cnn_layer = layer_conv_2d(input_1, filters = 4, kernel_size = c(5,5), activation = &#39;relu&#39;, padding = &#39;same&#39;) # pool layer pool = layer_average_pooling_2d(cnn_layer, pool_size = c(10, 10)) # flatten the features flat = layer_flatten(pool) # mlp of the features --&gt; project to dim 32 flat_proj = layer_dense(flat, units = 32, activation = &#39;relu&#39;) #input --&gt; fluxnet features input_2 = layer_input(shape=ncol(x_flux)) # concatenate features features = k_concatenate(c(flat_proj, input_2),axis=2) # hidden layer hidden_dense = layer_dense(features, units = 16, activation = &#39;relu&#39;) #output output = layer_dense(hidden_dense, units = 1, activation = &#39;linear&#39;) #create model model = keras_model(c(input_1, input_2), output) return(model) } cnn_model = create_cnn() summary(cnn_model) ## Model: &quot;model&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # Connected to ## ================================================================================ ## input_1 (InputLayer) [(None, 50, 50, 1 0 ## ________________________________________________________________________________ ## conv2d (Conv2D) (None, 50, 50, 4) 104 input_1[0][0] ## ________________________________________________________________________________ ## average_pooling2d (Averag (None, 5, 5, 4) 0 conv2d[0][0] ## ________________________________________________________________________________ ## flatten (Flatten) (None, 100) 0 average_pooling2d[0][0] ## ________________________________________________________________________________ ## dense_21 (Dense) (None, 32) 3232 flatten[0][0] ## ________________________________________________________________________________ ## input_2 (InputLayer) [(None, 9)] 0 ## ________________________________________________________________________________ ## tf.concat (TFOpLambda) (None, 41) 0 dense_21[0][0] ## input_2[0][0] ## ________________________________________________________________________________ ## dense_22 (Dense) (None, 16) 672 tf.concat[0][0] ## ________________________________________________________________________________ ## dense_23 (Dense) (None, 1) 17 dense_22[0][0] ## ================================================================================ ## Total params: 4,025 ## Trainable params: 4,025 ## Non-trainable params: 0 ## ________________________________________________________________________________ To evaluate the performance of the model we follow a cv approach by leaving out 10 tower sites. n_tower_test = 10 unique_towers = unique(data_cnn_complete$sitename) n_breaks = as.integer(length(unique_towers)/n_tower_test) folds = cut(1:length(unique_towers),breaks = n_breaks,labels = F) mse_per_fold_cnn = c() for(i in 1:n_breaks){ train_towers = unique(data_cnn_complete$sitename)[folds!=i] test_towers = unique(data_cnn_complete$sitename)[folds==i] train_ind = data_cnn_complete$sitename %in% train_towers test_ind = !train_ind model_cnn = create_cnn() #optimizer opt=optimizer_adam(lr=0.01) #compile compile(model_cnn, loss = &#39;mse&#39;, optimizer = opt, metrics=list(&#39;mse&#39;)) history_ffnn = fit(model_cnn, x = list(array_reshape(ndvi_pr[train_ind,,,], dim=c(-1,IMAGE_SIZE)), data.matrix(x_flux_scaled[train_ind,])), y = y[train_ind], batch_size = 512, epochs = 20, shuffle = T) pred = predict(model_cnn,list(array_reshape(ndvi_pr[test_ind,,,], dim=c(-1,IMAGE_SIZE)), data.matrix(x_flux_scaled[test_ind,]))) mse = mean((pred-y[test_ind])^2,na.rm=T) mse_per_fold_cnn = c(mse_per_fold_cnn, mse) cat(paste(test_towers, &#39;mse:&#39;, mse,&quot;\\n&quot;)) } cat(paste(&quot;\\n&quot;,&quot;CV MSE:&quot;,mean(mse_per_fold_cnn))) ## CV MSE: 3.64838966197611 The CNN yields better performances than the linear model and the FFNN. However, since we used different inputs, we should be careful with comparisons. 13.3 Exercise In class, we used the monthly FLUXNET data set and the NDVI images of 71 distinct tower sites. We first had to fit a linear model using only the average NDVI in order to predict the GPP - gross primary production . The first task for this exercise is to create a polynomial model of degree 2 using the average NDVI value. Is there an improvement on \\(R^2\\)? Can you also plot the corresponding fitting polynomial figure? In addition, we made a feed forward neural network using all the available features and evaluating the model on cv leaving out 10 sites. Your second task is to fit again a feed forward neural network but using more layers and/or neurons this time. You may want to add regularization methods (i.e dropout etc.) in order to avoid overfitting. Check the performance on cv leaving out 10 sites. Tasks: Fit a polynomial model of degree 2 using the average NDVI value as predictor in order to predict the GPP value. Compare the \\(R^2\\) with the linear model. Plot the polynomial model. Create a feed forward neural network with more layers and/or neurons than the one made in class. Take care to avoid overfitting. Evaluate your results on cv leaving out 10 tower sites. "],["references.html", "References", " References ``` "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
