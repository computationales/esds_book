<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Supervised Machine Learning II | Environmental Systems Data Science</title>
  <meta name="description" content="Tutorial and exercises for Environmental System Data Science, ETH Zürich." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Supervised Machine Learning II | Environmental Systems Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Tutorial and exercises for Environmental System Data Science, ETH Zürich." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Supervised Machine Learning II | Environmental Systems Data Science" />
  
  <meta name="twitter:description" content="Tutorial and exercises for Environmental System Data Science, ETH Zürich." />
  

<meta name="author" content="Loïc Pellissier, Joshua Payne, Benjamin Stocker" />


<meta name="date" content="2022-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-06.html"/>
<link rel="next" href="ch-08.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-description"><i class="fa fa-check"></i>Course Description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objectives"><i class="fa fa-check"></i>Course Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#content"><i class="fa fa-check"></i>Content</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#useful-prerequisites"><i class="fa fa-check"></i>Useful Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-01.html"><a href="ch-01.html"><i class="fa fa-check"></i><b>1</b> Primers</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-01.html"><a href="ch-01.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="ch-01.html"><a href="ch-01.html#key-points-from-the-lecture"><i class="fa fa-check"></i><b>1.1.1</b> Key Points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ch-01.html"><a href="ch-01.html#tutorial"><i class="fa fa-check"></i><b>1.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ch-01.html"><a href="ch-01.html#working-with-rstudio-on-renku"><i class="fa fa-check"></i><b>1.2.1</b> Working with RStudio on Renku</a></li>
<li class="chapter" data-level="1.2.2" data-path="ch-01.html"><a href="ch-01.html#git"><i class="fa fa-check"></i><b>1.2.2</b> Git</a></li>
<li class="chapter" data-level="1.2.3" data-path="ch-01.html"><a href="ch-01.html#libraries"><i class="fa fa-check"></i><b>1.2.3</b> Libraries</a></li>
<li class="chapter" data-level="1.2.4" data-path="ch-01.html"><a href="ch-01.html#r-scripts"><i class="fa fa-check"></i><b>1.2.4</b> R scripts</a></li>
<li class="chapter" data-level="1.2.5" data-path="ch-01.html"><a href="ch-01.html#rmarkdown"><i class="fa fa-check"></i><b>1.2.5</b> RMarkdown</a></li>
<li class="chapter" data-level="1.2.6" data-path="ch-01.html"><a href="ch-01.html#functions"><i class="fa fa-check"></i><b>1.2.6</b> Functions</a></li>
<li class="chapter" data-level="1.2.7" data-path="ch-01.html"><a href="ch-01.html#tidy-data"><i class="fa fa-check"></i><b>1.2.7</b> Tidy data</a></li>
<li class="chapter" data-level="1.2.8" data-path="ch-01.html"><a href="ch-01.html#r-projects"><i class="fa fa-check"></i><b>1.2.8</b> R projects</a></li>
<li class="chapter" data-level="1.2.9" data-path="ch-01.html"><a href="ch-01.html#working-with-data-frames"><i class="fa fa-check"></i><b>1.2.9</b> Working with data frames</a></li>
<li class="chapter" data-level="1.2.10" data-path="ch-01.html"><a href="ch-01.html#r-objects"><i class="fa fa-check"></i><b>1.2.10</b> R objects</a></li>
<li class="chapter" data-level="1.2.11" data-path="ch-01.html"><a href="ch-01.html#data-visualisation"><i class="fa fa-check"></i><b>1.2.11</b> Data visualisation</a></li>
<li class="chapter" data-level="1.2.12" data-path="ch-01.html"><a href="ch-01.html#conditionals"><i class="fa fa-check"></i><b>1.2.12</b> Conditionals</a></li>
<li class="chapter" data-level="1.2.13" data-path="ch-01.html"><a href="ch-01.html#loops"><i class="fa fa-check"></i><b>1.2.13</b> Loops</a></li>
<li class="chapter" data-level="1.2.14" data-path="ch-01.html"><a href="ch-01.html#where-to-find-help"><i class="fa fa-check"></i><b>1.2.14</b> Where to find help</a></li>
<li class="chapter" data-level="1.2.15" data-path="ch-01.html"><a href="ch-01.html#key-points-from-the-tutorial"><i class="fa fa-check"></i><b>1.2.15</b> Key points from the tutorial</a></li>
<li class="chapter" data-level="1.2.16" data-path="ch-01.html"><a href="ch-01.html#further-reading"><i class="fa fa-check"></i><b>1.2.16</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ch-01.html"><a href="ch-01.html#exercise"><i class="fa fa-check"></i><b>1.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-02.html"><a href="ch-02.html"><i class="fa fa-check"></i><b>2</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-02.html"><a href="ch-02.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ch-02.html"><a href="ch-02.html#learning-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch-02.html"><a href="ch-02.html#key-points-from-the-lecture-1"><i class="fa fa-check"></i><b>2.1.2</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch-02.html"><a href="ch-02.html#tutorial-1"><i class="fa fa-check"></i><b>2.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-02.html"><a href="ch-02.html#libraries-1"><i class="fa fa-check"></i><b>2.2.1</b> Libraries</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-02.html"><a href="ch-02.html#variables-in-a-data-frame"><i class="fa fa-check"></i><b>2.2.2</b> Variables in a data frame</a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-02.html"><a href="ch-02.html#time-objects"><i class="fa fa-check"></i><b>2.2.3</b> Time objects</a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-02.html"><a href="ch-02.html#variable-re--definition"><i class="fa fa-check"></i><b>2.2.4</b> Variable (re-) definition</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-02.html"><a href="ch-02.html#selecting-cleaning-and-gap-filling"><i class="fa fa-check"></i><b>2.2.5</b> Selecting, cleaning and gap-filling</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-02.html"><a href="ch-02.html#functions-1"><i class="fa fa-check"></i><b>2.2.6</b> Functions</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-02.html"><a href="ch-02.html#data-overview"><i class="fa fa-check"></i><b>2.2.7</b> Data overview</a></li>
<li class="chapter" data-level="2.2.8" data-path="ch-02.html"><a href="ch-02.html#data-visualisation-i"><i class="fa fa-check"></i><b>2.2.8</b> Data visualisation I</a></li>
<li class="chapter" data-level="2.2.9" data-path="ch-02.html"><a href="ch-02.html#aggregating"><i class="fa fa-check"></i><b>2.2.9</b> Aggregating</a></li>
<li class="chapter" data-level="2.2.10" data-path="ch-02.html"><a href="ch-02.html#data-visualisation-ii"><i class="fa fa-check"></i><b>2.2.10</b> Data visualisation II</a></li>
<li class="chapter" data-level="2.2.11" data-path="ch-02.html"><a href="ch-02.html#functional-programming-i"><i class="fa fa-check"></i><b>2.2.11</b> Functional programming I</a></li>
<li class="chapter" data-level="2.2.12" data-path="ch-02.html"><a href="ch-02.html#strings"><i class="fa fa-check"></i><b>2.2.12</b> Strings</a></li>
<li class="chapter" data-level="2.2.13" data-path="ch-02.html"><a href="ch-02.html#combining-relational-data"><i class="fa fa-check"></i><b>2.2.13</b> Combining relational data</a></li>
<li class="chapter" data-level="2.2.14" data-path="ch-02.html"><a href="ch-02.html#key-points-from-the-tutorial-1"><i class="fa fa-check"></i><b>2.2.14</b> Key points from the tutorial</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-02.html"><a href="ch-02.html#exercise-1"><i class="fa fa-check"></i><b>2.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-03.html"><a href="ch-03.html"><i class="fa fa-check"></i><b>3</b> Data variety</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-03.html"><a href="ch-03.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-03.html"><a href="ch-03.html#overview"><i class="fa fa-check"></i><b>3.1.1</b> Overview</a></li>
<li class="chapter" data-level="3.1.2" data-path="ch-03.html"><a href="ch-03.html#learning-objectives-1"><i class="fa fa-check"></i><b>3.1.2</b> Learning objectives</a></li>
<li class="chapter" data-level="3.1.3" data-path="ch-03.html"><a href="ch-03.html#key-points-from-the-lecture-2"><i class="fa fa-check"></i><b>3.1.3</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-03.html"><a href="ch-03.html#tutorial-2"><i class="fa fa-check"></i><b>3.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-03.html"><a href="ch-03.html#overview-1"><i class="fa fa-check"></i><b>3.2.1</b> Overview</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-03.html"><a href="ch-03.html#modis-remote-download"><i class="fa fa-check"></i><b>3.2.2</b> MODIS remote download</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-03.html"><a href="ch-03.html#points-on-the-globe"><i class="fa fa-check"></i><b>3.2.3</b> Points on the globe</a></li>
<li class="chapter" data-level="3.2.4" data-path="ch-03.html"><a href="ch-03.html#shapefiles"><i class="fa fa-check"></i><b>3.2.4</b> Shapefiles</a></li>
<li class="chapter" data-level="3.2.5" data-path="ch-03.html"><a href="ch-03.html#rasters"><i class="fa fa-check"></i><b>3.2.5</b> Rasters</a></li>
<li class="chapter" data-level="3.2.6" data-path="ch-03.html"><a href="ch-03.html#key-points-from-the-tutorial-2"><i class="fa fa-check"></i><b>3.2.6</b> Key points from the tutorial</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-03.html"><a href="ch-03.html#exercise-2"><i class="fa fa-check"></i><b>3.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html"><i class="fa fa-check"></i><b>4</b> Code version control with git</a>
<ul>
<li class="chapter" data-level="4.1" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#the-origins-of-the-software---a-short-history"><i class="fa fa-check"></i><b>4.1.1</b> The origins of the software - a short history</a></li>
<li class="chapter" data-level="4.1.2" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#the-scientific-use-case"><i class="fa fa-check"></i><b>4.1.2</b> The (scientific) use case</a></li>
<li class="chapter" data-level="4.1.3" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#conflict-free-collaboration---a-tree-like-project-structure"><i class="fa fa-check"></i><b>4.1.3</b> Conflict free collaboration - a tree-like project structure</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#git---a-research-workflow"><i class="fa fa-check"></i><b>4.2</b> Git - a research workflow</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#creating-a-local-repository"><i class="fa fa-check"></i><b>4.2.1</b> Creating a local repository</a></li>
<li class="chapter" data-level="4.2.2" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#creating-a-repository-from-a-remote-github-repository"><i class="fa fa-check"></i><b>4.2.2</b> Creating a repository from a remote (github) repository</a></li>
<li class="chapter" data-level="4.2.3" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#updating-a-repository-locally"><i class="fa fa-check"></i><b>4.2.3</b> Updating a repository (locally)</a></li>
<li class="chapter" data-level="4.2.4" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#branches-structuring-local-repository-changes"><i class="fa fa-check"></i><b>4.2.4</b> Branches &amp; structuring (local) repository changes</a></li>
<li class="chapter" data-level="4.2.5" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#integrating-branches"><i class="fa fa-check"></i><b>4.2.5</b> Integrating branches</a></li>
<li class="chapter" data-level="4.2.6" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#a-github-workflow"><i class="fa fa-check"></i><b>4.2.6</b> A git(hub) workflow</a></li>
<li class="chapter" data-level="4.2.7" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#managing-github-projects"><i class="fa fa-check"></i><b>4.2.7</b> Managing git(hub) projects</a></li>
<li class="chapter" data-level="4.2.8" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#best-git-workflow-practice-in-science"><i class="fa fa-check"></i><b>4.2.8</b> Best git workflow practice in science</a></li>
<li class="chapter" data-level="4.2.9" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#binary-files"><i class="fa fa-check"></i><b>4.2.9</b> Binary files</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#r-rstudio-git-integration"><i class="fa fa-check"></i><b>4.3</b> R &amp; RStudio git integration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#creating-a-git-enabled-r-project"><i class="fa fa-check"></i><b>4.3.1</b> creating a git enabled R project</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#github-side-benefits"><i class="fa fa-check"></i><b>4.4</b> Github side benefits</a></li>
<li class="chapter" data-level="4.5" data-path="code-version-control-with-git.html"><a href="code-version-control-with-git.html#references"><i class="fa fa-check"></i><b>4.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="open-science.html"><a href="open-science.html"><i class="fa fa-check"></i><b>5</b> Open Science</a>
<ul>
<li class="chapter" data-level="5.1" data-path="open-science.html"><a href="open-science.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="open-science.html"><a href="open-science.html#why-should-i-care"><i class="fa fa-check"></i><b>5.1.1</b> Why should I care?</a></li>
<li class="chapter" data-level="5.1.2" data-path="open-science.html"><a href="open-science.html#some-history"><i class="fa fa-check"></i><b>5.1.2</b> Some history</a></li>
<li class="chapter" data-level="5.1.3" data-path="open-science.html"><a href="open-science.html#why-is-it-important"><i class="fa fa-check"></i><b>5.1.3</b> Why is it important?</a></li>
<li class="chapter" data-level="5.1.4" data-path="open-science.html"><a href="open-science.html#obstacles-to-open-science"><i class="fa fa-check"></i><b>5.1.4</b> Obstacles to Open Science</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="open-science.html"><a href="open-science.html#data-resources"><i class="fa fa-check"></i><b>5.2</b> Data resources</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="open-science.html"><a href="open-science.html#environmental-data-repositories"><i class="fa fa-check"></i><b>5.2.1</b> Environmental data repositories</a></li>
<li class="chapter" data-level="5.2.2" data-path="open-science.html"><a href="open-science.html#some-of-our-favourites"><i class="fa fa-check"></i><b>5.2.2</b> Some of our favourites</a></li>
<li class="chapter" data-level="5.2.3" data-path="open-science.html"><a href="open-science.html#general-permanent-data-repositories"><i class="fa fa-check"></i><b>5.2.3</b> General permanent data repositories</a></li>
<li class="chapter" data-level="5.2.4" data-path="open-science.html"><a href="open-science.html#software-for-data-interfacing"><i class="fa fa-check"></i><b>5.2.4</b> Software for data interfacing</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="open-science.html"><a href="open-science.html#open-science-practice"><i class="fa fa-check"></i><b>5.3</b> Open science practice</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="open-science.html"><a href="open-science.html#organizing-the-working-environment"><i class="fa fa-check"></i><b>5.3.1</b> Organizing the working environment</a></li>
<li class="chapter" data-level="5.3.2" data-path="open-science.html"><a href="open-science.html#where-does-the-data-go"><i class="fa fa-check"></i><b>5.3.2</b> Where does the data go?</a></li>
<li class="chapter" data-level="5.3.3" data-path="open-science.html"><a href="open-science.html#reproducibility"><i class="fa fa-check"></i><b>5.3.3</b> Reproducibility</a></li>
<li class="chapter" data-level="5.3.4" data-path="open-science.html"><a href="open-science.html#a-quick-note-on-.bash_profile"><i class="fa fa-check"></i><b>5.3.4</b> A quick note on <em>.bash_profile</em></a></li>
<li class="chapter" data-level="5.3.5" data-path="open-science.html"><a href="open-science.html#best-practice-notes"><i class="fa fa-check"></i><b>5.3.5</b> Best practice notes</a></li>
<li class="chapter" data-level="5.3.6" data-path="open-science.html"><a href="open-science.html#in-a-nutshell"><i class="fa fa-check"></i><b>5.3.6</b> In a nutshell</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="open-science.html"><a href="open-science.html#an-open-science-workflow"><i class="fa fa-check"></i><b>5.4</b> An open science workflow</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="open-science.html"><a href="open-science.html#obtain-data"><i class="fa fa-check"></i><b>5.4.1</b> Obtain data</a></li>
<li class="chapter" data-level="5.4.2" data-path="open-science.html"><a href="open-science.html#document-data"><i class="fa fa-check"></i><b>5.4.2</b> Document data</a></li>
<li class="chapter" data-level="5.4.3" data-path="open-science.html"><a href="open-science.html#organise-analysis-code-in-a-rstudio-project"><i class="fa fa-check"></i><b>5.4.3</b> Organise analysis code in a Rstudio project</a></li>
<li class="chapter" data-level="5.4.4" data-path="open-science.html"><a href="open-science.html#document-and-communicate-analysis"><i class="fa fa-check"></i><b>5.4.4</b> Document and communicate analysis</a></li>
<li class="chapter" data-level="5.4.5" data-path="open-science.html"><a href="open-science.html#sync-outputs"><i class="fa fa-check"></i><b>5.4.5</b> Sync outputs</a></li>
<li class="chapter" data-level="5.4.6" data-path="open-science.html"><a href="open-science.html#code-versioning-and-publication"><i class="fa fa-check"></i><b>5.4.6</b> Code versioning and publication</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="open-science.html"><a href="open-science.html#remaining-bits"><i class="fa fa-check"></i><b>5.5</b> Remaining bits</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="open-science.html"><a href="open-science.html#getting-public-data"><i class="fa fa-check"></i><b>5.5.1</b> Getting public data</a></li>
<li class="chapter" data-level="5.5.2" data-path="open-science.html"><a href="open-science.html#git-attributes"><i class="fa fa-check"></i><b>5.5.2</b> Git Attributes</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="open-science.html"><a href="open-science.html#references-and-further-reading"><i class="fa fa-check"></i><b>5.6</b> References and further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-05.html"><a href="ch-05.html"><i class="fa fa-check"></i><b>6</b> Catch-up</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-05.html"><a href="ch-05.html#loops-in-r"><i class="fa fa-check"></i><b>6.1</b> Loops in R</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-05.html"><a href="ch-05.html#some-simple-examples"><i class="fa fa-check"></i><b>6.1.1</b> Some simple examples</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-05.html"><a href="ch-05.html#nested-loops"><i class="fa fa-check"></i><b>6.1.2</b> Nested loops</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-05.html"><a href="ch-05.html#exercise-3"><i class="fa fa-check"></i><b>6.1.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-05.html"><a href="ch-05.html#functional-programming-using-purr"><i class="fa fa-check"></i><b>6.2</b> Functional programming using purr</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-05.html"><a href="ch-05.html#shortcuts-in-a-purrr-function"><i class="fa fa-check"></i><b>6.2.1</b> Shortcuts in a <code>purrr</code> function</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-05.html"><a href="ch-05.html#workflow-nested-data-map-and-mutate"><i class="fa fa-check"></i><b>6.2.2</b> Workflow: nested data, map and mutate</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-05.html"><a href="ch-05.html#string-manipulations"><i class="fa fa-check"></i><b>6.3</b> String Manipulations</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-05.html"><a href="ch-05.html#introduction-to-strings"><i class="fa fa-check"></i><b>6.3.1</b> Introduction to strings</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-05.html"><a href="ch-05.html#matching-and-extracting-patterns"><i class="fa fa-check"></i><b>6.3.2</b> Matching and extracting patterns</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-05.html"><a href="ch-05.html#advanced-example"><i class="fa fa-check"></i><b>6.3.3</b> Advanced example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-05.html"><a href="ch-05.html#web-scraping-in-a-nut-shell"><i class="fa fa-check"></i><b>6.4</b> Web-scraping in a nut-shell</a></li>
<li class="chapter" data-level="6.5" data-path="ch-05.html"><a href="ch-05.html#tidyverses-filter-and-select"><i class="fa fa-check"></i><b>6.5</b> Tidyverse’s filter and select</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ch-05.html"><a href="ch-05.html#introduction-4"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="ch-05.html"><a href="ch-05.html#select"><i class="fa fa-check"></i><b>6.5.2</b> Select()</a></li>
<li class="chapter" data-level="6.5.3" data-path="ch-05.html"><a href="ch-05.html#filter"><i class="fa fa-check"></i><b>6.5.3</b> Filter()</a></li>
<li class="chapter" data-level="6.5.4" data-path="ch-05.html"><a href="ch-05.html#exercises"><i class="fa fa-check"></i><b>6.5.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5.5" data-path="ch-05.html"><a href="ch-05.html#solutions"><i class="fa fa-check"></i><b>6.5.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ch-05.html"><a href="ch-05.html#preparing-data-for-ggplot"><i class="fa fa-check"></i><b>6.6</b> Preparing data for ggplot()</a></li>
<li class="chapter" data-level="6.7" data-path="ch-05.html"><a href="ch-05.html#base-r-functions"><i class="fa fa-check"></i><b>6.7</b> Base R functions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-06.html"><a href="ch-06.html"><i class="fa fa-check"></i><b>7</b> Supervised Machine Learning I</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-06.html"><a href="ch-06.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-06.html"><a href="ch-06.html#learning-objectives-2"><i class="fa fa-check"></i><b>7.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-06.html"><a href="ch-06.html#important-points-from-the-lecture"><i class="fa fa-check"></i><b>7.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-06.html"><a href="ch-06.html#tutorial-3"><i class="fa fa-check"></i><b>7.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-06.html"><a href="ch-06.html#overfitting"><i class="fa fa-check"></i><b>7.2.1</b> Overfitting</a></li>
<li class="chapter" data-level="7.2.2" data-path="open-science.html"><a href="open-science.html#motivation"><i class="fa fa-check"></i><b>7.2.2</b> Modelling challenge</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-06.html"><a href="ch-06.html#a-selection-of-model-types"><i class="fa fa-check"></i><b>7.2.3</b> A selection of model types</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-06.html"><a href="ch-06.html#data-splitting"><i class="fa fa-check"></i><b>7.2.4</b> Data splitting</a></li>
<li class="chapter" data-level="7.2.5" data-path="ch-06.html"><a href="ch-06.html#preprocessing"><i class="fa fa-check"></i><b>7.2.5</b> Pre-processing</a></li>
<li class="chapter" data-level="7.2.6" data-path="ch-06.html"><a href="ch-06.html#model-formulation"><i class="fa fa-check"></i><b>7.2.6</b> Model formulation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-06.html"><a href="ch-06.html#exercise-4"><i class="fa fa-check"></i><b>7.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ch-06.html"><a href="ch-06.html#reading-and-cleaning"><i class="fa fa-check"></i><b>7.3.1</b> Reading and cleaning</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-06.html"><a href="ch-06.html#data-splitting-1"><i class="fa fa-check"></i><b>7.3.2</b> Data splitting</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-06.html"><a href="ch-06.html#linear-model"><i class="fa fa-check"></i><b>7.3.3</b> Linear model</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-06.html"><a href="ch-06.html#pre-processing"><i class="fa fa-check"></i><b>7.3.4</b> Pre-processing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-07.html"><a href="ch-07.html"><i class="fa fa-check"></i><b>8</b> Supervised Machine Learning II</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-07.html"><a href="ch-07.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ch-07.html"><a href="ch-07.html#learning-objectives-3"><i class="fa fa-check"></i><b>8.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.1.2" data-path="ch-07.html"><a href="ch-07.html#key-points-from-the-lecture-3"><i class="fa fa-check"></i><b>8.1.2</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ch-07.html"><a href="ch-07.html#tutorial-4"><i class="fa fa-check"></i><b>8.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-07.html"><a href="ch-07.html#model-formulation-using-train"><i class="fa fa-check"></i><b>8.2.1</b> Model formulation using <code>train()</code></a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-07.html"><a href="ch-07.html#model-training"><i class="fa fa-check"></i><b>8.2.2</b> Model training</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-07.html"><a href="ch-07.html#model-evaluation"><i class="fa fa-check"></i><b>8.2.3</b> Model evaluation</a></li>
<li class="chapter" data-level="8.2.4" data-path="ch-07.html"><a href="ch-07.html#bonus-model-interpretation"><i class="fa fa-check"></i><b>8.2.4</b> Bonus: Model interpretation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ch-07.html"><a href="ch-07.html#exercise-5"><i class="fa fa-check"></i><b>8.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ch-07.html"><a href="ch-07.html#setup"><i class="fa fa-check"></i><b>8.3.1</b> Setup</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-07.html"><a href="ch-07.html#linear-model-1"><i class="fa fa-check"></i><b>8.3.2</b> Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-07.html"><a href="ch-07.html#knn"><i class="fa fa-check"></i><b>8.4</b> KNN</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ch-07.html"><a href="ch-07.html#check-data"><i class="fa fa-check"></i><b>8.4.1</b> Check data</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-07.html"><a href="ch-07.html#training-1"><i class="fa fa-check"></i><b>8.4.2</b> Training</a></li>
<li class="chapter" data-level="8.4.3" data-path="ch-07.html"><a href="ch-07.html#prediction-1"><i class="fa fa-check"></i><b>8.4.3</b> Prediction</a></li>
<li class="chapter" data-level="8.4.4" data-path="ch-07.html"><a href="ch-07.html#sample-hyperparameters"><i class="fa fa-check"></i><b>8.4.4</b> Sample hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-07.html"><a href="ch-07.html#random-forest-1"><i class="fa fa-check"></i><b>8.5</b> Random forest</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ch-07.html"><a href="ch-07.html#training-2"><i class="fa fa-check"></i><b>8.5.1</b> Training</a></li>
<li class="chapter" data-level="8.5.2" data-path="ch-07.html"><a href="ch-07.html#prediction-2"><i class="fa fa-check"></i><b>8.5.2</b> Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-08.html"><a href="ch-08.html"><i class="fa fa-check"></i><b>9</b> Application 1: Variable selection</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-08.html"><a href="ch-08.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="ch-08.html"><a href="ch-08.html#application"><i class="fa fa-check"></i><b>9.2</b> Application</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-08.html"><a href="ch-08.html#warm-up-1-nested-for-loop"><i class="fa fa-check"></i><b>9.2.1</b> Warm-up 1: Nested for-loop</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-08.html"><a href="ch-08.html#warm-up-2-find-the-best-single-predictor"><i class="fa fa-check"></i><b>9.2.2</b> Warm-up 2: Find the best single predictor</a></li>
<li class="chapter" data-level="9.2.3" data-path="ch-08.html"><a href="ch-08.html#full-stepwise-regression"><i class="fa fa-check"></i><b>9.2.3</b> Full stepwise regression</a></li>
<li class="chapter" data-level="9.2.4" data-path="ch-08.html"><a href="ch-08.html#bonus-stepwise-regression-out-of-the-box"><i class="fa fa-check"></i><b>9.2.4</b> Bonus: Stepwise regression out-of-the-box</a></li>
<li class="chapter" data-level="9.2.5" data-path="ch-08.html"><a href="ch-08.html#bonus-best-subset-selection---not-included-in-application-for-students-too-long"><i class="fa fa-check"></i><b>9.2.5</b> Bonus: Best Subset Selection - Not included in application for students (too long…)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-09.html"><a href="ch-09.html"><i class="fa fa-check"></i><b>10</b> Supervised Neural Networks I</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-09.html"><a href="ch-09.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch-09.html"><a href="ch-09.html#learning-objectives-4"><i class="fa fa-check"></i><b>10.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.1.2" data-path="ch-09.html"><a href="ch-09.html#important-points-from-the-lecture-1"><i class="fa fa-check"></i><b>10.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-09.html"><a href="ch-09.html#tutorial-5"><i class="fa fa-check"></i><b>10.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-09.html"><a href="ch-09.html#set-up"><i class="fa fa-check"></i><b>10.2.1</b> Set-up</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-09.html"><a href="ch-09.html#keras-for-linear-models"><i class="fa fa-check"></i><b>10.2.2</b> Keras for linear models</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-09.html"><a href="ch-09.html#tuning-learning-rate"><i class="fa fa-check"></i><b>10.2.3</b> Tuning learning rate</a></li>
<li class="chapter" data-level="10.2.4" data-path="ch-09.html"><a href="ch-09.html#logistic-regression"><i class="fa fa-check"></i><b>10.2.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ch-09.html"><a href="ch-09.html#exercise-6"><i class="fa fa-check"></i><b>10.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-10.html"><a href="ch-10.html"><i class="fa fa-check"></i><b>11</b> Supervised Neural Networks II</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-10.html"><a href="ch-10.html#introduction-9"><i class="fa fa-check"></i><b>11.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-10.html"><a href="ch-10.html#learning-objectives-5"><i class="fa fa-check"></i><b>11.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-10.html"><a href="ch-10.html#important-points-from-the-lecture-2"><i class="fa fa-check"></i><b>11.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-10.html"><a href="ch-10.html#tutorial-6"><i class="fa fa-check"></i><b>11.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-10.html"><a href="ch-10.html#import-libraries-1"><i class="fa fa-check"></i><b>11.2.1</b> Import libraries</a></li>
<li class="chapter" data-level="11.2.2" data-path="ch-10.html"><a href="ch-10.html#construct-a-toy-dataset"><i class="fa fa-check"></i><b>11.2.2</b> Construct a toy dataset</a></li>
<li class="chapter" data-level="11.2.3" data-path="ch-10.html"><a href="ch-10.html#build-and-train-nn"><i class="fa fa-check"></i><b>11.2.3</b> Build and train NN</a></li>
<li class="chapter" data-level="11.2.4" data-path="ch-10.html"><a href="ch-10.html#model-performance"><i class="fa fa-check"></i><b>11.2.4</b> Model performance</a></li>
<li class="chapter" data-level="11.2.5" data-path="ch-10.html"><a href="ch-10.html#influence-of-nn-architecture"><i class="fa fa-check"></i><b>11.2.5</b> Influence of NN architecture</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-10.html"><a href="ch-10.html#exercise-7"><i class="fa fa-check"></i><b>11.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-11.html"><a href="ch-11.html"><i class="fa fa-check"></i><b>12</b> Application 2: Neural Networks and Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-11.html"><a href="ch-11.html#introduction-10"><i class="fa fa-check"></i><b>12.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-11.html"><a href="ch-11.html#learning-goals"><i class="fa fa-check"></i><b>12.1.1</b> Learning Goals</a></li>
<li class="chapter" data-level="12.1.2" data-path="ch-11.html"><a href="ch-11.html#key-points-from-previous-lectures"><i class="fa fa-check"></i><b>12.1.2</b> Key Points from Previous Lectures</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-11.html"><a href="ch-11.html#application-1"><i class="fa fa-check"></i><b>12.2</b> Application</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-11.html"><a href="ch-11.html#problem-statement"><i class="fa fa-check"></i><b>12.2.1</b> Problem Statement</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-11.html"><a href="ch-11.html#data-preparation"><i class="fa fa-check"></i><b>12.2.2</b> Data preparation</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-11.html"><a href="ch-11.html#center-and-scale"><i class="fa fa-check"></i><b>12.2.3</b> Center and scale</a></li>
<li class="chapter" data-level="12.2.4" data-path="ch-11.html"><a href="ch-11.html#building-a-simple-model-with-keras-subtask-1"><i class="fa fa-check"></i><b>12.2.4</b> Building a simple model with keras ( SubTask 1)</a></li>
<li class="chapter" data-level="12.2.5" data-path="ch-11.html"><a href="ch-11.html#cross-validation"><i class="fa fa-check"></i><b>12.2.5</b> Cross validation</a></li>
<li class="chapter" data-level="12.2.6" data-path="ch-11.html"><a href="ch-11.html#parameter-tuning"><i class="fa fa-check"></i><b>12.2.6</b> Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-12.html"><a href="ch-12.html"><i class="fa fa-check"></i><b>13</b> Supervised Deep Learning I</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-12.html"><a href="ch-12.html#introduction-11"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-12.html"><a href="ch-12.html#learning-objectives-6"><i class="fa fa-check"></i><b>13.1.1</b> Learning objectives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-12.html"><a href="ch-12.html#tutorial-7"><i class="fa fa-check"></i><b>13.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-12.html"><a href="ch-12.html#building-blocks-of-cnns"><i class="fa fa-check"></i><b>13.2.1</b> Building Blocks of CNNs</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-12.html"><a href="ch-12.html#build-the-model"><i class="fa fa-check"></i><b>13.2.2</b> Build the model</a></li>
<li class="chapter" data-level="13.2.3" data-path="ch-12.html"><a href="ch-12.html#compile-and-train-the-model"><i class="fa fa-check"></i><b>13.2.3</b> Compile and train the model</a></li>
<li class="chapter" data-level="13.2.4" data-path="ch-12.html"><a href="ch-12.html#reduce-overfitting"><i class="fa fa-check"></i><b>13.2.4</b> Reduce Overfitting</a></li>
<li class="chapter" data-level="13.2.5" data-path="ch-12.html"><a href="ch-12.html#visualizing-a-cnn"><i class="fa fa-check"></i><b>13.2.5</b> Visualizing a CNN</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-12.html"><a href="ch-12.html#exercise-8"><i class="fa fa-check"></i><b>13.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ch-12.html"><a href="ch-12.html#import-libraries-and-data"><i class="fa fa-check"></i><b>13.3.1</b> Import libraries and data</a></li>
<li class="chapter" data-level="13.3.2" data-path="ch-12.html"><a href="ch-12.html#tasks"><i class="fa fa-check"></i><b>13.3.2</b> Tasks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-13.html"><a href="ch-13.html"><i class="fa fa-check"></i><b>14</b> Supervised Deep Learning II - <em>A Scenario of Environmental Systems</em></a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-13.html"><a href="ch-13.html#introduction-12"><i class="fa fa-check"></i><b>14.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="ch-13.html"><a href="ch-13.html#learning-objectives-7"><i class="fa fa-check"></i><b>14.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="14.1.2" data-path="ch-13.html"><a href="ch-13.html#content-operations"><i class="fa fa-check"></i><b>14.1.2</b> Content &amp; Operations</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ch-13.html"><a href="ch-13.html#tutorial-8"><i class="fa fa-check"></i><b>14.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ch-13.html"><a href="ch-13.html#description-of-the-modelling-task"><i class="fa fa-check"></i><b>14.2.1</b> Description of the modelling task</a></li>
<li class="chapter" data-level="14.2.2" data-path="ch-13.html"><a href="ch-13.html#goal-of-the-tutorial"><i class="fa fa-check"></i><b>14.2.2</b> Goal of the tutorial</a></li>
<li class="chapter" data-level="14.2.3" data-path="ch-13.html"><a href="ch-13.html#dataset"><i class="fa fa-check"></i><b>14.2.3</b> Dataset</a></li>
<li class="chapter" data-level="14.2.4" data-path="ch-13.html"><a href="ch-13.html#lets-dive-into-the-code"><i class="fa fa-check"></i><b>14.2.4</b> Let’s dive into the code</a></li>
<li class="chapter" data-level="14.2.5" data-path="ch-13.html"><a href="ch-13.html#read-in-the-data"><i class="fa fa-check"></i><b>14.2.5</b> Read in the Data</a></li>
<li class="chapter" data-level="14.2.6" data-path="ch-13.html"><a href="ch-13.html#naive-models-old-world-linear-models"><i class="fa fa-check"></i><b>14.2.6</b> Naive models (Old World): Linear Models</a></li>
<li class="chapter" data-level="14.2.7" data-path="ch-13.html"><a href="ch-13.html#neural-networks-new-world"><i class="fa fa-check"></i><b>14.2.7</b> Neural Networks (New World)</a></li>
<li class="chapter" data-level="14.2.8" data-path="ch-13.html"><a href="ch-13.html#convolutional-neural-network"><i class="fa fa-check"></i><b>14.2.8</b> Convolutional Neural Network</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ch-13.html"><a href="ch-13.html#exercise-9"><i class="fa fa-check"></i><b>14.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Environmental Systems Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-07" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Supervised Machine Learning II</h1>
<div id="introduction-6" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Introduction</h2>
<div id="learning-objectives-3" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Learning objectives</h3>
<p>After this learning unit, you will be able to …</p>
<ul>
<li>Explain the effect of hyper-parameter tuning.</li>
<li>Assess the generalisability of a trained model.</li>
<li>Understand the purpose of resampling.</li>
<li>Avoid data leakage during model training.</li>
</ul>
</div>
<div id="key-points-from-the-lecture-3" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Key points from the lecture</h3>
<p><strong>Training data</strong> is the data that is used to train our model - “to fit
a curve to the data points.”</p>
<p><strong>Testing data</strong> is set aside at the initial split and not “touched”
during model training. It is key to test a model’s predictive power and
whether it is overfitted.</p>
<p><strong>Validation data</strong> is used for determining the loss during model
training. The reason for distinguishing between testing and validation
data is to assure we’re not misleading model training by some
peculiarities of the training data and we get an assessment of
generalisability based on data that was not seen during model training.
This distinction might be somewhat confusing for now, have a look at
this <a href="https://machinelearningmastery.com/difference-test-validation-datasets/">blog
post</a>
for additional explanations.</p>
<p><strong>Model training</strong> minimises the loss. In other words, it optimises the
agreement between predicted and observed values. The loss is most
commonly measured by the root mean square error (RMSE).</p>
<p>To tune a model, you can set <em>hyperparameters</em> that determine model
structure or calibrate the coefficients. The <em>k</em> in KNN is such a
hyperparameter.</p>
<p><strong>Generalisability</strong> refers to the model’s performance on data not seen
during the training - the testing data. To avoid overfitting, model
generalisability is desired already during model training. One method to
guide model training is <strong>cross validation</strong>.</p>
<p><strong>Data leakage</strong> is when information from the testing dataset “creeps”
into the training data. To avoid this the testing set must be left
completely untouched!</p>
</div>
</div>
<div id="tutorial-4" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Tutorial</h2>
<div id="model-formulation-using-train" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Model formulation using <code>train()</code></h3>
<p>In the previous tutorial you learned how to wrangle, split and
pre-process data (remember the recipes) and how to formulate a linear
model in base R. To learn more about model creation, let us first repeat
the data preparation from the previous tutorial:</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb593-1"><a href="ch-07.html#cb593-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Load data and extrat relevant subset</span></span>
<span id="cb593-2"><a href="ch-07.html#cb593-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb593-3"><a href="ch-07.html#cb593-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb593-4"><a href="ch-07.html#cb593-4" aria-hidden="true" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb593-5"><a href="ch-07.html#cb593-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">starts_with</span>(<span class="st">&quot;TIMESTAMP&quot;</span>),</span>
<span id="cb593-6"><a href="ch-07.html#cb593-6" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ends_with</span>(<span class="st">&quot;_F&quot;</span>),</span>
<span id="cb593-7"><a href="ch-07.html#cb593-7" aria-hidden="true" tabindex="-1"></a>         GPP_NT_VUT_REF,</span>
<span id="cb593-8"><a href="ch-07.html#cb593-8" aria-hidden="true" tabindex="-1"></a>         NEE_VUT_REF_QC,</span>
<span id="cb593-9"><a href="ch-07.html#cb593-9" aria-hidden="true" tabindex="-1"></a>         <span class="sc">-</span><span class="fu">contains</span>(<span class="st">&quot;JSB&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb593-10"><a href="ch-07.html#cb593-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">TIMESTAMP =</span> lubridate<span class="sc">::</span><span class="fu">ymd</span>(TIMESTAMP)) <span class="sc">%&gt;%</span></span>
<span id="cb593-11"><a href="ch-07.html#cb593-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na_if</span>(<span class="sc">-</span><span class="dv">9999</span>) <span class="sc">%&gt;%</span></span>
<span id="cb593-12"><a href="ch-07.html#cb593-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="fu">ends_with</span>(<span class="st">&quot;_QC&quot;</span>), NEE_VUT_REF_QC) <span class="sc">%&gt;%</span> </span>
<span id="cb593-13"><a href="ch-07.html#cb593-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">GPP_NT_VUT_REF =</span> <span class="fu">ifelse</span>(NEE_VUT_REF_QC <span class="sc">&lt;</span> <span class="fl">0.9</span>, <span class="cn">NA</span>, GPP_NT_VUT_REF)) <span class="sc">%&gt;%</span> </span>
<span id="cb593-14"><a href="ch-07.html#cb593-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>NEE_VUT_REF_QC) <span class="sc">%&gt;%</span> </span>
<span id="cb593-15"><a href="ch-07.html#cb593-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">drop_na</span>()</span>
<span id="cb593-16"><a href="ch-07.html#cb593-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb593-17"><a href="ch-07.html#cb593-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Split data into testing and training data</span></span>
<span id="cb593-18"><a href="ch-07.html#cb593-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rsample)</span>
<span id="cb593-19"><a href="ch-07.html#cb593-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb593-20"><a href="ch-07.html#cb593-20" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1982</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb593-21"><a href="ch-07.html#cb593-21" aria-hidden="true" tabindex="-1"></a>split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ddf, <span class="at">prop =</span> <span class="fl">0.7</span>)</span>
<span id="cb593-22"><a href="ch-07.html#cb593-22" aria-hidden="true" tabindex="-1"></a>ddf_train <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb593-23"><a href="ch-07.html#cb593-23" aria-hidden="true" tabindex="-1"></a>ddf_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span>
<span id="cb593-24"><a href="ch-07.html#cb593-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb593-25"><a href="ch-07.html#cb593-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Base R formulation for linear model</span></span>
<span id="cb593-26"><a href="ch-07.html#cb593-26" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, <span class="at">data =</span> ddf)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf)
## 
## Coefficients:
## (Intercept)      SW_IN_F        VPD_F         TA_F  
##     1.08298      0.01334     -0.34573      0.30404</code></pre>
<p>Ideally, the way we formulate a model should be independent of the
algorithm, or <em>engine</em> that takes care of fitting <span class="math inline">\(f(X)\)</span>. As mentioned
in the previous tutorial, the R package
<a href="https://topepo.github.io/caret/"><strong>caret</strong></a> provides a unified
interface for using different ML algorithms implemented in separate
packages. In other words, <strong>caret</strong> acts as a <em>wrapper</em> for multiple
different model fitting, or ML algorithms. This has the advantage that
it unifies the interface (the way arguments are provided). <strong>caret</strong>
also provides implementations for a set of commonly used tools for data
processing, model training, and evaluation. We’ll use <strong>caret</strong> for
model training with the function <code>train()</code>. Using caret for specifying
the same linear regression model as above, using the base-R <code>lm()</code>
function, can be done with caret in a generalized form as:</p>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb595-1"><a href="ch-07.html#cb595-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb595-2"><a href="ch-07.html#cb595-2" aria-hidden="true" tabindex="-1"></a><span class="fu">train</span>(</span>
<span id="cb595-3"><a href="ch-07.html#cb595-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">form =</span> GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, </span>
<span id="cb595-4"><a href="ch-07.html#cb595-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> ddf, </span>
<span id="cb595-5"><a href="ch-07.html#cb595-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;lm&quot;</span></span>
<span id="cb595-6"><a href="ch-07.html#cb595-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 5893 samples
##    3 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 5893, 5893, 5893, 5893, 5893, 5893, ... 
## Resampling results:
## 
##   RMSE     Rsquared  MAE     
##   1.58713  0.668969  1.228773
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>Of course, this is an overkill compared to just writing <code>lm(...)</code>. But
the advantage of the unified interface is that we can simply replace the
<code>method</code> argument to use a different ML algorithm. For example, to use a
random forest model implemented by the <strong>ranger</strong> package, we can write:</p>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="ch-07.html#cb597-1" aria-hidden="true" tabindex="-1"></a><span class="fu">train</span>(</span>
<span id="cb597-2"><a href="ch-07.html#cb597-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">form =</span> GPP_NT_VUT_REF <span class="sc">~</span> SW_F_IN <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, </span>
<span id="cb597-3"><a href="ch-07.html#cb597-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> ddf, </span>
<span id="cb597-4"><a href="ch-07.html#cb597-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>,</span>
<span id="cb597-5"><a href="ch-07.html#cb597-5" aria-hidden="true" tabindex="-1"></a>  ...</span>
<span id="cb597-6"><a href="ch-07.html#cb597-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>The <code>...</code> hints at the fact that there are a few more arguments to be
specified for training a random forest model with ranger. We learn more
about these arguments below in Chapter <a href="ch-06.html#training">7.3.3.1</a>.</p>
<p>Another useful aspect of using <strong>caret</strong> to formulate models is that it
takes recipes from the <strong>recipes</strong> packages as input. This way, we can
make sure that the exact same pre-processing steps are taken for the
training data, testing data and as will be introduced later, the
validation data.</p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="ch-07.html#cb598-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(recipes)</span>
<span id="cb598-2"><a href="ch-07.html#cb598-2" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">recipe</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, <span class="at">data =</span> ddf) <span class="sc">%&gt;%</span> </span>
<span id="cb598-3"><a href="ch-07.html#cb598-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_center</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb598-4"><a href="ch-07.html#cb598-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_scale</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>())</span></code></pre></div>
<p>The object <code>pp</code> can then be supplied to <code>train()</code> as its first argument:</p>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb599-1"><a href="ch-07.html#cb599-1" aria-hidden="true" tabindex="-1"></a><span class="fu">train</span>(</span>
<span id="cb599-2"><a href="ch-07.html#cb599-2" aria-hidden="true" tabindex="-1"></a>  pp, </span>
<span id="cb599-3"><a href="ch-07.html#cb599-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> ddf, </span>
<span id="cb599-4"><a href="ch-07.html#cb599-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>,</span>
<span id="cb599-5"><a href="ch-07.html#cb599-5" aria-hidden="true" tabindex="-1"></a>  ...</span>
<span id="cb599-6"><a href="ch-07.html#cb599-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="model-training" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Model training</h3>
<p>Model training in supervised ML is guided by the match (or mismatch)
between the predicted and observed target variable(s), that is, between
<span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(Y\)</span>. The <em>loss</em> function quantifies this mismatch
(<span class="math inline">\(L(\hat{Y}, Y)\)</span>), and the algorithm takes care of progressively
reducing the loss during model training.</p>
<p>Let’s say the ML model contains two parameters and predictions can be
considered a function of the two (<span class="math inline">\(\hat{Y}(w_1, w_2)\)</span>). <span class="math inline">\(Y\)</span> doesn’t
change as a function of $w_1$ and $w_2$. Thus, the loss function is
effectively a function <span class="math inline">\(L(w_1, w_2)\)</span>. Therefore, we can consider the
model training as a search of the parameter space of the machine
learning model <span class="math inline">\((w_1, w_2)\)</span> to find the minimum of the loss. Common loss
functions are the root mean square error (RMSE), or the mean square
error (MSE), or the mean absolute error (MAE). Loss minimization is a
general feature of ML model training.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-254"></span>
<img src="figures/loss_plane.png" alt="Visualization of a loss function as a plane spanned by the two parameters $w_1$ and $w_2$." width="50%" />
<p class="caption">
Figure 8.1: Visualization of a loss function as a plane spanned by the two parameters <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>.
</p>
</div>
<p>Model training is implemented in R for different algorithms in different
packages. Some algorithms are even implemented by multiple packages
(e.g., <code>nnet</code> and <code>neuralnet</code> for artificial neural networks). Again,
<strong>caret</strong> comes to the rescue as it offers a large selection of ML model
implementations from different packages (see
<a href="https://topepo.github.io/caret/available-models.html">here</a> for an
overview of available models). The <code>train()</code> function takes an argument
<code>metric</code>, which specifies the loss function and defaults to the RMSE for
regression models and to accuracy for classification (see chapter on
metrics below).</p>
<div id="hyperparameter-tuning" class="section level4" number="8.2.2.1">
<h4><span class="header-section-number">8.2.2.1</span> Hyperparameter tuning</h4>
<p>All ML algorithms have some “knobs” to turn in order to achieve
efficient model training and predictive performance. Such “knobs” are
called <em>hyperparameters</em> and their meaning and effect depend on the ML
algorithm.</p>
<p>For k-nearest neighbour (KNN), it is <code>k</code> - the number of neighbours to
consider for determining distances. There is always an optimum <span class="math inline">\(k\)</span>.
Obviously, if <span class="math inline">\(k = n\)</span>, we consider all observations as neighbours and
each prediction is simply the mean of all observed target values <span class="math inline">\(Y\)</span>,
irrespective of the predictor values. This cannot be optimal and such a
model is likely underfit. On the other extreme, with <span class="math inline">\(k = 1\)</span>, the model
will be strongly affected by the noise in the single nearest neighbour
and its generalisability will suffer. This should be reflected in a poor
performance on the validation data.</p>
<p>For random forests from the <strong>ranger</strong> package, hyperparameters are:</p>
<ul>
<li><code>mtry</code>: the number of variables to consider to make decisions, often
taken as <span class="math inline">\(p/3\)</span>, where <span class="math inline">\(p\)</span> is the number of predictors.</li>
<li><code>min.node.size</code>: the number of data points at the “bottom” of each
decision tree</li>
<li><code>splitrule</code>: the function applied to data in each branch of a tree,
used for determining the goodness of a decision</li>
</ul>
<p>Hyperparameters usually have to be “tuned” and their optimal values
depend on the data. Therefore they cannot be known <em>a priori</em> but must
be found on-the-go. In <strong>caret</strong>, hyperparameter tuning is implemented
as part of the <code>train()</code> function. Values of hyperparameters to consider
are to be specified by the argument <code>tuneGrid</code>, which takes a data frame
with column(s) named according to the name(s) of the hyperparameter(s)
and rows for each combination of hyperparameters to consider. To specify
the three hyperparameters of a random forest model would look like this:</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="ch-07.html#cb600-1" aria-hidden="true" tabindex="-1"></a><span class="fu">train</span>(</span>
<span id="cb600-2"><a href="ch-07.html#cb600-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">form =</span> GPP_NT_VUT_REF <span class="sc">~</span> SW_F_IN <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, </span>
<span id="cb600-3"><a href="ch-07.html#cb600-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> ddf, </span>
<span id="cb600-4"><a href="ch-07.html#cb600-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>,</span>
<span id="cb600-5"><a href="ch-07.html#cb600-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>( <span class="at">.mtry =</span> <span class="fu">floor</span>(<span class="dv">6</span> <span class="sc">/</span> <span class="dv">3</span>),</span>
<span id="cb600-6"><a href="ch-07.html#cb600-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">.min.node.size =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">9</span>,<span class="dv">15</span>, <span class="dv">30</span>),</span>
<span id="cb600-7"><a href="ch-07.html#cb600-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">.splitrule =</span> <span class="fu">c</span>(<span class="st">&quot;variance&quot;</span>, <span class="st">&quot;maxstat&quot;</span>)),</span>
<span id="cb600-8"><a href="ch-07.html#cb600-8" aria-hidden="true" tabindex="-1"></a>  ...</span>
<span id="cb600-9"><a href="ch-07.html#cb600-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Here, <code>expand.grid()</code> is used to generate a data frame with all possible
combinations of individual vectors (e.g. min node size) and their
provided values (e.g. 3, 5, 9, 15, 30). For a KNN, it suffices to enter
a dataframe with one vector of numbers as argument to <code>tuneGrid</code> (e.g.,
<code>data.frame(k = c(2, 5, 10, 15, 18, 20, 22, 24, 26, 30, 35, 40, 60, 100))</code>).
The <code>train()</code> function will then create a KNN for each of these numbers
to find the optimal k.</p>
</div>
<div id="resampling" class="section level4" number="8.2.2.2">
<h4><span class="header-section-number">8.2.2.2</span> Resampling</h4>
<p>The goal of model training is to achieve the best possible model
generalisability. That is, the best possible model performance when
predicting to data that was not used for training - the test data.
Resampling mimicks the comparison of predictions to the test data.
Instead of using all training data, the training data is <em>resampled</em>
into a number further splits - pairs of training and <em>validation</em> data.
Model training is then guided by minimising the average loss determined
on each resample of the validation data. Having multiple resamples
(multiple <em>folds</em> of training-validation splits) avoids the loss
minimization from being misguided by random peculiarities in the
training and/or validation data.</p>
<p>A common resampling method is <em>k-fold cross validation</em>, where the
training data is split into <em>k</em> equally sized subsets (<em>folds</em>). Then,
there will be <em>k</em> iterations, where each fold is used for validation
once (while the remaining folds are used for training). An extreme case
is <em>leave-one-out cross validation</em>, where <em>k</em> corresponds to the number
of data points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cv"></span>
<img src="figures/cv.png" alt="Cross-validation resampling (figure from [Boehmke &amp; Greenwell (2019)](https://bradleyboehmke.github.io/HOML/process.html))." width="50%" />
<p class="caption">
Figure 8.2: Cross-validation resampling (figure from <a href="https://bradleyboehmke.github.io/HOML/process.html">Boehmke &amp; Greenwell (2019)</a>).
</p>
</div>
<p>To do a k-fold cross validation during model training in R, we don’t
have to implement the loops around folds ourselves. The resampling
procedure can be specified in the <strong>caret</strong> function <code>train()</code> with the
argument <code>trControl</code>. The object that this argument takes is the output
of a function call to <code>trainControl()</code>. This can be implemented in two
steps. For example, to do a 10-fold cross-validation, we can write:</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="ch-07.html#cb601-1" aria-hidden="true" tabindex="-1"></a><span class="fu">train</span>(</span>
<span id="cb601-2"><a href="ch-07.html#cb601-2" aria-hidden="true" tabindex="-1"></a>  pp, </span>
<span id="cb601-3"><a href="ch-07.html#cb601-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> ddf_train, </span>
<span id="cb601-4"><a href="ch-07.html#cb601-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>,</span>
<span id="cb601-5"><a href="ch-07.html#cb601-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>( <span class="at">.mtry =</span> <span class="fu">floor</span>(<span class="dv">6</span> <span class="sc">/</span> <span class="dv">3</span>),</span>
<span id="cb601-6"><a href="ch-07.html#cb601-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">.min.node.size =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">9</span>,<span class="dv">15</span>, <span class="dv">30</span>),</span>
<span id="cb601-7"><a href="ch-07.html#cb601-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">.splitrule =</span> <span class="fu">c</span>(<span class="st">&quot;variance&quot;</span>, <span class="st">&quot;maxstat&quot;</span>)),</span>
<span id="cb601-8"><a href="ch-07.html#cb601-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb601-9"><a href="ch-07.html#cb601-9" aria-hidden="true" tabindex="-1"></a>  ...</span>
<span id="cb601-10"><a href="ch-07.html#cb601-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>In certain cases, data points stem from different “groups,” and
generalisability across groups is critical. In such cases, data from a
given group must not be used both in the training and validation sets.
Instead, splits should be made along group delineations. The <em>caret</em>
function <code>groupKFold()</code> offers the solution for this case.</p>
</div>
</div>
<div id="model-evaluation" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Model evaluation</h3>
<p>In previous chapters, you have already encountered different metrics
that can be used to quantify the agreement between predicted (<span class="math inline">\(\hat{y}\)</span>)
and observed (<span class="math inline">\(y\)</span>) values (e.g., the root mean square error). These
metrics are essential to guide model training and inform the evaluation.
Different metrics measure different aspects of the model-data agreement
and different metrics are used for regression and classification models.</p>
<div id="metrics-for-regression-models" class="section level4" number="8.2.3.1">
<h4><span class="header-section-number">8.2.3.1</span> Metrics for regression models</h4>
<p>Different metrics measure, for example, the correlation between modeled
and observed values or the magnitude of the errors. To get an intuitive
understanding of their different abilities, compare the scatterplots in
Figure <a href="ch-07.html#fig:metrics">8.3</a> and how different aspects of the model-data
agreement are measured by different metrics. Their definitions will
follow below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:metrics"></span>
<img src="figures/correlation_error.png" alt="Comparison of model metrics on different data sets." width="50%" />
<p class="caption">
Figure 8.3: Comparison of model metrics on different data sets.
</p>
</div>
<ul>
<li><p><strong>MSE</strong>: The mean squared error is defined, as its name suggests,
as: <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2
\]</span>It measures accuracy, i.e., the magnitude of the errors, and is
minimized during model training when used as a loss function. Note
that since it scales with the square of the errors, the MSE is
particularly sensitive to large errors in single points (including
outliers). You may notice the difference to the error variance
<span class="math inline">\(\widehat{\sigma}^2\)</span>. It was defined in Chapter <a href="ch-06.html#ch-06">7</a> similar
to the definition of the MSE above, but with <span class="math inline">\(n-p\)</span> in the
denominator instead of <span class="math inline">\(n\)</span>. The denominator <span class="math inline">\(n-p\)</span> corresponds to the
<em>degrees of freedom</em> (size of our sample minus the number of
parameters to estimate). Here we want to compute the mean of the
errors, hence we divide by <span class="math inline">\(n\)</span>.</p></li>
<li><p><strong>RMSE</strong>: The root mean squared error is, as its name suggests, the
root of the MSE: <span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2}
\]</span>Like the MSE, the RMSE also measures accuracy (the magnitude of
the errors) and is minimized during model training. By taking the
square root of mean square errors, the RMSE is in the same units as
the data <span class="math inline">\(y\)</span> and is less sensitive to outliers as the MSE.</p></li>
</ul>
<p><strong>Checkpoint</strong></p>
<p>Implement the formula for RMSE using simple “low-level” functions like
<code>sqrt()</code> and <code>mean()</code>. Confirm that the function <code>rmse()</code> from the
yardstick package computes the RMSE the same way.</p>
<p><strong>Solution</strong></p>
<div class="sourceCode" id="cb602"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb602-1"><a href="ch-07.html#cb602-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generate random data </span></span>
<span id="cb602-2"><a href="ch-07.html#cb602-2" aria-hidden="true" tabindex="-1"></a>df_demo <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb602-3"><a href="ch-07.html#cb602-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_obs =</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>),</span>
<span id="cb602-4"><a href="ch-07.html#cb602-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">y_pred =</span> x)</span>
<span id="cb602-5"><a href="ch-07.html#cb602-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb602-6"><a href="ch-07.html#cb602-6" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rmse</span>(df_demo, y_obs, y_pred) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span></code></pre></div>
<pre><code>## [1] 1.01994</code></pre>
<div class="sourceCode" id="cb604"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb604-1"><a href="ch-07.html#cb604-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> df_demo<span class="sc">$</span>y_obs)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 1.01994</code></pre>
<ul>
<li><p><span class="math inline">\(R^2\)</span>, also called the <em>coefficient of determination</em>, describes the
proportion of variation in <span class="math inline">\(y\)</span> that is captured by modelled values
<span class="math inline">\(\hat{y}\)</span>. In this case, the goal is to maximize the metric, thus
trying the explain as much variation as possible. In contrast to the
MSE and RMSE, <span class="math inline">\(R^2\)</span> measures consistency, or correlation, or
goodness of fit, and not accuracy. It is traditionally defined as:
<span class="math display">\[
R^2 = 1 - \frac{\sum_i (\hat{y}_i - y_i)^2}{\sum_i (y_i - \bar{y})^2}
\]</span>A perfect fit is quantified by <span class="math inline">\(R^2 = 1\)</span> and uninformative
estimates have an <span class="math inline">\(R^2\)</span> approaching zero when compared to
observations.</p></li>
<li><p><strong>Pearson’s</strong> <span class="math inline">\(r^2\)</span>: The linear association between to variables
(here <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span>) is measured by the <em>Pearson’s correlation
coefficient</em> <span class="math inline">\(r\)</span>. Its square is closely related to the coefficient
of determination and in common cases of prediction-observation
comparisons almost identical. <span class="math display">\[
r = \frac{\sum_i (y_i - \bar{y}) (\hat{y_i} - \hat{\bar{y}}) }{\sqrt{ \sum_i(y_i-\bar{y})^2(\hat{y_i}-\hat{\bar{y}})^2 } }
\]</span></p></li>
</ul>
<p>The distinction between uppercase and lowercase nomenclature is often
not consistent in the literature. The uppercase <span class="math inline">\(R^2\)</span> is commonly used
in the context of comparing observed and predicted values with the
coefficient of determination. When the <em>correlation</em> between two
different variables in a sample is quantified, the lowercase <span class="math inline">\(r^2\)</span> is
commonly used. In a linear regression with an estimated intercept, the
coefficient of determination and the square of the Pearson’s correlation
coefficient are equal. However, when comparing estimated and observed
values, the coefficient of determination can return negative values for
uninformative estimates.</p>
<p>Metrics for correlation should not be used as a loss function because
they do not penalise biased models. This is illustrated also in the
plots above.</p>
<p>The yardstick library implements the definition of the coefficient of
determination with its function <code>rsq_trad()</code>.</p>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb606-1"><a href="ch-07.html#cb606-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generate random data </span></span>
<span id="cb606-2"><a href="ch-07.html#cb606-2" aria-hidden="true" tabindex="-1"></a>df_demo <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb606-3"><a href="ch-07.html#cb606-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_obs =</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>),</span>
<span id="cb606-4"><a href="ch-07.html#cb606-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">y_pred =</span> x)</span>
<span id="cb606-5"><a href="ch-07.html#cb606-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb606-6"><a href="ch-07.html#cb606-6" aria-hidden="true" tabindex="-1"></a><span class="do">## the equation given above for the coefficient of determination corresponds to &#39;rsq_trad()&#39;</span></span>
<span id="cb606-7"><a href="ch-07.html#cb606-7" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> df_demo<span class="sc">$</span>y_obs)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">sum</span>((df_demo<span class="sc">$</span>y_obs <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_obs))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.4611166</code></pre>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb608-1"><a href="ch-07.html#cb608-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq_trad</span>(df_demo, y_obs, y_pred) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span></code></pre></div>
<pre><code>## [1] 0.4611166</code></pre>
<p>The square of the Pearson’s correlation coefficient, as defined above,
is implemented by the yardstick function <code>rsq()</code>, and corresponds also
to the value reported for <code>Multiple R-squared</code> by <code>summary()</code> on a
linear model object, or simply to <code>cor(...)^2</code>.</p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="ch-07.html#cb610-1" aria-hidden="true" tabindex="-1"></a><span class="do">## the equation given above for the squared Pearson&#39;s correlation coefficient corresponds to &#39;rsq()&#39;, &#39;cor()^2&#39;, and `summary()$r.squared</span></span>
<span id="cb610-2"><a href="ch-07.html#cb610-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb610-3"><a href="ch-07.html#cb610-3" aria-hidden="true" tabindex="-1"></a>(<span class="fu">sum</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_pred))<span class="sc">*</span>(df_demo<span class="sc">$</span>y_obs <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_obs))))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span></span>
<span id="cb610-4"><a href="ch-07.html#cb610-4" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">sum</span>((df_demo<span class="sc">$</span>y_obs <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_obs))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">sum</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_pred))<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.506538</code></pre>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="ch-07.html#cb612-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq</span>(df_demo, y_obs, y_pred) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span></code></pre></div>
<pre><code>## [1] 0.506538</code></pre>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="ch-07.html#cb614-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y_obs <span class="sc">~</span> y_pred, <span class="at">data =</span> df_demo))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.506538</code></pre>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="ch-07.html#cb616-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(df_demo<span class="sc">$</span>y_obs, df_demo<span class="sc">$</span>y_pred)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.506538</code></pre>
<p>The <span class="math inline">\(R^2\)</span> generally increases when predictors are added to a model, even
if predictors are not informative. This is particularly critical in the
context of machine learning when we compare alternative models that
differ by their number of predictors. In other words, the <span class="math inline">\(R^2\)</span> of a
model with a large number of predictors tends to give an overconfident
estimate of its predictive power.</p>
<p>Above, we have seen how cross-validation can be used to assess
generalisability (model performance on the validation set). This is the
“gold-standard.” But when the number of data points is small, cross
validation estimates may not be robust. Without resorting to cross
validation, the effect of spuriously improving the evaluation metric by
adding uninformative predictors can also be mitigated by penalising by
the number of predictors <span class="math inline">\(p\)</span>. Different metrics are available.</p>
<ul>
<li><p><strong>Adjusted</strong> <span class="math inline">\(R^2\)</span>: The adjusted <span class="math inline">\(R^2\)</span> discounts values by the
number of predictors as <span class="math display">\[
\bar{R}^2 = 1 - (1-R^2) \; \frac{n-1}{n-p-1} \;,
\]</span>where <span class="math inline">\(n\)</span> (as before) is the number of observations and <span class="math inline">\(p\)</span> the
number of predictors. As for <span class="math inline">\(R^2\)</span>, the goal is to maximize it. For
a fitted model in R <code>modl</code>, it is returned by
<code>summary(modl)$adj.r.squared</code>.</p></li>
<li><p><strong>AIC</strong>: the Akaike’s Information Criterion is defined as <span class="math display">\[
\text{AIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + 2(p+2)
\]</span>where <span class="math inline">\(N\)</span> is the number of observations used for estimation, <span class="math inline">\(p\)</span>
is the number of predictors in the model and SSE is the sum of
squared estimate of errors (SSE<span class="math inline">\(= \sum_i (y_i-\hat{y_i})^2\)</span>). Also
in this case we have to minimize it and the model with the minimum
value of the AIC is often the best model for forecasting. For large
values of<br />
<span class="math inline">\(n\)</span>, minimising the AIC is equivalent to minimising the
cross-validated MSE.</p></li>
<li><p><strong>AIC</strong><span class="math inline">\(_c\)</span>: For small values of <span class="math inline">\(n\)</span> the AIC tends to select too
many predictors. A bias-corrected version of the AIC is defined as:
<span class="math display">\[
\text{AIC}_c = \text{AIC} + \frac{2(p + 2)(p + 3)}{n-p-3}
\]</span>Also AIC<span class="math inline">\(_c\)</span> is minimised for an optimal predictive model.</p></li>
<li><p><strong>BIC</strong>: the Schwarz’s Bayesian Information Criterion is defined as
<span class="math display">\[
\text{BIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + (p+2)  \log(n)
\]</span>Also in this case our goal is to minimize the BIC. This metric has
the feature that if there is a true underlying model, the BIC will
select that model given enough data. The BIC tends to select the
model with fewer predictors than AIC.</p></li>
</ul>
</div>
<div id="metrics-for-classification" class="section level4" number="8.2.3.2">
<h4><span class="header-section-number">8.2.3.2</span> Metrics for classification</h4>
<p>In the examples for this course, we have thus far focused on regression
models. For classification, different metrics for measuring the
agreement between predicted and observed values are used. They will be
introduced in a later chapter. If you’re curious already now, good
overviews are provided in the following links:</p>
<ul>
<li><a href="https://bradleyboehmke.github.io/HOML/process.html#model-eval">Hands On Machine Learning in R, Bradley &amp;
Boehmke</a>
for brief definitions.</li>
<li><a href="https://en.wikipedia.org/wiki/Confusion_matrix">Wikipedia on Confusion
Matrix</a></li>
<li><a href="https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b">Blogpost by M. Sunasra on
Medium</a></li>
</ul>
</div>
<div id="residual-analysis" class="section level4" number="8.2.3.3">
<h4><span class="header-section-number">8.2.3.3</span> Residual analysis</h4>
<p>Quantifying metrics is one part of model evaluation. The other part is
to get an intuitive understanding of the model-observation agreement and
where and why they fail. Getting there is an integral part of
exploratory data analysis. One of the first steps after obtaining
results from your initial model is to investigate its residuals (the
difference between predicted and observed values) and their pattern. If
you can detect a clear pattern or trend in your residuals, then your
model has room for improvement.</p>
<p>Our example data set contains time series of multiple variables. Above,
we have not used time as a predictor but values in the data frame are
ordered by time along rows. An obvious first step is to look at
residuals and their relationship with time (or simply row number in our
case).</p>
<p>A handy function to add predictions and residuals from a fitted model to
the data (must contain the variables used in the model), is
<a href="https://broom.tidymodels.org/reference/data.frame_tidiers.html"><code>augment</code></a>
from the tidyverse <a href="https://broom.tidymodels.org/index.html"><em>broom</em></a>
package <span class="citation">(<a href="#ref-R-broom" role="doc-biblioref">Robinson, Hayes, and Couch 2021</a>)</span>. It adds information about observations to a data
set.</p>
<p>Let’s fit a linear regression model <code>GPP_NT_VUT_REF ~ SW_IN_F</code> and look
at the residuals.</p>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="ch-07.html#cb618-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb618-2"><a href="ch-07.html#cb618-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb618-3"><a href="ch-07.html#cb618-3" aria-hidden="true" tabindex="-1"></a>linmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F, <span class="at">data =</span> ddf)</span>
<span id="cb618-4"><a href="ch-07.html#cb618-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb618-5"><a href="ch-07.html#cb618-5" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">augment</span>(linmod1, <span class="at">data =</span> ddf)</span>
<span id="cb618-6"><a href="ch-07.html#cb618-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb618-7"><a href="ch-07.html#cb618-7" aria-hidden="true" tabindex="-1"></a>df1 <span class="sc">%&gt;%</span> </span>
<span id="cb618-8"><a href="ch-07.html#cb618-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">id =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb618-9"><a href="ch-07.html#cb618-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(id, .resid)) <span class="sc">+</span> </span>
<span id="cb618-10"><a href="ch-07.html#cb618-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">alpha =</span> .<span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb618-11"><a href="ch-07.html#cb618-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Row number&quot;</span>) <span class="sc">+</span></span>
<span id="cb618-12"><a href="ch-07.html#cb618-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="sc">+</span></span>
<span id="cb618-13"><a href="ch-07.html#cb618-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Univariate linear regression&quot;</span>, <span class="at">subtitle =</span> <span class="st">&quot;GPP_NT_VUT_REF ~ SW_IN_F&quot;</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-260-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Apparently, there is a clear pattern in the residuals with an apparent
autocorrelation (the residual in one data point is correlated with the
residual in its preceding data point). This suggests that there is
temporal information that we did not account for in our model. This
could either be from additional predictors, not included here, that have
a temporal pattern (note that we only included one predictor in this
model), or from the inherent dynamics of the system itself, which is not
captured by the predictors (e.g., memory effects).</p>
<p>We can also plot residuals versus additional, potentially influential
but not included predictors to guide the revision of the model. However,
it is often not possible to determine effects by eye, especially if
interactions between variables are important. We learn about methods to
sequentially add predictors to a linear regression model in the
‘Application’ session (see Chapter <a href="ch-08.html#ch-08">9</a>).</p>
</div>
</div>
<div id="bonus-model-interpretation" class="section level3" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Bonus: Model interpretation</h3>
<p>Until now, we have seen how to build and train models, tune the
hyperparameters and evaluate models. ML models work so well because they
can effectively make use of large amounts of data and are flexible
enough to model non-linear relationships and interactions, and predict
rare and faint phenomena. This is their great advantage over classical
statistical methods. However, this flexibility is underpinned by a high
model complexity and a large number of parameters. This complexity also
contrasts with traditional statistical methods, e.g., linear regression
where fitted coefficients (<span class="math inline">\(\beta_i\)</span> in Chapter <a href="ch-06.html#ch-06">7</a>) can
directly be interpreted and yield information about the sensitivity of
the target variable to the different predictors and even about the
statistical significance of their effect (we haven’t looked at that
part). In contrast, the complexity of ML models renders their
interpretation difficult. The ML algorithms’ predictive power comes at
the cost of reduced <em>model interpretability</em>. ML models often appear to
be “black boxes” which may limit their usefulness in typical
applications in research, where we’re often not only interested in
<em>predicting</em>, but also in <em>understanding</em> how the model arrived at its
predictions. We are often interested in identifying patterns in the data
that would otherwise not be visible, but that the algorithm apparently
identified and learned.</p>
<p>In order to interpret a ML model and understand its inner workings, we
have to “ask specific questions” that can be translated into an
evaluation of the trained model that then yields the answer we want.
There are basically two types of questions we can ask:</p>
<ul>
<li>How “important” is each predictor variable in our model? This is
answered by <em>variable importance analysis</em>.</li>
<li>What’s the functional relationship between the target variable and
each predictor? This is answered by <em>partial residual analysis</em>.</li>
</ul>
<div id="variable-importance" class="section level4" number="8.2.4.1">
<h4><span class="header-section-number">8.2.4.1</span> Variable importance</h4>
<p><strong>Model specific approach (for linear regression)</strong></p>
<p>Some of the approaches for evaluating feature importances are
<em>model-specific</em>. For instance, the absolute value of <em>t</em>-statistic, in
case of linear models, as a measure of feature importance. Such
model-specific interpretation tools are limited to their respective
model classes. There can be some advantages to using these
model-specific approaches as they are more closely linked to the model
and its performance, and can thus directly use parameters (coefficients)
of the fitted model.</p>
<p>For linear regression models, we can quantify the significance of its
coefficients <span class="math inline">\(\beta\)</span>, by testing a null-hypothesis that the coefficient
is, in reality, zero. As an example, let’s assume that our data are
generated from the following linear model <span class="math display">\[
y = \beta_0 + \beta_1 x + \epsilon
\]</span> where we assume that our errors <span class="math inline">\(\epsilon\)</span> are independent and come
from a normal distribution. Given only one realization of our data, our
aim is to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Let’s generate random data
for a univariate linear relationship where the true coefficients are
<span class="math inline">\(\beta_1 = 3\)</span> and <span class="math inline">\(\beta_0 =2\)</span>. Note that in practice we do not know the
real coefficients and we don’t know what the true predictors are. Let’s
consider an additional predictor that is available in our data but does
not contribute to the actual data generation process (<code>x2</code>). We include
it in the model and want to know whether the respective coefficient it
yields is significant. How can we determine information about its
<em>importance</em>? As we’ve seen before, the <code>summary()</code> function generates a
human-readable output. What information does it provide about variable
importance?</p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb619-1"><a href="ch-07.html#cb619-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generate random data</span></span>
<span id="cb619-2"><a href="ch-07.html#cb619-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb619-3"><a href="ch-07.html#cb619-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb619-4"><a href="ch-07.html#cb619-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb619-5"><a href="ch-07.html#cb619-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb619-6"><a href="ch-07.html#cb619-6" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># sample size</span></span>
<span id="cb619-7"><a href="ch-07.html#cb619-7" aria-hidden="true" tabindex="-1"></a>b_1 <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb619-8"><a href="ch-07.html#cb619-8" aria-hidden="true" tabindex="-1"></a>b_0 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb619-9"><a href="ch-07.html#cb619-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb619-10"><a href="ch-07.html#cb619-10" aria-hidden="true" tabindex="-1"></a>df_demo <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="fu">rnorm</span>(n), <span class="at">x2 =</span> <span class="fu">rnorm</span>(n)) <span class="sc">%&gt;%</span></span>
<span id="cb619-11"><a href="ch-07.html#cb619-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> b_1 <span class="sc">*</span> x1 <span class="sc">+</span> b_0 <span class="sc">+</span> <span class="fu">rnorm</span>(n ,<span class="at">mean =</span> <span class="dv">0</span> , <span class="at">sd =</span> <span class="dv">1</span>))  <span class="co"># no x2 here</span></span>
<span id="cb619-12"><a href="ch-07.html#cb619-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb619-13"><a href="ch-07.html#cb619-13" aria-hidden="true" tabindex="-1"></a><span class="do">## fit model</span></span>
<span id="cb619-14"><a href="ch-07.html#cb619-14" aria-hidden="true" tabindex="-1"></a>linmod_demo <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> df_demo)  <span class="co"># x2 here because it&#39;s in the data - is it significant?</span></span>
<span id="cb619-15"><a href="ch-07.html#cb619-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb619-16"><a href="ch-07.html#cb619-16" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_demo)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = df_demo)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8730 -0.6607 -0.1245  0.6214  2.0798 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.13507    0.09614  22.208   &lt;2e-16 ***
## x1           2.86683    0.10487  27.337   &lt;2e-16 ***
## x2           0.02381    0.09899   0.241     0.81    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9513 on 97 degrees of freedom
## Multiple R-squared:  0.8853, Adjusted R-squared:  0.8829 
## F-statistic: 374.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>You see that the linear regression fit yields estimates of the
coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that are relatively close to the
real ones (2.13507 for <span class="math inline">\(\beta_0\)</span> instead of 2, 2.86683 for <span class="math inline">\(\beta_1\)</span>
instead of 3). We also see that the estimate (<code>Estimate</code>) for (the fake)
coefficient <code>x2</code> is close to zero. The standard error of the coefficient
estimate <code>Std. Error</code> is an estimate of the standard deviation of
coefficient estimates we would get if the above random data generation
process was repeated many times. It decreases with the sample size (<code>n</code>
in the code above). The <em>t</em>-statistic (<code>t value</code>) is
<span class="math inline">\(\frac{\mbox{Estimate}}{\mbox{Std.Error}}\)</span>. Assuming a t-distribution of
coefficient estimates, the <em>p</em>-value (<code>Pr(&gt;|t|)</code>) quantifies the
probability of our coefficient estimate (considering its <em>t</em>-statistic)
if the true coefficient was zero - our null hypothesis. In our example,
the <em>p</em>-value for the <code>x2</code> estimate is 0.81. This is not significant at
any significance level (indicated by the <code>*</code> to the right of the
reported p-value, key given by <code>Signif. codes</code>). In contrast, the
estimates for coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are highly
significant. Their very low <em>p</em>-values indicate that it is highly
unlikely that their true value was zero.</p>
<p>This is illustrated below.</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="ch-07.html#cb621-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb621-2"><a href="ch-07.html#cb621-2" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">7</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb621-3"><a href="ch-07.html#cb621-3" aria-hidden="true" tabindex="-1"></a>t_dist <span class="ot">&lt;-</span> <span class="fu">dt</span>(grid, <span class="at">df =</span> n<span class="dv">-3</span>)           <span class="co"># 3 is the number of estimated coefficients</span></span>
<span id="cb621-4"><a href="ch-07.html#cb621-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb621-5"><a href="ch-07.html#cb621-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()<span class="sc">+</span></span>
<span id="cb621-6"><a href="ch-07.html#cb621-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> grid, <span class="at">y =</span> t_dist,<span class="at">color =</span> <span class="st">&#39;Null Distribution&#39;</span>), <span class="at">lwd=</span><span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb621-7"><a href="ch-07.html#cb621-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="fl">22.208</span>, <span class="at">linetype =</span> <span class="st">&#39; t value </span><span class="sc">\n</span><span class="st"> for intercept&#39;</span>), <span class="at">color=</span><span class="st">&#39;red&#39;</span>) <span class="sc">+</span></span>
<span id="cb621-8"><a href="ch-07.html#cb621-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">name =</span> <span class="st">&#39;&#39;</span>,<span class="at">values =</span> <span class="st">&#39;black&#39;</span>) <span class="sc">+</span></span>
<span id="cb621-9"><a href="ch-07.html#cb621-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(<span class="at">name=</span><span class="st">&#39;&#39;</span>, <span class="at">values =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb621-10"><a href="ch-07.html#cb621-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">25</span>)) <span class="sc">+</span></span>
<span id="cb621-11"><a href="ch-07.html#cb621-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&#39;t value&#39;</span>, <span class="at">y =</span> <span class="st">&#39;Density&#39;</span>, <span class="at">title =</span> <span class="st">&#39;Students t-distribution&#39;</span>) <span class="sc">+</span></span>
<span id="cb621-12"><a href="ch-07.html#cb621-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&#39;top&#39;</span>) <span class="sc">+</span></span>
<span id="cb621-13"><a href="ch-07.html#cb621-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">1</span>),</span>
<span id="cb621-14"><a href="ch-07.html#cb621-14" aria-hidden="true" tabindex="-1"></a>         <span class="at">linetype =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">2</span>))</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-262-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="ch-07.html#cb622-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()<span class="sc">+</span></span>
<span id="cb622-2"><a href="ch-07.html#cb622-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> grid, <span class="at">y =</span> t_dist, <span class="at">color =</span> <span class="st">&#39;Null Distribution&#39;</span>), <span class="at">lwd=</span><span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb622-3"><a href="ch-07.html#cb622-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="fl">0.241</span>, <span class="at">linetype =</span> <span class="st">&#39; t value </span><span class="sc">\n</span><span class="st"> for x2&#39;</span>), <span class="at">color=</span><span class="st">&#39;red&#39;</span>) <span class="sc">+</span></span>
<span id="cb622-4"><a href="ch-07.html#cb622-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">name =</span> <span class="st">&#39;&#39;</span>,<span class="at">values =</span> <span class="st">&#39;black&#39;</span>) <span class="sc">+</span></span>
<span id="cb622-5"><a href="ch-07.html#cb622-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(<span class="at">name=</span><span class="st">&#39;&#39;</span>, <span class="at">values =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb622-6"><a href="ch-07.html#cb622-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">25</span>)) <span class="sc">+</span></span>
<span id="cb622-7"><a href="ch-07.html#cb622-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&#39;t value&#39;</span>, <span class="at">y =</span> <span class="st">&#39;Density&#39;</span>, <span class="at">title =</span> <span class="st">&#39;Students t-distribution&#39;</span>) <span class="sc">+</span></span>
<span id="cb622-8"><a href="ch-07.html#cb622-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&#39;top&#39;</span>) <span class="sc">+</span></span>
<span id="cb622-9"><a href="ch-07.html#cb622-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">1</span>),</span>
<span id="cb622-10"><a href="ch-07.html#cb622-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">linetype =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">2</span>))</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-262-2.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Variable importance in a fitted model can be assessed in R using the
<strong>vip</strong> package <span class="citation">(<a href="#ref-R-vip" role="doc-biblioref">Greenwell and Boehmke 2020</a>)</span>. If we provide a linear regression model as
argument, it automatically uses the absolute value of the <em>t</em>-statistic
for quantifying the variable importance.</p>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="ch-07.html#cb623-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span>
<span id="cb623-2"><a href="ch-07.html#cb623-2" aria-hidden="true" tabindex="-1"></a>linmod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> ddf <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>TIMESTAMP))</span>
<span id="cb623-3"><a href="ch-07.html#cb623-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(linmod3)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-263-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p><strong>Checkpoint</strong></p>
<p>Verify that the bars plotted in the vip plot correspond to the
t-statistic of the linear regression model.</p>
<p><strong>Solution</strong></p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb624-1"><a href="ch-07.html#cb624-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod3)  <span class="co"># compare with values under `t-value`</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ ., data = ddf %&gt;% dplyr::select(-TIMESTAMP))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1828 -1.0365 -0.1325  0.8901  7.7224 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.929937   2.899304  -1.355    0.175    
## TA_F         0.158395   0.008958  17.682  &lt; 2e-16 ***
## SW_IN_F      0.013815   0.000326  42.381  &lt; 2e-16 ***
## LW_IN_F      0.020617   0.001004  20.535  &lt; 2e-16 ***
## VPD_F       -0.137645   0.016515  -8.334  &lt; 2e-16 ***
## PA_F        -0.004198   0.033688  -0.125    0.901    
## P_F         -0.017538   0.003979  -4.407 1.06e-05 ***
## WS_F        -0.150129   0.033470  -4.486 7.41e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.537 on 5885 degrees of freedom
## Multiple R-squared:  0.6906, Adjusted R-squared:  0.6902 
## F-statistic:  1877 on 7 and 5885 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The results of the feature importance plot using <code>vip()</code>, agree with the
statistics obtaind from the <code>summary()</code> function. The variables
<code>LW_IN_F</code> and <code>VPD_F</code> have largest <span class="math inline">\(|t|\)</span> statistic values (thus, the
values on the tail ends of the t-distribution have low measure), which
results in a small p-value, thus indicating that these are the most
important features. On the other end, the variable <code>P_F</code> has the
smallest <span class="math inline">\(|t|\)</span> statistc, which results in a large p-value, thus
indicating that this variable is not as important.</p>
<p>To sum this up, variable importance can be measured for linear
regression models using the <em>t</em>-statistic.</p>
<p>While <em>t</em>-statistic values provide information for feature importance
analysis that is straight-forward to quantify and interpret,
corresponding information has to be extracted by other means for other
ML algorithms (e.g., accuracy degradation in random forests). This makes
it difficult to compare their results across different classes of ML
models.</p>
<p><strong>Model-Agnostic approaches</strong></p>
<p>Model-Agnostic approaches offer a solution to the intercomparability
problem for feature importance analysis. These treat the ML algorithm as
a “black-box,” and the separation of the specific model types from their
interpretability measure enables us to compare the feature importance
across different models and ML algorithms. <em>Permutation-based feature
importance</em> is such an algorithm. Its idea is that if we <em>permute</em>
(re-shuffle) the values of a feature in the training data, we
effectively destroy the relationship between that particular feature and
the target variable. Thus, if the feature is important, the model
performance should degrade substantially. In contrast, for unimportant
features, permuting their values should not degrade the model
performance much.</p>
<p>Algorithmically speaking, this works as follows:</p>
<ul>
<li><p>We first compute the loss for the original model to establish a
reference loss.</p></li>
<li><p>For predictor <span class="math inline">\(i\)</span> in <span class="math inline">\(\{1,2, …, p\}\)</span></p>
<ul>
<li>Randomly permute values of predictor <span class="math inline">\(i\)</span> in the dataset.</li>
<li>Train the model.</li>
<li>Compute the loss.</li>
<li>Compute the feature importance, e.g., as the ratio or difference
of the loss with the permuted data and the reference loss.</li>
<li>Store this as the feature importance for feature <span class="math inline">\(i\)</span></li>
</ul></li>
<li><p>Sort the variable by their feature importance</p></li>
</ul>
<p>This can be easily implemented using the <code>vip</code> package along with a few
extra parameters</p>
<p>Arguments to the function <code>vip()</code>, for permutation based feature
importance</p>
<ul>
<li><code>linmod3</code>: A fitted model object</li>
<li><code>train</code> : the training data to be used to compute the feature importances</li>
<li><code>method</code> : <code>"permute"</code> for permutation based method</li>
<li><code>target</code> : specifying the target variable for the train datset</li>
<li><code>metric</code> : <code>"RMSE"</code> root mean squared error for the regression based task</li>
<li><code>n_sim</code> : number of times the simulation is to be repeated, the result is averaged over all the simulations ; choose greater values to increase the stability of estimates</li>
<li><code>sample_frac</code> : specifies the proportion of the training data to be used in each simulation. Default value is NULL i.e. all of training data is used</li>
<li><code>pred_wrapper</code> : Prediction function that requires two arguments, <code>object</code> and <code>newdata</code>. The output of this function should be determined by the metric being used - A numeric vector of predicted outcomes for a regression task - A vector of predicted class labels, or a matrix of predicted class probabilitites for a classification task</li>
<li><code>type</code> : Character string specifying how to compare the baseline and permuted performance metrics; default value is <code>'difference'</code>, and the other option is <code>'ratio'</code></li>
</ul>
<p><em>Note: In case of large datasets, <code>n_sim</code> and <code>sample_frac</code> can be used
to reduce the execution time of the algorithm, as permutation based
approaches can become slow as the number of predictors grows.</em></p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="ch-07.html#cb626-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inline function defined to output the predictions for newdata given a particular model object </span></span>
<span id="cb626-2"><a href="ch-07.html#cb626-2" aria-hidden="true" tabindex="-1"></a>pfun <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata) <span class="fu">predict</span>(object, <span class="at">newdata =</span> newdata) </span>
<span id="cb626-3"><a href="ch-07.html#cb626-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb626-4"><a href="ch-07.html#cb626-4" aria-hidden="true" tabindex="-1"></a><span class="do">## variable importance plot using the permute method, with type == &#39;ratio&#39; </span></span>
<span id="cb626-5"><a href="ch-07.html#cb626-5" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb626-6"><a href="ch-07.html#cb626-6" aria-hidden="true" tabindex="-1"></a>  linmod3,</span>
<span id="cb626-7"><a href="ch-07.html#cb626-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> ddf <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>TIMESTAMP),</span>
<span id="cb626-8"><a href="ch-07.html#cb626-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb626-9"><a href="ch-07.html#cb626-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb626-10"><a href="ch-07.html#cb626-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb626-11"><a href="ch-07.html#cb626-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb626-12"><a href="ch-07.html#cb626-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb626-13"><a href="ch-07.html#cb626-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun,</span>
<span id="cb626-14"><a href="ch-07.html#cb626-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;ratio&#39;</span></span>
<span id="cb626-15"><a href="ch-07.html#cb626-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-265-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="ch-07.html#cb627-1" aria-hidden="true" tabindex="-1"></a><span class="do">## variable importance plot using the permute method, with type == &#39;difference&#39; </span></span>
<span id="cb627-2"><a href="ch-07.html#cb627-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb627-3"><a href="ch-07.html#cb627-3" aria-hidden="true" tabindex="-1"></a>  linmod3,</span>
<span id="cb627-4"><a href="ch-07.html#cb627-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> ddf <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>TIMESTAMP),</span>
<span id="cb627-5"><a href="ch-07.html#cb627-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb627-6"><a href="ch-07.html#cb627-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb627-7"><a href="ch-07.html#cb627-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb627-8"><a href="ch-07.html#cb627-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb627-9"><a href="ch-07.html#cb627-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb627-10"><a href="ch-07.html#cb627-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun, </span>
<span id="cb627-11"><a href="ch-07.html#cb627-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;difference&#39;</span> </span>
<span id="cb627-12"><a href="ch-07.html#cb627-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-265-2.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Now we can compare the importance plots obtained from the model-specific
t-statistic approach and the model agnostic permuation based method. The
feature importances obtained from both approaches differ a little,
because of the underlying differences in the feature importance
evaluation. Despite of the differences, both methods robustly return the
key important features, particularly their ranks.</p>
<p><strong>Checkpoint</strong></p>
<p>Compute the variable importance for the best fit knn model, which we
obtained earlier in the tutorial, using the <code>vip()</code> function in default
mode. Repeat the same with permuatation based method, with type as
‘difference’ and ‘ratio.’ Do the two return the same top-4 features ?
Verify.</p>
<p><strong>Solution</strong></p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb628-1"><a href="ch-07.html#cb628-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Data wrangling done above</span></span>
<span id="cb628-2"><a href="ch-07.html#cb628-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb628-3"><a href="ch-07.html#cb628-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Set up recipe</span></span>
<span id="cb628-4"><a href="ch-07.html#cb628-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(recipes)</span>
<span id="cb628-5"><a href="ch-07.html#cb628-5" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">recipe</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> ddf_train <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>TIMESTAMP)) <span class="sc">%&gt;%</span></span>
<span id="cb628-6"><a href="ch-07.html#cb628-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_center</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span>        <span class="co"># normalizes numeric data to have a mean of zero</span></span>
<span id="cb628-7"><a href="ch-07.html#cb628-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_scale</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>())      </span>
<span id="cb628-8"><a href="ch-07.html#cb628-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb628-9"><a href="ch-07.html#cb628-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Set up knn</span></span>
<span id="cb628-10"><a href="ch-07.html#cb628-10" aria-hidden="true" tabindex="-1"></a>my_cv <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb628-11"><a href="ch-07.html#cb628-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,      <span class="co"># method define the resampling method such as &#39;boot&#39;, &#39;none&#39;, &#39;cv&#39;, etc.</span></span>
<span id="cb628-12"><a href="ch-07.html#cb628-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">number =</span> <span class="dv">10</span>,                <span class="co"># number of folds or number of resampling iterations</span></span>
<span id="cb628-13"><a href="ch-07.html#cb628-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">repeats =</span> <span class="dv">5</span>                 <span class="co"># the number of complete sets of folds to compute (only for repeated k-fold cross-validation)</span></span>
<span id="cb628-14"><a href="ch-07.html#cb628-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb628-15"><a href="ch-07.html#cb628-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb628-16"><a href="ch-07.html#cb628-16" aria-hidden="true" tabindex="-1"></a>hyper_grid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">18</span>, <span class="dv">20</span>, <span class="dv">22</span>, <span class="dv">24</span>, <span class="dv">26</span>, <span class="dv">30</span>, <span class="dv">35</span>, <span class="dv">40</span>, <span class="dv">60</span>, <span class="dv">100</span>))</span>
<span id="cb628-17"><a href="ch-07.html#cb628-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb628-18"><a href="ch-07.html#cb628-18" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb628-19"><a href="ch-07.html#cb628-19" aria-hidden="true" tabindex="-1"></a>knn_fit <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb628-20"><a href="ch-07.html#cb628-20" aria-hidden="true" tabindex="-1"></a>  pp,</span>
<span id="cb628-21"><a href="ch-07.html#cb628-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> ddf_train <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>TIMESTAMP),</span>
<span id="cb628-22"><a href="ch-07.html#cb628-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb628-23"><a href="ch-07.html#cb628-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">trControl =</span> my_cv,</span>
<span id="cb628-24"><a href="ch-07.html#cb628-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> hyper_grid,</span>
<span id="cb628-25"><a href="ch-07.html#cb628-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span></span>
<span id="cb628-26"><a href="ch-07.html#cb628-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb628-27"><a href="ch-07.html#cb628-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb628-28"><a href="ch-07.html#cb628-28" aria-hidden="true" tabindex="-1"></a><span class="do">## variable importance plot without the permute method</span></span>
<span id="cb628-29"><a href="ch-07.html#cb628-29" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(knn_fit)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-266-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="ch-07.html#cb629-1" aria-hidden="true" tabindex="-1"></a><span class="do">## variable importance plot using the permute method, with type == &#39;ratio&#39; </span></span>
<span id="cb629-2"><a href="ch-07.html#cb629-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb629-3"><a href="ch-07.html#cb629-3" aria-hidden="true" tabindex="-1"></a>  knn_fit,</span>
<span id="cb629-4"><a href="ch-07.html#cb629-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> ddf <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>TIMESTAMP),</span>
<span id="cb629-5"><a href="ch-07.html#cb629-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb629-6"><a href="ch-07.html#cb629-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb629-7"><a href="ch-07.html#cb629-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb629-8"><a href="ch-07.html#cb629-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb629-9"><a href="ch-07.html#cb629-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb629-10"><a href="ch-07.html#cb629-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun, </span>
<span id="cb629-11"><a href="ch-07.html#cb629-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;difference&#39;</span> </span>
<span id="cb629-12"><a href="ch-07.html#cb629-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-266-2.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb630-1"><a href="ch-07.html#cb630-1" aria-hidden="true" tabindex="-1"></a><span class="do">## variable importance plot using the permute method, with type == &#39;difference&#39; </span></span>
<span id="cb630-2"><a href="ch-07.html#cb630-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb630-3"><a href="ch-07.html#cb630-3" aria-hidden="true" tabindex="-1"></a>  knn_fit,</span>
<span id="cb630-4"><a href="ch-07.html#cb630-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> ddf <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>TIMESTAMP),</span>
<span id="cb630-5"><a href="ch-07.html#cb630-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb630-6"><a href="ch-07.html#cb630-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb630-7"><a href="ch-07.html#cb630-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb630-8"><a href="ch-07.html#cb630-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb630-9"><a href="ch-07.html#cb630-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb630-10"><a href="ch-07.html#cb630-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun, </span>
<span id="cb630-11"><a href="ch-07.html#cb630-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;ratio&#39;</span> </span>
<span id="cb630-12"><a href="ch-07.html#cb630-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-266-3.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="partial-dependence-plots" class="section level4" number="8.2.4.2">
<h4><span class="header-section-number">8.2.4.2</span> Partial dependence plots</h4>
<p>While variable importance analysis is helpful for understanding which
predictors are helping us predict, <em>partial dependence plots</em> (PDPs)
yield information on what the relationship between one individual
predictor and the target variable looks like. It quantifies how the
response of the target variable changes when we vary one particular
predictor and hold all the remaining predictors at a constant value. If
this is still a little muddy, the pseudo-code of this algorithm should
make things clear (see
<a href="https://bradleyboehmke.github.io/HOML/iml.html#partial-dependence">this</a>
by <span class="citation"><a href="#ref-boehmke_hands-machine_2019" role="doc-biblioref">Brad Boehmke and Greenwell</a> (<a href="#ref-boehmke_hands-machine_2019" role="doc-biblioref">2019</a>)</span>).</p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="ch-07.html#cb631-1" aria-hidden="true" tabindex="-1"></a>For a selected <span class="fu">predictor</span> (x)</span>
<span id="cb631-2"><a href="ch-07.html#cb631-2" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> Construct a grid of j evenly spaced values across the range of x<span class="sc">:</span> {x1, x2, ..., xj}</span>
<span id="cb631-3"><a href="ch-07.html#cb631-3" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> For i <span class="cf">in</span> {<span class="dv">1</span>,...,j} do</span>
<span id="cb631-4"><a href="ch-07.html#cb631-4" aria-hidden="true" tabindex="-1"></a>     <span class="sc">|</span> Copy the training data and replace the original values of x with the constant xi</span>
<span id="cb631-5"><a href="ch-07.html#cb631-5" aria-hidden="true" tabindex="-1"></a>     <span class="sc">|</span> Apply the fitted ML model to obtain vector of predictions <span class="cf">for</span> each data point.</span>
<span id="cb631-6"><a href="ch-07.html#cb631-6" aria-hidden="true" tabindex="-1"></a>     <span class="sc">|</span> Average predictions across all data points.</span>
<span id="cb631-7"><a href="ch-07.html#cb631-7" aria-hidden="true" tabindex="-1"></a>   End</span>
<span id="cb631-8"><a href="ch-07.html#cb631-8" aria-hidden="true" tabindex="-1"></a><span class="fl">3.</span> Plot the averaged predictions against x1, x2, ..., xj</span></code></pre></div>
<p>As the steps explain, PDPs plot the change in the average predicted
value (<span class="math inline">\(\hat{y}\)</span>) as a function of a specified feature. As you will see
in later chapters, PDPs become more useful when non-linear relationships
are present. However, PDPs of linear models help illustrate how a fixed
change in <span class="math inline">\(x_i\)</span> relates to a fixed linear change in <span class="math inline">\(\hat{y_i}\)</span> while
taking into account the average effect of all the other features in the
model. For linear regression models, the slope of the PDP is equal to
the corresponding coefficient of the model.</p>
<p>Considering Figure <a href="ch-07.html#fig:pdp">8.4</a>, to compute the PDP of target variable
with respect to Gr_Liv_Area, we modify the values of this variable for
all the rows in the data set, for each [j1, j2, …, j20] in the grid.
Feed this modified dataset to the pre-trained model and compute the mean
of our target variable (mean taken over all rows). Finally at the end of
the algorithm we have 20 mean responses of the target variable for 20
different values of Gr_Liv_Area. Finally we plot the mean response of
the target variable with respect to the changing variable Gr_Liv_Area to
get the parital dependence plot.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pdp"></span>
<img src="figures/pdp-illustration.png" alt="Visualiation of a partial dependence plot (from [Boehmke &amp; Greenwell (2019)](https://bradleyboehmke.github.io/HOML/images/pdp-illustration.png))." width="50%" />
<p class="caption">
Figure 8.4: Visualiation of a partial dependence plot (from <a href="https://bradleyboehmke.github.io/HOML/images/pdp-illustration.png">Boehmke &amp; Greenwell (2019)</a>).
</p>
</div>
<p>Lucky for us, this code has already been implemented in the <strong>pdp</strong>
package in R <span class="citation">(<a href="#ref-R-pdp" role="doc-biblioref">Greenwell 2017</a>)</span>. We use the <code>partial()</code> function to plot the
partial dependence plot for a single predictor variable, <code>SW_IN_F</code> or
<code>VPD_F</code>, or for a subset of both these variables. Since our models are
linear, we expect the PDP curves to be linear as well. The first curve
shows a positive linear relationship between <code>SW_IN_F</code> and the target
y_hat (<code>GPP_NT_VUT_REF</code>), i.e. as the <code>SW_IN_F</code> increases, the target
increases. Whereas the second curve shows us that there is a negative
linear dependence between <code>VPD_F</code> and <code>GPP_NT_VUT_REF</code>. The final plot
shows a 2D partial dependence plot with <code>SW_IN_F</code> and <code>VPD_F</code>, and this
follows a linear relationship too. As expected, we get the highest value
for our target variable in the lower right corner and the smallest value
in the upper left corner.</p>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb632-1"><a href="ch-07.html#cb632-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pdp)</span>
<span id="cb632-2"><a href="ch-07.html#cb632-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb632-3"><a href="ch-07.html#cb632-3" aria-hidden="true" tabindex="-1"></a><span class="fu">partial</span>(linmod3, <span class="st">&quot;SW_IN_F&quot;</span>, <span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-268-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="ch-07.html#cb633-1" aria-hidden="true" tabindex="-1"></a><span class="fu">partial</span>(linmod3, <span class="st">&quot;VPD_F&quot;</span>, <span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-268-2.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb634"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb634-1"><a href="ch-07.html#cb634-1" aria-hidden="true" tabindex="-1"></a><span class="fu">partial</span>(linmod3, <span class="fu">c</span>(<span class="st">&quot;SW_IN_F&quot;</span>, <span class="st">&quot;VPD_F&quot;</span>), <span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-268-3.png" width="50%" style="display: block; margin: auto;" /></p>
<p>The <code>pdp</code> package works with various model classes. For classes that are
not available, a prediction wrapper function has to be specified as
described for example
<a href="https://bradleyboehmke.github.io/HOML/iml.html#implementation-6">here</a>
<span class="citation">(<a href="#ref-boehmke_hands-machine_2019" role="doc-biblioref">Brad Boehmke and Greenwell 2019</a>)</span>. If you want to learn more about PDP, have
a look the <a href="https://christophm.github.io/interpretable-ml-book/">online
book</a> by
<span class="citation"><a href="#ref-molnar_interpretable_2019" role="doc-biblioref">Molnar</a> (<a href="#ref-molnar_interpretable_2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Note: The computational demand of partial dependence analysis rapidly
increases as the number of search grid points (<code>j</code> in the algorithm
description above) rises.</p>
<p><em>For the interested: If you do not want to average the y_hat,
predictions over all the rows for each value of</em> <span class="math inline">\(X_i\)</span>, look into
Individual conditional expectation (ICE) curves, which rather than
averaging the predicted values across all observations, observe and plot
the individual observation-level predictions.</p>
</div>
</div>
</div>
<div id="exercise-5" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Exercise</h2>
<div id="setup" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Setup</h3>
<p>The following exercise builds on the exercise of the previous tutorial.
So let’s load the work done there first:</p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="ch-07.html#cb635-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Load data and extrat relevant subset</span></span>
<span id="cb635-2"><a href="ch-07.html#cb635-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb635-3"><a href="ch-07.html#cb635-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb635-4"><a href="ch-07.html#cb635-4" aria-hidden="true" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb635-5"><a href="ch-07.html#cb635-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">starts_with</span>(<span class="st">&quot;TIMESTAMP&quot;</span>),</span>
<span id="cb635-6"><a href="ch-07.html#cb635-6" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ends_with</span>(<span class="st">&quot;_F&quot;</span>),</span>
<span id="cb635-7"><a href="ch-07.html#cb635-7" aria-hidden="true" tabindex="-1"></a>         GPP_NT_VUT_REF,</span>
<span id="cb635-8"><a href="ch-07.html#cb635-8" aria-hidden="true" tabindex="-1"></a>         NEE_VUT_REF_QC,</span>
<span id="cb635-9"><a href="ch-07.html#cb635-9" aria-hidden="true" tabindex="-1"></a>         <span class="sc">-</span><span class="fu">contains</span>(<span class="st">&quot;JSB&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb635-10"><a href="ch-07.html#cb635-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">TIMESTAMP =</span> lubridate<span class="sc">::</span><span class="fu">ymd</span>(TIMESTAMP)) <span class="sc">%&gt;%</span></span>
<span id="cb635-11"><a href="ch-07.html#cb635-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na_if</span>(<span class="sc">-</span><span class="dv">9999</span>) <span class="sc">%&gt;%</span></span>
<span id="cb635-12"><a href="ch-07.html#cb635-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="fu">ends_with</span>(<span class="st">&quot;_QC&quot;</span>), NEE_VUT_REF_QC) <span class="sc">%&gt;%</span> </span>
<span id="cb635-13"><a href="ch-07.html#cb635-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">GPP_NT_VUT_REF =</span> <span class="fu">ifelse</span>(NEE_VUT_REF_QC <span class="sc">&lt;</span> <span class="fl">0.9</span>, <span class="cn">NA</span>, GPP_NT_VUT_REF)) <span class="sc">%&gt;%</span> </span>
<span id="cb635-14"><a href="ch-07.html#cb635-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>NEE_VUT_REF_QC) <span class="sc">%&gt;%</span> </span>
<span id="cb635-15"><a href="ch-07.html#cb635-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">drop_na</span>()</span>
<span id="cb635-16"><a href="ch-07.html#cb635-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb635-17"><a href="ch-07.html#cb635-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Split data into testing and training data</span></span>
<span id="cb635-18"><a href="ch-07.html#cb635-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rsample)</span>
<span id="cb635-19"><a href="ch-07.html#cb635-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb635-20"><a href="ch-07.html#cb635-20" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1982</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb635-21"><a href="ch-07.html#cb635-21" aria-hidden="true" tabindex="-1"></a>split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ddf, <span class="at">prop =</span> <span class="fl">0.7</span>)</span>
<span id="cb635-22"><a href="ch-07.html#cb635-22" aria-hidden="true" tabindex="-1"></a>ddf_train <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb635-23"><a href="ch-07.html#cb635-23" aria-hidden="true" tabindex="-1"></a>ddf_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
</div>
<div id="linear-model-1" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Linear Model</h3>
<p>In the previous exercise, you fitted a linear regression model using the
base-R function <code>lm()</code> and the training set. The target variable was
<code>"GPP_NT_VUT_REF"</code>, and predictor variables were all available
meterological variables in the dataset.</p>
<p><strong>First</strong>, repeat these same steps but use <strong>caret</strong> and the function
<code>train()</code> for fitting the same linear regression model (with all
predictors) on the same data. You will have to set the argument
<code>trControl</code> accordingly to avoid resampling, and instead fit the model
on all data in <code>ddf_train</code>. Does it yield identical results as using
<code>lm()</code> directly?</p>
<p><strong>Second</strong>, repeat the visualisation and display of model metrics. Feel
free to copy-paste from the previous exercise /solutions. You are
repeating this step mainly to refresh the previous results and to
compare them to your work on the next tasks.</p>
<p><strong>Third</strong>, in the previous exercise we found that <code>PA_F</code> was not
significant. Build another linear model with <code>train()</code> but this time
without using <code>PA_F</code> as predictor. Can you find a model metric that
supports dropping <code>PA_F</code>from further analysis?</p>
<p>Hints:</p>
<ul>
<li>To specify the argument <code>trControl</code> you will have to use the
function <code>trainControl</code>. See the tutorial for the correct ssyntax
and enter <code>?trainControl</code> into the console to find out how to
specify the right method.</li>
<li>You can use <code>summary()</code> also on the object returned by the function
<code>train()</code> (similar to the object returned by <code>lm()</code>).</li>
<li>To support dropping <code>PA_F</code>, you might want to consider one of the
introduced information criteria.</li>
</ul>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb636-1"><a href="ch-07.html#cb636-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span>
<span id="cb636-2"><a href="ch-07.html#cb636-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 1. Setting up a linear model with train()</span></span>
<span id="cb636-3"><a href="ch-07.html#cb636-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb636-4"><a href="ch-07.html#cb636-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 2. Repeat visualisation and display of model metrics from Exercise 06</span></span>
<span id="cb636-5"><a href="ch-07.html#cb636-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb636-6"><a href="ch-07.html#cb636-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 3. Compare model performance with and without `PA_F` as predictor</span></span></code></pre></div>
</div>
</div>
<div id="knn" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> KNN</h2>
<div id="check-data" class="section level3" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Check data</h3>
<p>The variable <code>PA_F</code> was not significant in the linear model. Exclude it from
further analysis by specifying a “recipe” where <code>GPP_NT_VUT_REF</code> is the target variable, and all available variables except <code>PA_F</code> and <code>TIMESTAMP</code> are used as predictors.</p>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="ch-07.html#cb637-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
<div id="training-1" class="section level3" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Training</h3>
<p>Fit two KNN models on <code>ddf_train</code> (excluding <code>"PA_F"</code>), one with <span class="math inline">\(k = 2\)</span>
and one with <span class="math inline">\(k = 30\)</span>, both without resampling. Use the RMSE as the loss
function. Center and scale data as part of the pre-processing and model
formulation specification using the function <code>recipe()</code>.</p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="ch-07.html#cb638-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
<div id="prediction-1" class="section level3" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span> Prediction</h3>
<p>With the two models fitted above, predict <code>"GPP_NT_VUT_REF"</code> for both
and training and the testing sets, and evaluate them as above (metrics
and visualisation).</p>
<p>Which model do you expect to perform better on the training set and
which to perform better on the testing set? Why? Do you find evidence
for overfitting in any of the models?</p>
<p>Hint: Make use of the visualization code that you used above and in
Exercise 06.</p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="ch-07.html#cb639-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
<div id="sample-hyperparameters" class="section level3" number="8.4.4">
<h3><span class="header-section-number">8.4.4</span> Sample hyperparameters</h3>
<p>Train a KNN model with hyperparameter (<span class="math inline">\(k\)</span>) tuned, and with five-fold
cross validation, using the training set. As the loss function, use
RMSE. Sample the following values for <span class="math inline">\(k\)</span>: 2, 5, 10, 15, 18, 20, 22, 24,
26, 30, 35, 40, 60, 100. Visualise the RMSE as a function of <span class="math inline">\(k\)</span>.</p>
<p>Hint:</p>
<ul>
<li>The visualisation of cross-validation results can be visualised with
the <code>plot(model_object)</code> of <code>ggplot(model_object)</code>.</li>
</ul>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb640-1"><a href="ch-07.html#cb640-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
</div>
<div id="random-forest-1" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Random forest</h2>
<div id="training-2" class="section level3" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Training</h3>
<p>Fit a random forest model with <code>ddf_train</code> and all predictors excluding
<code>"PA_F"</code> and five-fold cross validation. Use RMSE as the loss function.</p>
<p>Hints:</p>
<ul>
<li>Use the package <em>ranger</em> which implements the random forest
algorithm.</li>
<li>See <a href="https://topepo.github.io/caret/available-models.html">here</a> for
information about hyperparameters available for tuning with caret.</li>
<li>Set the argument <code>savePredictions = "final"</code> of function
<code>trainControl()</code>.</li>
</ul>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb641-1"><a href="ch-07.html#cb641-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
<div id="prediction-2" class="section level3" number="8.5.2">
<h3><span class="header-section-number">8.5.2</span> Prediction</h3>
<p>Evaluate the trained model on the training and on the test set, giving
metrics and a visualisation as above.</p>
<p>How are differences in performance to be interpreted? Compare the
performances of linear regression, KNN, and random forest, considering
the evaluation on the test set.</p>
<p>Hint: Make use of the visualization code that you used above and in
Exercise 06.</p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="ch-07.html#cb642-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
<!-- Show the model performance (metrics and visualisation) on the validation -->
<!-- sets all cross validation folds combined. -->
<!-- Do you expect it to be more similar to the model performance on the -->
<!-- training set or the testing set in the evaluation above? Why? -->
<!-- ```{r} -->
<!-- ## write your code here -->
<!-- ``` -->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-boehmke_hands-machine_2019" class="csl-entry">
Boehmke, Brad, and Brandon M. Greenwell. 2019. <em>Hands-on Machine Learning with <span>R</span></em>. Chapman &amp; <span>Hall</span>/<span>CRC</span> the <span>R</span> Series. Boca Raton: CRC Press.
</div>
<div id="ref-R-pdp" class="csl-entry">
Greenwell, Brandon M. 2017. <span>“Pdp: An r Package for Constructing Partial Dependence Plots.”</span> <em>The R Journal</em> 9 (1): 421–36. <a href="https://journal.r-project.org/archive/2017/RJ-2017-016/index.html">https://journal.r-project.org/archive/2017/RJ-2017-016/index.html</a>.
</div>
<div id="ref-R-vip" class="csl-entry">
Greenwell, Brandon M., and Bradley C. Boehmke. 2020. <span>“Variable Importance Plots—an Introduction to the Vip Package.”</span> <em>The R Journal</em> 12 (1): 343–66. <a href="https://doi.org/10.32614/RJ-2020-013">https://doi.org/10.32614/RJ-2020-013</a>.
</div>
<div id="ref-molnar_interpretable_2019" class="csl-entry">
Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making <span>Black</span> <span>Box</span> <span>Models</span> Interpretable</em>. Morisville, North Carolina: Lulu.
</div>
<div id="ref-R-broom" class="csl-entry">
Robinson, David, Alex Hayes, and Simon Couch. 2021. <em>Broom: Convert Statistical Objects into Tidy Tibbles</em>. <a href="https://CRAN.R-project.org/package=broom">https://CRAN.R-project.org/package=broom</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-06.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-08.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
