<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Supervised Machine Learning II | Environmental Systems Data Science</title>
  <meta name="description" content="Text book and exercises for the course Environmental System Data Science at ETH Zürich." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Supervised Machine Learning II | Environmental Systems Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Text book and exercises for the course Environmental System Data Science at ETH Zürich." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Supervised Machine Learning II | Environmental Systems Data Science" />
  
  <meta name="twitter:description" content="Text book and exercises for the course Environmental System Data Science at ETH Zürich." />
  

<meta name="author" content="Loïc Pellissier, Joshua Payne, Benjamin Stocker" />


<meta name="date" content="2021-03-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-06.html"/>
<link rel="next" href="ch-08.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-description"><i class="fa fa-check"></i>Course Description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objectives"><i class="fa fa-check"></i>Course Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#content"><i class="fa fa-check"></i>Content</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#useful-prerequisites"><i class="fa fa-check"></i>Useful Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-01.html"><a href="ch-01.html"><i class="fa fa-check"></i><b>1</b> Primers</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-01.html"><a href="ch-01.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="ch-01.html"><a href="ch-01.html#important-points-from-the-lecture"><i class="fa fa-check"></i><b>1.1.1</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ch-01.html"><a href="ch-01.html#tutorial"><i class="fa fa-check"></i><b>1.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ch-01.html"><a href="ch-01.html#working-with-jupyter-notebook"><i class="fa fa-check"></i><b>1.2.1</b> Working with Jupyter notebook</a></li>
<li class="chapter" data-level="1.2.2" data-path="ch-01.html"><a href="ch-01.html#using-git-for-version-control"><i class="fa fa-check"></i><b>1.2.2</b> Using Git for version control</a></li>
<li class="chapter" data-level="1.2.3" data-path="ch-01.html"><a href="ch-01.html#basics-of-r-code"><i class="fa fa-check"></i><b>1.2.3</b> Basics of R code</a></li>
<li class="chapter" data-level="1.2.4" data-path="ch-01.html"><a href="ch-01.html#reading-data-into-r"><i class="fa fa-check"></i><b>1.2.4</b> Reading data into R</a></li>
<li class="chapter" data-level="1.2.5" data-path="ch-01.html"><a href="ch-01.html#r-objects"><i class="fa fa-check"></i><b>1.2.5</b> R Objects</a></li>
<li class="chapter" data-level="1.2.6" data-path="ch-01.html"><a href="ch-01.html#data-visualisation"><i class="fa fa-check"></i><b>1.2.6</b> Data visualisation</a></li>
<li class="chapter" data-level="1.2.7" data-path="ch-01.html"><a href="ch-01.html#example-code-loops"><i class="fa fa-check"></i><b>1.2.7</b> Example code: Loops</a></li>
<li class="chapter" data-level="1.2.8" data-path="ch-01.html"><a href="ch-01.html#where-to-find-help"><i class="fa fa-check"></i><b>1.2.8</b> Where to find Help</a></li>
<li class="chapter" data-level="1.2.9" data-path="ch-01.html"><a href="ch-01.html#further-reading"><i class="fa fa-check"></i><b>1.2.9</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ch-01.html"><a href="ch-01.html#exercise"><i class="fa fa-check"></i><b>1.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-02.html"><a href="ch-02.html"><i class="fa fa-check"></i><b>2</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-02.html"><a href="ch-02.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ch-02.html"><a href="ch-02.html#data-transformation-with-dplyr"><i class="fa fa-check"></i><b>2.1.1</b> Data transformation with dplyr</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch-02.html"><a href="ch-02.html#data-visualisation-with-ggplot2"><i class="fa fa-check"></i><b>2.1.2</b> Data visualisation with ggplot2</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch-02.html"><a href="ch-02.html#tutorial-1"><i class="fa fa-check"></i><b>2.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-02.html"><a href="ch-02.html#dataset-1-half-hourly-flux-data"><i class="fa fa-check"></i><b>2.2.1</b> Dataset 1 (half-hourly flux data)</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-02.html"><a href="ch-02.html#dataset-2-daily-flux-data"><i class="fa fa-check"></i><b>2.2.2</b> Dataset 2 (daily flux data)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-02.html"><a href="ch-02.html#exercise-1"><i class="fa fa-check"></i><b>2.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-03.html"><a href="ch-03.html"><i class="fa fa-check"></i><b>3</b> Data variety</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-03.html"><a href="ch-03.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-03.html"><a href="ch-03.html#overview"><i class="fa fa-check"></i><b>3.1.1</b> Overview</a></li>
<li class="chapter" data-level="3.1.2" data-path="ch-03.html"><a href="ch-03.html#learning-objectives"><i class="fa fa-check"></i><b>3.1.2</b> Learning objectives</a></li>
<li class="chapter" data-level="3.1.3" data-path="ch-03.html"><a href="ch-03.html#key-points-of-the-lecture"><i class="fa fa-check"></i><b>3.1.3</b> Key points of the lecture</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-03.html"><a href="ch-03.html#tutorial-2"><i class="fa fa-check"></i><b>3.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-03.html"><a href="ch-03.html#overview-1"><i class="fa fa-check"></i><b>3.2.1</b> Overview</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-03.html"><a href="ch-03.html#modis-remote-download"><i class="fa fa-check"></i><b>3.2.2</b> MODIS remote download</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-03.html"><a href="ch-03.html#points-on-the-globe"><i class="fa fa-check"></i><b>3.2.3</b> Points on the globe</a></li>
<li class="chapter" data-level="3.2.4" data-path="ch-03.html"><a href="ch-03.html#shapefiles"><i class="fa fa-check"></i><b>3.2.4</b> Shapefiles</a></li>
<li class="chapter" data-level="3.2.5" data-path="ch-03.html"><a href="ch-03.html#rasters"><i class="fa fa-check"></i><b>3.2.5</b> Rasters</a></li>
<li class="chapter" data-level="3.2.6" data-path="ch-03.html"><a href="ch-03.html#key-points-of-the-tutorial"><i class="fa fa-check"></i><b>3.2.6</b> Key points of the tutorial</a></li>
<li class="chapter" data-level="3.2.7" data-path="ch-03.html"><a href="ch-03.html#bonus-species-occurrence-trait-data-and-pcas"><i class="fa fa-check"></i><b>3.2.7</b> Bonus: Species Occurrence, Trait Data and PCAs</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-03.html"><a href="ch-03.html#exercise-2"><i class="fa fa-check"></i><b>3.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch-03.html"><a href="ch-03.html#part-1-plotting-elevation-differences"><i class="fa fa-check"></i><b>3.3.1</b> Part 1: Plotting Elevation differences</a></li>
<li class="chapter" data-level="3.3.2" data-path="ch-03.html"><a href="ch-03.html#part-2-temperature-and-elevation-correlations"><i class="fa fa-check"></i><b>3.3.2</b> Part 2: Temperature and Elevation Correlations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-04.html"><a href="ch-04.html"><i class="fa fa-check"></i><b>4</b> Data Scraping</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-04.html"><a href="ch-04.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-04.html"><a href="ch-04.html#theory"><i class="fa fa-check"></i><b>4.1.1</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-04.html"><a href="ch-04.html#tutorial-3"><i class="fa fa-check"></i><b>4.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-04.html"><a href="ch-04.html#r-packages-and-functions"><i class="fa fa-check"></i><b>4.2.1</b> R-Packages and Functions</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-04.html"><a href="ch-04.html#the-fishbase-website"><i class="fa fa-check"></i><b>4.2.2</b> The FishBase website</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-04.html"><a href="ch-04.html#accessing-fishbase"><i class="fa fa-check"></i><b>4.2.3</b> Accessing FishBase</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-04.html"><a href="ch-04.html#scraping-numbers"><i class="fa fa-check"></i><b>4.2.4</b> Scraping Numbers</a></li>
<li class="chapter" data-level="4.2.5" data-path="ch-04.html"><a href="ch-04.html#scraping-text-snippetrs"><i class="fa fa-check"></i><b>4.2.5</b> Scraping Text Snippetrs</a></li>
<li class="chapter" data-level="4.2.6" data-path="ch-04.html"><a href="ch-04.html#scraping-tables"><i class="fa fa-check"></i><b>4.2.6</b> Scraping Tables</a></li>
<li class="chapter" data-level="4.2.7" data-path="ch-04.html"><a href="ch-04.html#the-fishbase-package"><i class="fa fa-check"></i><b>4.2.7</b> The Fishbase Package</a></li>
<li class="chapter" data-level="4.2.8" data-path="ch-04.html"><a href="ch-04.html#summary"><i class="fa fa-check"></i><b>4.2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-04.html"><a href="ch-04.html#case-study"><i class="fa fa-check"></i><b>4.3</b> Case Study</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-04.html"><a href="ch-04.html#creating-list-of-species"><i class="fa fa-check"></i><b>4.3.1</b> Creating List of Species</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-04.html"><a href="ch-04.html#extracting-iucn-status-for-all-species"><i class="fa fa-check"></i><b>4.3.2</b> Extracting IUCN Status for all species</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-04.html"><a href="ch-04.html#proportion-of-species-in-netherlands"><i class="fa fa-check"></i><b>4.3.3</b> Proportion of species in Netherlands</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-04.html"><a href="ch-04.html#cleaning-data-with-iucn-status"><i class="fa fa-check"></i><b>4.3.4</b> Cleaning data with IUCN Status</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-04.html"><a href="ch-04.html#maps-of-species-richness-and-red-list-species-proportions"><i class="fa fa-check"></i><b>4.3.5</b> Maps of species richness and Red List species proportions</a></li>
<li class="chapter" data-level="4.3.6" data-path="ch-04.html"><a href="ch-04.html#relation-of-basin-size-and-species-richness"><i class="fa fa-check"></i><b>4.3.6</b> Relation of basin size and species richness</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-04.html"><a href="ch-04.html#exercise-3"><i class="fa fa-check"></i><b>4.4</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-05.html"><a href="ch-05.html"><i class="fa fa-check"></i><b>5</b> Catch-up</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-05.html"><a href="ch-05.html#loops-in-r"><i class="fa fa-check"></i><b>5.1</b> Loops in R</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-05.html"><a href="ch-05.html#some-simple-examples"><i class="fa fa-check"></i><b>5.1.1</b> Some simple examples</a></li>
<li class="chapter" data-level="5.1.2" data-path="ch-05.html"><a href="ch-05.html#nested-loops"><i class="fa fa-check"></i><b>5.1.2</b> Nested loops</a></li>
<li class="chapter" data-level="5.1.3" data-path="ch-05.html"><a href="ch-05.html#exercise-4"><i class="fa fa-check"></i><b>5.1.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-05.html"><a href="ch-05.html#functional-programming-using-purr"><i class="fa fa-check"></i><b>5.2</b> Functional programming using purr</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-05.html"><a href="ch-05.html#shortcuts-in-a-purrr-function"><i class="fa fa-check"></i><b>5.2.1</b> Shortcuts in a <code>purrr</code> function</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-05.html"><a href="ch-05.html#workflow-nested-data-map-and-mutate"><i class="fa fa-check"></i><b>5.2.2</b> Workflow: nested data, map and mutate</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-05.html"><a href="ch-05.html#string-manipulations"><i class="fa fa-check"></i><b>5.3</b> String Manipulations</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-05.html"><a href="ch-05.html#introduction-to-strings"><i class="fa fa-check"></i><b>5.3.1</b> Introduction to strings</a></li>
<li class="chapter" data-level="5.3.2" data-path="ch-05.html"><a href="ch-05.html#matching-and-extracting-patterns"><i class="fa fa-check"></i><b>5.3.2</b> Matching and extracting patterns</a></li>
<li class="chapter" data-level="5.3.3" data-path="ch-05.html"><a href="ch-05.html#advanced-example"><i class="fa fa-check"></i><b>5.3.3</b> Advanced example</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-05.html"><a href="ch-05.html#web-scraping-in-a-nut-shell"><i class="fa fa-check"></i><b>5.4</b> Web-scraping in a nut-shell</a></li>
<li class="chapter" data-level="5.5" data-path="ch-05.html"><a href="ch-05.html#tidyverses-filter-and-select"><i class="fa fa-check"></i><b>5.5</b> Tidyverse’s filter and select</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="ch-05.html"><a href="ch-05.html#introduction-4"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="ch-05.html"><a href="ch-05.html#select"><i class="fa fa-check"></i><b>5.5.2</b> Select()</a></li>
<li class="chapter" data-level="5.5.3" data-path="ch-05.html"><a href="ch-05.html#filter"><i class="fa fa-check"></i><b>5.5.3</b> Filter()</a></li>
<li class="chapter" data-level="5.5.4" data-path="ch-05.html"><a href="ch-05.html#exercises"><i class="fa fa-check"></i><b>5.5.4</b> Exercises</a></li>
<li class="chapter" data-level="5.5.5" data-path="ch-05.html"><a href="ch-05.html#solutions"><i class="fa fa-check"></i><b>5.5.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="ch-05.html"><a href="ch-05.html#preparing-data-for-ggplot"><i class="fa fa-check"></i><b>5.6</b> Preparing data for ggplot()</a></li>
<li class="chapter" data-level="5.7" data-path="ch-05.html"><a href="ch-05.html#base-r-functions"><i class="fa fa-check"></i><b>5.7</b> Base R functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-06.html"><a href="ch-06.html"><i class="fa fa-check"></i><b>6</b> Supervised Machine Learning I</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-06.html"><a href="ch-06.html#introduction-5"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-06.html"><a href="ch-06.html#learning-objectives-1"><i class="fa fa-check"></i><b>6.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-06.html"><a href="ch-06.html#important-points-from-the-lecture-1"><i class="fa fa-check"></i><b>6.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-06.html"><a href="ch-06.html#tutorial-4"><i class="fa fa-check"></i><b>6.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-06.html"><a href="ch-06.html#linear-regression"><i class="fa fa-check"></i><b>6.2.1</b> Linear regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-06.html"><a href="ch-06.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>6.2.2</b> K-nearest neighbours</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-06.html"><a href="ch-06.html#essential-methods-for-the-modelling-process"><i class="fa fa-check"></i><b>6.2.3</b> Essential methods for the modelling process</a></li>
<li class="chapter" data-level="6.2.4" data-path="ch-06.html"><a href="ch-06.html#bonus"><i class="fa fa-check"></i><b>6.2.4</b> Bonus</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-06.html"><a href="ch-06.html#exercise-5"><i class="fa fa-check"></i><b>6.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-07.html"><a href="ch-07.html"><i class="fa fa-check"></i><b>7</b> Supervised Machine Learning II</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-07.html"><a href="ch-07.html#introduction-6"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-07.html"><a href="ch-07.html#leaerning-objectives"><i class="fa fa-check"></i><b>7.1.1</b> Leaerning objectives</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-07.html"><a href="ch-07.html#key-points-from-the-lecture"><i class="fa fa-check"></i><b>7.1.2</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-07.html"><a href="ch-07.html#tutorial-5"><i class="fa fa-check"></i><b>7.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-07.html"><a href="ch-07.html#model-training-and-the-loss-function"><i class="fa fa-check"></i><b>7.2.1</b> Model training and the loss function</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-07.html"><a href="ch-07.html#putting-it-all-together"><i class="fa fa-check"></i><b>7.2.2</b> Putting it all together</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-07.html"><a href="ch-07.html#model-evaluation"><i class="fa fa-check"></i><b>7.2.3</b> Model evaluation</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-07.html"><a href="ch-07.html#model-interpretation"><i class="fa fa-check"></i><b>7.2.4</b> Model interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-07.html"><a href="ch-07.html#exercise-6"><i class="fa fa-check"></i><b>7.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-08.html"><a href="ch-08.html"><i class="fa fa-check"></i><b>8</b> Application 1: Variable selection</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ch-08.html"><a href="ch-08.html#introduction-7"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="ch-08.html"><a href="ch-08.html#application"><i class="fa fa-check"></i><b>8.2</b> Application</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ch-08.html"><a href="ch-08.html#warm-up-1-nested-for-loop"><i class="fa fa-check"></i><b>8.2.1</b> Warm-up 1: Nested for-loop</a></li>
<li class="chapter" data-level="8.2.2" data-path="ch-08.html"><a href="ch-08.html#warm-up-2-find-the-best-single-predictor"><i class="fa fa-check"></i><b>8.2.2</b> Warm-up 2: Find the best single predictor</a></li>
<li class="chapter" data-level="8.2.3" data-path="ch-08.html"><a href="ch-08.html#full-stepwise-regression"><i class="fa fa-check"></i><b>8.2.3</b> Full stepwise regression</a></li>
<li class="chapter" data-level="8.2.4" data-path="ch-08.html"><a href="ch-08.html#bonus-stepwise-regression-out-of-the-box"><i class="fa fa-check"></i><b>8.2.4</b> Bonus: Stepwise regression out-of-the-box</a></li>
<li class="chapter" data-level="8.2.5" data-path="ch-08.html"><a href="ch-08.html#bonus-best-subset-selection"><i class="fa fa-check"></i><b>8.2.5</b> Bonus: Best Subset Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-09.html"><a href="ch-09.html"><i class="fa fa-check"></i><b>9</b> Supervised Neural Networks I</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-09.html"><a href="ch-09.html#introduction-8"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-09.html"><a href="ch-09.html#learning-objectives-2"><i class="fa fa-check"></i><b>9.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-09.html"><a href="ch-09.html#important-points-from-the-lecture-2"><i class="fa fa-check"></i><b>9.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-09.html"><a href="ch-09.html#tutorial-6"><i class="fa fa-check"></i><b>9.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-09.html"><a href="ch-09.html#set-up"><i class="fa fa-check"></i><b>9.2.1</b> Set-up</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-09.html"><a href="ch-09.html#keras-for-linear-models"><i class="fa fa-check"></i><b>9.2.2</b> Keras for linear models</a></li>
<li class="chapter" data-level="9.2.3" data-path="ch-09.html"><a href="ch-09.html#tuning-learning-rate"><i class="fa fa-check"></i><b>9.2.3</b> Tuning learning rate</a></li>
<li class="chapter" data-level="9.2.4" data-path="ch-09.html"><a href="ch-09.html#logistic-regression"><i class="fa fa-check"></i><b>9.2.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-09.html"><a href="ch-09.html#exercise-7"><i class="fa fa-check"></i><b>9.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-10.html"><a href="ch-10.html"><i class="fa fa-check"></i><b>10</b> Supervised Neural Networks II</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-10.html"><a href="ch-10.html#introduction-9"><i class="fa fa-check"></i><b>10.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch-10.html"><a href="ch-10.html#learning-objectives-3"><i class="fa fa-check"></i><b>10.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.1.2" data-path="ch-10.html"><a href="ch-10.html#important-points-from-the-lecture-3"><i class="fa fa-check"></i><b>10.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-10.html"><a href="ch-10.html#tutorial-7"><i class="fa fa-check"></i><b>10.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-10.html"><a href="ch-10.html#import-libraries-1"><i class="fa fa-check"></i><b>10.2.1</b> Import libraries</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-10.html"><a href="ch-10.html#construct-a-toy-dataset"><i class="fa fa-check"></i><b>10.2.2</b> Construct a toy dataset</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-10.html"><a href="ch-10.html#build-and-train-nn"><i class="fa fa-check"></i><b>10.2.3</b> Build and train NN</a></li>
<li class="chapter" data-level="10.2.4" data-path="ch-10.html"><a href="ch-10.html#model-performance"><i class="fa fa-check"></i><b>10.2.4</b> Model performance</a></li>
<li class="chapter" data-level="10.2.5" data-path="ch-10.html"><a href="ch-10.html#influence-of-nn-architecture"><i class="fa fa-check"></i><b>10.2.5</b> Influence of NN architecture</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ch-10.html"><a href="ch-10.html#exercise-8"><i class="fa fa-check"></i><b>10.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-11.html"><a href="ch-11.html"><i class="fa fa-check"></i><b>11</b> Application 2: Neural Networks and Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-11.html"><a href="ch-11.html#introduction-10"><i class="fa fa-check"></i><b>11.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-11.html"><a href="ch-11.html#learning-goals"><i class="fa fa-check"></i><b>11.1.1</b> Learning Goals</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-11.html"><a href="ch-11.html#key-points-from-previous-lectures"><i class="fa fa-check"></i><b>11.1.2</b> Key Points from Previous Lectures</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-11.html"><a href="ch-11.html#application-1"><i class="fa fa-check"></i><b>11.2</b> Application</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-11.html"><a href="ch-11.html#problem-statement"><i class="fa fa-check"></i><b>11.2.1</b> Problem Statement</a></li>
<li class="chapter" data-level="11.2.2" data-path="ch-11.html"><a href="ch-11.html#data-preparation"><i class="fa fa-check"></i><b>11.2.2</b> Data preparation</a></li>
<li class="chapter" data-level="11.2.3" data-path="ch-11.html"><a href="ch-11.html#center-and-scale"><i class="fa fa-check"></i><b>11.2.3</b> Center and scale</a></li>
<li class="chapter" data-level="11.2.4" data-path="ch-11.html"><a href="ch-11.html#building-a-simple-model-with-keras-subtask-1"><i class="fa fa-check"></i><b>11.2.4</b> Building a simple model with keras ( SubTask 1)</a></li>
<li class="chapter" data-level="11.2.5" data-path="ch-11.html"><a href="ch-11.html#cross-validation"><i class="fa fa-check"></i><b>11.2.5</b> Cross validation</a></li>
<li class="chapter" data-level="11.2.6" data-path="ch-11.html"><a href="ch-11.html#parameter-tuning"><i class="fa fa-check"></i><b>11.2.6</b> Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-12.html"><a href="ch-12.html"><i class="fa fa-check"></i><b>12</b> Supervised Deep Learning I</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-12.html"><a href="ch-12.html#introduction-11"><i class="fa fa-check"></i><b>12.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-12.html"><a href="ch-12.html#learning-objectives-4"><i class="fa fa-check"></i><b>12.1.1</b> Learning objectives</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-12.html"><a href="ch-12.html#tutorial-8"><i class="fa fa-check"></i><b>12.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-12.html"><a href="ch-12.html#building-blocks-of-cnns"><i class="fa fa-check"></i><b>12.2.1</b> Building Blocks of CNNs</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-12.html"><a href="ch-12.html#build-the-model"><i class="fa fa-check"></i><b>12.2.2</b> Build the model</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-12.html"><a href="ch-12.html#compile-and-train-the-model"><i class="fa fa-check"></i><b>12.2.3</b> Compile and train the model</a></li>
<li class="chapter" data-level="12.2.4" data-path="ch-12.html"><a href="ch-12.html#reduce-overfitting"><i class="fa fa-check"></i><b>12.2.4</b> Reduce Overfitting</a></li>
<li class="chapter" data-level="12.2.5" data-path="ch-12.html"><a href="ch-12.html#visualizing-a-cnn"><i class="fa fa-check"></i><b>12.2.5</b> Visualizing a CNN</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-12.html"><a href="ch-12.html#exercise-9"><i class="fa fa-check"></i><b>12.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ch-12.html"><a href="ch-12.html#import-libraries-and-data"><i class="fa fa-check"></i><b>12.3.1</b> Import libraries and data</a></li>
<li class="chapter" data-level="12.3.2" data-path="ch-12.html"><a href="ch-12.html#tasks"><i class="fa fa-check"></i><b>12.3.2</b> Tasks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-13.html"><a href="ch-13.html"><i class="fa fa-check"></i><b>13</b> Supervised Deep Learning II</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-13.html"><a href="ch-13.html#introduction-12"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-13.html"><a href="ch-13.html#learning-objectives-5"><i class="fa fa-check"></i><b>13.1.1</b> Learning objectives</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-13.html"><a href="ch-13.html#tutorial-9"><i class="fa fa-check"></i><b>13.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-13.html"><a href="ch-13.html#dataset"><i class="fa fa-check"></i><b>13.2.1</b> Dataset</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-13.html"><a href="ch-13.html#naive-models-old-world"><i class="fa fa-check"></i><b>13.2.2</b> Naive models (Old World)</a></li>
<li class="chapter" data-level="13.2.3" data-path="ch-13.html"><a href="ch-13.html#neural-networks-new-world"><i class="fa fa-check"></i><b>13.2.3</b> Neural Networks (New World)</a></li>
<li class="chapter" data-level="13.2.4" data-path="ch-13.html"><a href="ch-13.html#model-comparison"><i class="fa fa-check"></i><b>13.2.4</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ch-13.html"><a href="ch-13.html#exercise-10"><i class="fa fa-check"></i><b>13.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ch-13.html"><a href="ch-13.html#import-libraries-2"><i class="fa fa-check"></i><b>13.3.1</b> Import libraries</a></li>
<li class="chapter" data-level="13.3.2" data-path="ch-13.html"><a href="ch-13.html#load-data"><i class="fa fa-check"></i><b>13.3.2</b> Load data</a></li>
<li class="chapter" data-level="13.3.3" data-path="ch-13.html"><a href="ch-13.html#preprocess-ndvi-images"><i class="fa fa-check"></i><b>13.3.3</b> Preprocess NDVI images</a></li>
<li class="chapter" data-level="13.3.4" data-path="ch-13.html"><a href="ch-13.html#part-1"><i class="fa fa-check"></i><b>13.3.4</b> Part 1</a></li>
<li class="chapter" data-level="13.3.5" data-path="ch-13.html"><a href="ch-13.html#part-2"><i class="fa fa-check"></i><b>13.3.5</b> Part 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Environmental Systems Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-07" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Supervised Machine Learning II</h1>
<div id="introduction-6" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Introduction</h2>
<div id="leaerning-objectives" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Leaerning objectives</h3>
<p>After this learning unit, you will be able to …</p>
<ul>
<li>Explain the effect of hyper-parameter tuning in the context of the K-nearest neighbour algorithm.</li>
<li>Assess the generalisability of a trained model.</li>
<li>Explain the purpose and the method of cross-validation.</li>
<li>Avoid data leakage during model training.</li>
</ul>
</div>
<div id="key-points-from-the-lecture" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Key points from the lecture</h3>
<p>** Training data ** is the data that is used to train our model, “to fit a curve to the data points.”</p>
<p><strong>Testing data</strong> is set aside at the initial split and not “touched” during model training. It is key to test a model’s predictive power or whether it is overfitted.</p>
<p><strong>Validation data</strong> is used for determining the loss during model training. The reason for distinguishing between testing and validation data is to assure we’re not misleading model training by some peculiarities of the training data and we get an assessment of generalisability based on data that was not seen during model training. This distinction might be somewhat confusing for now, have a look at this <a href="https://machinelearningmastery.com/difference-test-validation-datasets/">blog post</a> for additional explanations.</p>
<p><strong>Model training</strong> minimises the loss. In other words, it optimises the agreement between predicted and observed values. The loss is most commonly measured by the root mean square error (RMSE).</p>
<p>To tune a model, you can set <em>hyperparameters</em> that determine model structure or calibrate the coefficients. The <em>k</em> in KNN is such a hyperparameter.</p>
<p><strong>Generalisability</strong> refers to the model’s performance on data not seen during the training - the testing data. To avoid overfitting, model generalisability is desired already during model training. One method to guide model training is <strong>cross validation</strong>.</p>
<p><strong>Data leakage</strong> is when data from the testing dataset creeps into the training data. To avoid this the testing set must be left completely untouched!</p>
</div>
</div>
<div id="tutorial-5" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Tutorial</h2>
<div id="model-training-and-the-loss-function" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Model training and the loss function</h3>
<p>Model training in supervised ML is guided by the match (or mismatch) between the predicted and observed target variable(s), that is, between <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(Y\)</span>. The <em>loss</em> function quantifies this mismatch (<span class="math inline">\(L(\hat{Y}, Y)\)</span>), and the algorithm takes care of progressively reducing the loss during model training. Let’s say the ML model contains two parameters and predictions can be considered a function of the two (<span class="math inline">\(\hat{Y}(w_1, w_2)\)</span>). <span class="math inline">\(Y\)</span> is actually constant. Thus, the loss function is effectively a function <span class="math inline">\(L(w_1, w_2)\)</span>. Therefore, we can consider the model training as a search of the parameter space of the machine learning model <span class="math inline">\((w_1, w_2)\)</span> to find the minimum of the loss. Common loss functions are the root mean square error (RMSE), or the mean square error, or the mean absolute error.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-353"></span>
<img src="figures/loss_plane.png" alt="Visualization of a loss function as a plane spanned by the two parameters $w_1$ and $w_2$." width="50%" />
<p class="caption">
Figure 7.1: Visualization of a loss function as a plane spanned by the two parameters <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>.
</p>
</div>
<p>Loss minimization is a general feature of ML model training. Practically all ML algorithms have some “knobs” to turn in order to achieve efficient model training and predictive performance. What these knobs are, depends on the ML algorithm.</p>
<p>In Video 6B you learned how the loss minimization, at least for some ML methods (e.g., artificial neural networks), is guided by <em>gradient descent</em>. It offers a principle for determining in what direction to jump and search the parameter space. <em>Gradient descent</em> changes parameters relative to a reference such that for a given “distance” of the “jump,” the loss is reduced by as much as possible. In other words, it descends along the steepest gradient of the loss hyperplane (the yellow-red plane in the figure above). You can imagine this as the trajectory of a ball rolling into the loss “depression.”</p>
<p>Model training is implemented in R for different algorithms in different packages. Some algorithms are even implemented by multiple packages (e.g., <code>nnet</code> and <code>neuralnet</code> for artificial neural networks). As described in Chapter <a href="ch-06.html#ch-06">6</a>, the <strong>caret</strong> package provides “wrappers” that handle a large selection of different ML model implementations in different packages with a unified interface (see <a href="https://topepo.github.io/caret/available-models.html">here</a> for an overview of available models). The <strong>caret</strong> function <code>train()</code> is the centre piece. Its argument <code>metric</code> specifies the loss function and defaults to RMSE for regression models and accuracy for classification (see sub-section on metrics below). A complete implementation of model training with caret is demonstrated further below.</p>
<div id="hyperparameter-tuning" class="section level4" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Hyperparameter tuning</h4>
<p>As for practically all ML algorithms, there are free parameters that determine their characteristics and how the learning operates - <em>hyperparameters</em>. Turning back to the analogue of the ball and the loss depression, imagine the effect of how fast the ball mentioned before is rolling. If it rolls too fast (that is if the “jump” of the parameter search is too large), it descends into the depression fast but may shoot beyond the minimum and it has to do a “180-degrees turn” and continue the search. That’s not very efficient. On the other extreme, a very slowly rolling ball (the parameter space is searched with small jumps) will take much longer to arrive at the bottom of the depression. Hence, there is an optimum in between. This “size of the jump” is called the <em>learning rate</em>, and it usually has to be tuned for optimal performance of model training of artificial neural networks. Other ML algorithms have other types of hyperparameters. While some hyperparameters determine the performance of the model training, other hyperparameters are decisive for the model’s predictive skills. Hyperparameters are not to be confused with the model coefficients. For example, in linear regression, the number of predictors can be considered a hyperparameter, while the values of <span class="math inline">\(\beta\)</span> are the coefficients. In general, hyperparameters determine the <em>structure</em> of the algorithm.</p>
<p>Let’s turn to the K-nearest neighbour (KNN) algorithm as an example. In KNN, the hyperparameter is <span class="math inline">\(k\)</span>. That is, the number of neighbours to consider taking their mean. With KNN, there is always an optimum <span class="math inline">\(k\)</span>. Obviously, if <span class="math inline">\(k = n\)</span>, we consider all observations as neighbours and each prediction is simply the mean of all observed target values <span class="math inline">\(Y\)</span>, irrespective of the predictor values. This cannot be optimal and such a model will likely underfit. On the other extreme, with <span class="math inline">\(k = 1\)</span>, the model will be strongly affected by the noise in the single nearest neighbour and its generalisability will suffer. This should be reflected in a poor performance on the validation data. Indeed, it is, as the Figure <a href="ch-07.html#fig:hyper">7.2</a> illustrates.</p>
<div class="figure" style="text-align: center"><span id="fig:hyper"></span>
<img src="figures/knn_hyperparameter_tuning.png" alt="Improvement in RMSE on validation data with increasing number of neighbours." width="50%" />
<p class="caption">
Figure 7.2: Improvement in RMSE on validation data with increasing number of neighbours.
</p>
</div>
<p>You will encounter different hyperparameters for neural networks in later chapters.</p>
<p>In <strong>caret</strong>, hyperparameter tuning is implemented as part of the <code>train()</code> function. Values of hyperparameters to consider are to be specified by the argument <code>tuneGrid</code>, which takes a data frame with column(s) named according to the name(s) of the hyperparameter(s) and rows for each combination of hyperparameters to consider. More explicit examples follow below.</p>
</div>
<div id="resampling" class="section level4" number="7.2.1.2">
<h4><span class="header-section-number">7.2.1.2</span> Resampling</h4>
<p>At the beginning of this tutorial, we demonstrated a case of overfitting. In the example with KNN above, the increasing error on the validation data (measured by RMSE) is an indication of poor <em>generalisability</em>. The goal of model training is to achieve the best possible generalisation performance. That is, the lowest validation error measured by applying the trained model on the testing data set from the initial split. Note the distinction: validation data is what is used during model training, testing data is held out at the initial split and not used during model training.</p>
<p>But how can the generalisability be assessed when the testing set is held out completely during the training step? To measure the model’s generalisability and “direct” the ML algorithm to minimize the validation error during the training step, we can further split the training set into one or more training and validation sub-set. This is called <em>resampling</em>. The model performance determined on this resampled validation set is then a good estimate of the generalisation error we get when evaluated against the testing data that was set aside from the initial split.</p>
<p>The challenge to be met here is that we further reduce the number of data points in the resampled training and validation sets which may lead to evaluation statistics not being sufficiently robust and bears the potential that we’re training to some peculiarities in the (relatively small) validation set. In order to control for this, common practice is to do multiple resamples (multiple <em>folds</em> of training-validation splits) which is the <em>repeat</em> part of repeated CV.</p>
<p>By repeating cross-validation we are picking different folds for training and validating. For each repetition, we can determine the validation error, i.e., how well the trained model performed on the validation fold. Taking the mean performance across all repetitions gives us the average performance we would expect for the model to have on unseen data. This provides a general assessment of how good the model may perform. To actually check for its performance, we can use the training data set that we excluded from the beginning and was not used in the CV process.</p>
<p>Taking the mean validation error across all repetitions, we can This is called <em>k-fold cross validation</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-354"></span>
<img src="figures/cv.png" alt="Example of 5-fold cross validation. Figure taken from Figure from [Bradley &amp; Boehmke](https://bradleyboehmke.github.io/HOML/process.html#k-fold-cross-validation). Note that the orange boxes called *Test* refer to a test fold which we refer to as validation set. Be precise here, this *Test* is not the held-out test data set!" width="50%" />
<p class="caption">
Figure 7.3: Example of 5-fold cross validation. Figure taken from Figure from <a href="https://bradleyboehmke.github.io/HOML/process.html#k-fold-cross-validation">Bradley &amp; Boehmke</a>. Note that the orange boxes called <em>Test</em> refer to a test fold which we refer to as validation set. Be precise here, this <em>Test</em> is not the held-out test data set!
</p>
</div>
<p>There is no formal rule about the number of folds and hence the number of data points in each training and testing fold. <em>Leave-one-out</em> cross validation is an extreme variant of k-fold cross validation, where k equals the number of data points in the full set of training data.</p>
<p>To do a hyperparameter tuning and k-fold cross validation during model training in R, we don’t have to implement the loops ourselves. The resampling procedure can be specified in the <strong>caret</strong> function <code>train()</code> with the argument <code>trControl</code>. The object that this argument takes is the output of a function call to <code>trainControl()</code>. This can be implemented in two steps. For example, to do a 10-fold cross-validation, repeated five times, we can write:</p>
<div class="sourceCode" id="cb777"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb777-1"><a href="ch-07.html#cb777-1" aria-hidden="true" tabindex="-1"></a>my_cv <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb777-2"><a href="ch-07.html#cb777-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb777-3"><a href="ch-07.html#cb777-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb777-4"><a href="ch-07.html#cb777-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">repeats =</span> <span class="dv">5</span></span>
<span id="cb777-5"><a href="ch-07.html#cb777-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb777-6"><a href="ch-07.html#cb777-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb777-7"><a href="ch-07.html#cb777-7" aria-hidden="true" tabindex="-1"></a><span class="fu">train</span>(..., <span class="at">trControl =</span> my_cv)</span></code></pre></div>
<p><strong>Bonus Material</strong></p>
<p>An example of model training in environmental sciences is where we have time series data from multiple sites and we are interested in making a prediction of temporal dynamics at an entirely new site. In this case, the data has an additional “dimension” (location in addition to time), and the generalisability with respect to location has to be determined where the data splitting is done along data belonging to different sites. In other words, we have to make sure that one site’s data is entirely either in the training <em>or</em> testing set, but never in both. Otherwise, the validation error, measuring the generalisability of the model to a new site, is too optimistic.</p>
<p>This can be done using the <a href="https://topepo.github.io/caret/data-splitting.html#simple-splitting-with-important-groups"><code>groupKFold</code></a> function of the <strong>caret</strong> package as follows, using a site identify vector that has the same length as the training data has rows (<code>df_train$siteid</code> below).</p>
<div class="sourceCode" id="cb778"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb778-1"><a href="ch-07.html#cb778-1" aria-hidden="true" tabindex="-1"></a>cv <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb778-2"><a href="ch-07.html#cb778-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,  <span class="co"># not repeated CV</span></span>
<span id="cb778-3"><a href="ch-07.html#cb778-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">index =</span> <span class="fu">groupKFold</span>(df_train<span class="sc">$</span>siteid, <span class="at">k =</span> <span class="dv">10</span>)</span>
<span id="cb778-4"><a href="ch-07.html#cb778-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
</div>
<div id="putting-it-all-together" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Putting it all together</h3>
<p>Now, we know enough to turn to the implementation of a full model training workflow in R. Below, we implement the example of hyperparameter tuning of the KNN algorithm for which you saw the visualisation above. The workhorse for model training with <strong>caret</strong> is the function <code>train()</code>. Let’s recap its arguments:</p>
<ul>
<li>A formula specifying the model of the form <code>&lt;target&gt; ~ &lt;predictor1&gt; + &lt;predictor2&gt; + ...</code>; or <code>x</code> (a data frame with predictors) and <code>y</code> (a vector of the target variable, see section <em>Model formulation</em> in Chapter <a href="ch-06.html#ch-06">6</a>); or a recipe (object returned by a <code>recipes::recipe()</code> function call).</li>
<li><code>data</code>: The data set containing the target and predictor variables as separate columns. This is required only when using a formula or recipe as the first argument.</li>
<li><code>method</code>: The machine learning algorithm (the “engine”) to be used. We’ll use <code>"knn"</code> here. For an overview of available algorithms see <a href="http://topepo.github.io/caret/train-models-by-tag.html">here</a>.</li>
<li><code>metric</code>: The loss function. For regression problems, it defaults to <code>"RMSE"</code>.</li>
<li><code>preProcess</code>: The pre-processing instructions given as a vector of strings. This can also be a “recipe,” i.e., the output of a function call to <code>recipe()</code> of the recipes package (see sub-section <em>Pre-processing</em> in Chapter <a href="ch-06.html#ch-06">6</a>)</li>
<li><code>tuneGrid</code>: A data frame with combinations of hyperparameter values. The columns are named the same as the tuning parameters. In the KNN example below, this is simply <code>k</code>.</li>
<li><code>trControl</code>: The output of a function call to <code>trainControl()</code> (see above).</li>
</ul>
<p>We’ve learned about the danger of <em>data leakage</em>. It happens when information from the testing data somehow finds its way into the training step. This is relevant not only for leakage from the testing set to the training set determined from the initial split but also for the resampling splits within each resample fold. Therefore, we should apply the pre-processing blueprint independently on each fold. For example, we should do the centering and scaling by subtracting the mean and dividing by the standard deviation, where the mean and standard deviation are calculated separately on each fold of the training resamples (not including the validation resample). This is illustrated in Figure <a href="ch-07.html#fig:leakage">7.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:leakage"></span>
<img src="figures/minimize-leakage.png" alt="Visualization of how data leakage can be avoided. Figure taken from Figure from [Bradley &amp; Boehmke](https://bradleyboehmke.github.io/HOML/engineering.html#proper-implementation)." width="50%" />
<p class="caption">
Figure 7.4: Visualization of how data leakage can be avoided. Figure taken from Figure from <a href="https://bradleyboehmke.github.io/HOML/engineering.html#proper-implementation">Bradley &amp; Boehmke</a>.
</p>
</div>
<p>This also illustrates why the pre-processing step has to be considered as a blueprint that is applied to several different subsets of the data during model training. Of course, this also means that the pre-processing is not to actually be executed on the data before training, but will be executed, following the same general rules, several times anew during training.</p>
<p>Let’s finally turn to the implementation with KNN. First, we make the initial data split.</p>
<div class="sourceCode" id="cb779"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb779-1"><a href="ch-07.html#cb779-1" aria-hidden="true" tabindex="-1"></a><span class="co"># As always we start by loading the packages we&#39;ll need</span></span>
<span id="cb779-2"><a href="ch-07.html#cb779-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb779-3"><a href="ch-07.html#cb779-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb779-4"><a href="ch-07.html#cb779-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb779-5"><a href="ch-07.html#cb779-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(recipes)</span>
<span id="cb779-6"><a href="ch-07.html#cb779-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb779-7"><a href="ch-07.html#cb779-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<div class="sourceCode" id="cb780"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb780-1"><a href="ch-07.html#cb780-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Load daily data from all sites</span></span>
<span id="cb780-2"><a href="ch-07.html#cb780-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;./data/ddf_allsites_nested_joined.RData&quot;</span>)</span>
<span id="cb780-3"><a href="ch-07.html#cb780-3" aria-hidden="true" tabindex="-1"></a><span class="co">#load(&quot;ddf_allsites.RData&quot;)</span></span>
<span id="cb780-4"><a href="ch-07.html#cb780-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb780-5"><a href="ch-07.html#cb780-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Load daily data and rename for use below</span></span>
<span id="cb780-6"><a href="ch-07.html#cb780-6" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;./data/ddf_ch_lae.RData&quot;</span>) <span class="co"># loads &#39;ddf_ch_lae&#39;</span></span>
<span id="cb780-7"><a href="ch-07.html#cb780-7" aria-hidden="true" tabindex="-1"></a><span class="co">#load(&quot;ddf_ch_lae.RData&quot;)</span></span>
<span id="cb780-8"><a href="ch-07.html#cb780-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb780-9"><a href="ch-07.html#cb780-9" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> ddf_ch_lae <span class="sc">%&gt;%</span> </span>
<span id="cb780-10"><a href="ch-07.html#cb780-10" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>NEE_VUT_REF_QC, <span class="sc">-</span>TIMESTAMP) <span class="sc">%&gt;%</span>  <span class="co"># not numeric features</span></span>
<span id="cb780-11"><a href="ch-07.html#cb780-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">drop_na</span>()                                <span class="co"># drop rows with missing data</span></span></code></pre></div>
<div class="sourceCode" id="cb781"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb781-1"><a href="ch-07.html#cb781-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb781-2"><a href="ch-07.html#cb781-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb781-3"><a href="ch-07.html#cb781-3" aria-hidden="true" tabindex="-1"></a>index_caret <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(</span>
<span id="cb781-4"><a href="ch-07.html#cb781-4" aria-hidden="true" tabindex="-1"></a>  df<span class="sc">$</span>GPP_NT_VUT_REF, <span class="at">p =</span> <span class="fl">0.7</span>,</span>
<span id="cb781-5"><a href="ch-07.html#cb781-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">list =</span> <span class="cn">FALSE</span></span>
<span id="cb781-6"><a href="ch-07.html#cb781-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb781-7"><a href="ch-07.html#cb781-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb781-8"><a href="ch-07.html#cb781-8" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb781-9"><a href="ch-07.html#cb781-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(index_caret)</span>
<span id="cb781-10"><a href="ch-07.html#cb781-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb781-11"><a href="ch-07.html#cb781-11" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb781-12"><a href="ch-07.html#cb781-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="sc">-</span>index_caret)</span></code></pre></div>
<p>We then define the pre-processing blueprint as <code>pp</code>.</p>
<div class="sourceCode" id="cb782"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb782-1"><a href="ch-07.html#cb782-1" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">recipe</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> df_train) <span class="sc">%&gt;%</span></span>
<span id="cb782-2"><a href="ch-07.html#cb782-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_center</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span>        <span class="co"># normalizes numeric data to have a mean of zero</span></span>
<span id="cb782-3"><a href="ch-07.html#cb782-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_scale</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>())             <span class="co"># normalizes numeric data to have a standard deviation of one</span></span></code></pre></div>
<p>Next, we specify the parameters determining the model training. Here, we’re doing a 10-fold cross validation, repeated five times.</p>
<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb783-1"><a href="ch-07.html#cb783-1" aria-hidden="true" tabindex="-1"></a>my_cv <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb783-2"><a href="ch-07.html#cb783-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,      <span class="co"># method define the resampling method such as &#39;boot&#39;, &#39;none&#39;, &#39;cv&#39;, etc.</span></span>
<span id="cb783-3"><a href="ch-07.html#cb783-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">number =</span> <span class="dv">10</span>,                <span class="co"># number of folds or number of resampling iterations</span></span>
<span id="cb783-4"><a href="ch-07.html#cb783-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">repeats =</span> <span class="dv">5</span>                 <span class="co"># the number of complete sets of folds to compute (only for repeated k-fold cross-validation)</span></span>
<span id="cb783-5"><a href="ch-07.html#cb783-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Then the hyperparameter search is specified.</p>
<div class="sourceCode" id="cb784"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb784-1"><a href="ch-07.html#cb784-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid of hyperparameter values</span></span>
<span id="cb784-2"><a href="ch-07.html#cb784-2" aria-hidden="true" tabindex="-1"></a>hyper_grid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">18</span>, <span class="dv">20</span>, <span class="dv">22</span>, <span class="dv">24</span>, <span class="dv">26</span>, <span class="dv">30</span>, <span class="dv">35</span>, <span class="dv">40</span>, <span class="dv">60</span>, <span class="dv">100</span>))</span></code></pre></div>
<p>This object will be provided to <code>train()</code> through its argument <code>tuneGrid</code>. It requires a data frame where column names correspond to the hyperparameters available for the particular model, and rows are for each combination of hyperparameters to be assessed.</p>
<p>We can now combine it all with the <code>train()</code> function. We supply the pre-processing blueprint <code>pp</code> as the first argument to the train function. Using recipes in combination with <strong>caret</strong> offers a nice way to combine the clear way of specifying the pre-processing blueprint with recipes and the handy <code>train()</code> function of <strong>caret</strong>.</p>
<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb785-1"><a href="ch-07.html#cb785-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb785-2"><a href="ch-07.html#cb785-2" aria-hidden="true" tabindex="-1"></a>knn_fit <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb785-3"><a href="ch-07.html#cb785-3" aria-hidden="true" tabindex="-1"></a>  pp,</span>
<span id="cb785-4"><a href="ch-07.html#cb785-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> df_train,</span>
<span id="cb785-5"><a href="ch-07.html#cb785-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb785-6"><a href="ch-07.html#cb785-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">trControl =</span> my_cv,</span>
<span id="cb785-7"><a href="ch-07.html#cb785-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> hyper_grid,</span>
<span id="cb785-8"><a href="ch-07.html#cb785-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span></span>
<span id="cb785-9"><a href="ch-07.html#cb785-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb785-10"><a href="ch-07.html#cb785-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb785-11"><a href="ch-07.html#cb785-11" aria-hidden="true" tabindex="-1"></a>knn_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(knn_fit) <span class="sc">+</span> <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;KNN&quot;</span>, <span class="at">subtitle =</span> <span class="st">&quot;RMSE on validation data&quot;</span>)</span>
<span id="cb785-12"><a href="ch-07.html#cb785-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb785-13"><a href="ch-07.html#cb785-13" aria-hidden="true" tabindex="-1"></a>knn_plot</span></code></pre></div>
<p><img src="figures/knn_plot.png" /></p>
<p>This illustrates the generalisation error estimated from repeated k-fold cross-validation as a function of the number of neighbours. As shown in the table above, the lowest RMSE of 2.487583 was achieved with <code>k=24</code>. This model was then saved directly in the variable <code>knn_fit</code></p>
<p>Let’s apply the final <em>best-performing model</em> on the validation data from the initial split that was held out completely during model training. The final model is automatically used when predicting values with new data using the generic <code>predict()</code> function.</p>
<div class="sourceCode" id="cb786"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb786-1"><a href="ch-07.html#cb786-1" aria-hidden="true" tabindex="-1"></a>df_test<span class="sc">$</span>GPP_NT_VUT_REF_predicted <span class="ot">&lt;-</span> <span class="fu">predict</span>(knn_fit, <span class="at">newdata =</span> df_test)</span>
<span id="cb786-2"><a href="ch-07.html#cb786-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb786-3"><a href="ch-07.html#cb786-3" aria-hidden="true" tabindex="-1"></a>df_test <span class="sc">%&gt;%</span></span>
<span id="cb786-4"><a href="ch-07.html#cb786-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GPP_NT_VUT_REF_predicted, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb786-5"><a href="ch-07.html#cb786-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb786-6"><a href="ch-07.html#cb786-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span><span class="dv">0</span>, <span class="at">slope=</span><span class="dv">1</span>, <span class="at">linetype=</span><span class="st">&quot;dotted&quot;</span>) <span class="sc">+</span></span>
<span id="cb786-7"><a href="ch-07.html#cb786-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb786-8"><a href="ch-07.html#cb786-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Predicted GPP&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Observed GPP&quot;</span>) <span class="sc">+</span></span>
<span id="cb786-9"><a href="ch-07.html#cb786-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>, <span class="at">color=</span><span class="st">&quot;red&quot;</span>, <span class="at">size=</span><span class="fl">0.5</span>, <span class="at">se=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-365-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb787-1"><a href="ch-07.html#cb787-1" aria-hidden="true" tabindex="-1"></a>df_test <span class="sc">%&gt;%</span> yardstick<span class="sc">::</span><span class="fu">metrics</span>(GPP_NT_VUT_REF, GPP_NT_VUT_REF_predicted)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       2.47 
## 2 rsq     standard       0.666
## 3 mae     standard       1.87</code></pre>
<p>This looks reasonable. The KNN model, using 10 predictors for modelling daily GPP at the SwissFluxNet site CH-Lae (Lägern) achieves an R<span class="math inline">\(^2\)</span> of 0.67 and an RMSE of 2.46 gC m<span class="math inline">\(^-2\)</span> d<span class="math inline">\(^-1\)</span>. The slope of the relationship between observed vs. modelled values falls very closely onto the 1:1 line (dotted), indicating that the model captured the relationships reliably across the full range of values. Let’s keep the evaluation metrics as a reference in mind and remember it in later chapters when we model this data with other algorithms. Interestingly, the evaluation on the testing set from the initial split is even slightly better than estimated from the k-fold cross validation.</p>
</div>
<div id="model-evaluation" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Model evaluation</h3>
<p>In previous chapters, you have already encountered different metrics that can be used to quantify the agreement between predicted (<span class="math inline">\(\hat{y}\)</span>) and observed (<span class="math inline">\(y\)</span>) values (e.g., the root mean square error). These metrics are essential to guide model training (where the metric defines the loss function) and inform the evaluation. Different metrics measure different aspects of the model-data agreement and different metrics are used for regression and classification models.</p>
<div id="metrics-for-regression-models" class="section level4" number="7.2.3.1">
<h4><span class="header-section-number">7.2.3.1</span> Metrics for regression models</h4>
<p>Different metrics measure, for example, the correlation between modeled and observed values or the magnitude of the errors. To get an intuitive understanding of their different abilities, compare the scatterplots in Figure <a href="ch-07.html#fig:metrics">7.5</a> and how different aspects of the model-data agreement are measured by different metrics. Their definitions will follow below.</p>
<div class="figure" style="text-align: center"><span id="fig:metrics"></span>
<img src="figures/correlation_error.png" alt="Comparison of model metrics on different data sets." width="50%" />
<p class="caption">
Figure 7.5: Comparison of model metrics on different data sets.
</p>
</div>
<ul>
<li><p><strong>MSE</strong>: The mean squared error is defined, as its name suggests, as:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2
\]</span>
It measures accuracy, i.e., the magnitude of the errors, and is minimized during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is particularly sensitive to large errors in single points (including outliers). You may notice the difference to the error variance <span class="math inline">\(\widehat{\sigma}^2\)</span>. It was defined in Chapter <a href="ch-06.html#ch-06">6</a> similar to the definition of the MSE above, but with <span class="math inline">\(n-p\)</span> in the denominator instead of <span class="math inline">\(n\)</span>. The denominator <span class="math inline">\(n-p\)</span> corresponds to the <em>degrees of freedom</em> (size of our sample minus the number of parameters to estimate). Here we want to compute the mean of the errors, hence we divide by <span class="math inline">\(n\)</span>.</p></li>
<li><p><strong>RMSE</strong>: The root mean squared error is, as its name suggests, the root of the MSE:
<span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2}
\]</span>
Like the MSE, the RMSE also measures accuracy (the magnitude of the errors) and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data <span class="math inline">\(y\)</span> and is less sensitive to outliers as the MSE.</p></li>
</ul>
<p><strong>Checkpoint</strong></p>
<p>Implement the formula for RMSE using simple “low-level” functions like <code>sqrt()</code> and <code>mean()</code>. Confirm that the function <code>rmse()</code> from the yardstick package computes the RMSE the same way.</p>
<p><strong>Solution</strong></p>
<div class="sourceCode" id="cb789"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb789-1"><a href="ch-07.html#cb789-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generate random data </span></span>
<span id="cb789-2"><a href="ch-07.html#cb789-2" aria-hidden="true" tabindex="-1"></a>df_demo <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb789-3"><a href="ch-07.html#cb789-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_obs =</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>),</span>
<span id="cb789-4"><a href="ch-07.html#cb789-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">y_pred =</span> x)</span>
<span id="cb789-5"><a href="ch-07.html#cb789-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb789-6"><a href="ch-07.html#cb789-6" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rmse</span>(df_demo, y_obs, y_pred) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span></code></pre></div>
<pre><code>## [1] 1.151575</code></pre>
<div class="sourceCode" id="cb791"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb791-1"><a href="ch-07.html#cb791-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> df_demo<span class="sc">$</span>y_obs)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 1.151575</code></pre>
<ul>
<li><p><strong><span class="math inline">\(R^2\)</span></strong>, also called the <em>coefficient of determination</em>, describes the proportion of variation in <span class="math inline">\(y\)</span> that is captured by modelled values <span class="math inline">\(\hat{y}\)</span>. In this case, the goal is to maximize the metric, thus trying the explain as much variation as possible. In contrast to the MSE and RMSE, <span class="math inline">\(R^2\)</span> measures consistency, or correlation, or goodness of fit, and not accuracy. It is traditionally defined as:
<span class="math display">\[
R^2 = 1 - \frac{\sum_i (\hat{y}_i - y_i)^2}{\sum_i (y_i - \bar{y})^2}
\]</span>
A perfect fit is quantified by <span class="math inline">\(R^2 = 1\)</span> and uninformative estimates have an <span class="math inline">\(R^2\)</span> approaching zero when compared to observations.</p></li>
<li><p><strong>Pearson’s <span class="math inline">\(r^2\)</span></strong>: The linear association between to variables (here <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span>) is measured by the <em>Pearson’s correlation coefficient</em> <span class="math inline">\(r\)</span>. Its square is closely related to the coefficient of determination and in common cases of prediction-observation comparisons almost identical.
<span class="math display">\[
r = \frac{\sum_i (y_i - \bar{y}) (\hat{y_i} - \hat{\bar{y}}) }{\sqrt{ \sum_i(y_i-\bar{y})^2(\hat{y_i}-\hat{\bar{y}})^2 } }
\]</span></p></li>
</ul>
<p>The distinction between uppercase and lowercase nomenclature is often not consistent in the literature. The uppercase <span class="math inline">\(R^2\)</span> is commonly used in the context of comparing observed and predicted values with the coefficient of determination. When the <em>correlation</em> between two different variables in a sample is quantified, the lowercase <span class="math inline">\(r^2\)</span> is commonly used. In a linear regression with an estimated intercept, the coefficient of determination and the square of the Pearson’s correlation coefficient are equal. However, when comparing estimated and observed values, the coefficient of determination can return negative values for uninformative estimates.</p>
<p>Metrics for correlation should not be used as a loss function because they do not penalise biased models. This is illustrated also in the plots above.</p>
<p>The yardstick library implements the definition of the coefficient of determination with its function <code>rsq_trad()</code>.</p>
<div class="sourceCode" id="cb793"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb793-1"><a href="ch-07.html#cb793-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generate random data </span></span>
<span id="cb793-2"><a href="ch-07.html#cb793-2" aria-hidden="true" tabindex="-1"></a>df_demo <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb793-3"><a href="ch-07.html#cb793-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_obs =</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>),</span>
<span id="cb793-4"><a href="ch-07.html#cb793-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">y_pred =</span> x)</span>
<span id="cb793-5"><a href="ch-07.html#cb793-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb793-6"><a href="ch-07.html#cb793-6" aria-hidden="true" tabindex="-1"></a><span class="do">## the equation given above for the coefficient of determination corresponds to &#39;rsq_trad()&#39;</span></span>
<span id="cb793-7"><a href="ch-07.html#cb793-7" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> df_demo<span class="sc">$</span>y_obs)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">sum</span>((df_demo<span class="sc">$</span>y_obs <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_obs))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.540435</code></pre>
<div class="sourceCode" id="cb795"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb795-1"><a href="ch-07.html#cb795-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq_trad</span>(df_demo, y_obs, y_pred) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span></code></pre></div>
<pre><code>## [1] 0.540435</code></pre>
<p>The square of the Pearson’s correlation coefficient, as defined above, is implemented by the yardstick function <code>rsq()</code>, and corresponds also to the value reported for <code>Multiple R-squared</code> by <code>summary()</code> on a linear model object, or simply to <code>cor(...)^2</code>.</p>
<div class="sourceCode" id="cb797"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb797-1"><a href="ch-07.html#cb797-1" aria-hidden="true" tabindex="-1"></a><span class="do">## the equation given above for the squared Pearson&#39;s correlation coefficient corresponds to &#39;rsq()&#39;, &#39;cor()^2&#39;, and `summary()$r.squared</span></span>
<span id="cb797-2"><a href="ch-07.html#cb797-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb797-3"><a href="ch-07.html#cb797-3" aria-hidden="true" tabindex="-1"></a>(<span class="fu">sum</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_pred))<span class="sc">*</span>(df_demo<span class="sc">$</span>y_obs <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_obs))))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span></span>
<span id="cb797-4"><a href="ch-07.html#cb797-4" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">sum</span>((df_demo<span class="sc">$</span>y_obs <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_obs))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">sum</span>((df_demo<span class="sc">$</span>y_pred <span class="sc">-</span> <span class="fu">mean</span>(df_demo<span class="sc">$</span>y_pred))<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.5426649</code></pre>
<div class="sourceCode" id="cb799"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb799-1"><a href="ch-07.html#cb799-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq</span>(df_demo, y_obs, y_pred) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span></code></pre></div>
<pre><code>## [1] 0.5426649</code></pre>
<div class="sourceCode" id="cb801"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb801-1"><a href="ch-07.html#cb801-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y_obs <span class="sc">~</span> y_pred, <span class="at">data =</span> df_demo))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.5426649</code></pre>
<div class="sourceCode" id="cb803"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb803-1"><a href="ch-07.html#cb803-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(df_demo<span class="sc">$</span>y_obs, df_demo<span class="sc">$</span>y_pred)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.5426649</code></pre>
<p>The <span class="math inline">\(R^2\)</span> generally increases when predictors are added to a model, even if predictors are not informative. This is particularly critical in the context of machine learning when we compare alternative models that differ by their number of predictors. In other words, the <span class="math inline">\(R^2\)</span> of a model with a large number of predictors tends to give an overconfident estimate of its predictive power. We have encountered cross-validation which yields good predictive power. This is the “gold-standard.” But when the number of data points is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalising by the number of predictors <span class="math inline">\(p\)</span>. Different metrics are available.</p>
<ul>
<li><p><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>: The adjusted <span class="math inline">\(R^2\)</span> discounts values by the number of predictors as
<span class="math display">\[
\bar{R}^2 = 1 - (1-R^2) \; \frac{n-1}{n-p-1} \;,
\]</span>
where <span class="math inline">\(n\)</span> (as before) is the number of observations and <span class="math inline">\(p\)</span> the number of predictors. As for <span class="math inline">\(R^2\)</span>, the goal is to maximize it. For a fitted model in R <code>modl</code>, it is returned by <code>summary(modl)$adj.r.squared</code>.</p></li>
<li><p><strong>AIC</strong>: the Akaike’s Information Criterion is defined as
<span class="math display">\[
\text{AIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + 2(p+2)
\]</span>
where <span class="math inline">\(N\)</span> is the number of observations used for estimation, <span class="math inline">\(p\)</span> is the number of predictors in the model and SSE is the sum of squared estimate of errors (SSE<span class="math inline">\(= \sum_i (y_i-\hat{y_i})^2\)</span>). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for forecasting. For large values of<br />
<span class="math inline">\(n\)</span>, minimising the AIC is equivalent to minimising the cross-validated MSE.</p></li>
<li><p><strong>AIC<span class="math inline">\(_c\)</span></strong>: For small values of <span class="math inline">\(n\)</span> the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as:
<span class="math display">\[
\text{AIC}_c = \text{AIC} + \frac{2(p + 2)(p + 3)}{n-p-3}
\]</span>
Also AIC<span class="math inline">\(_c\)</span> is minimised for an optimal predictive model.</p></li>
<li><p><strong>BIC</strong>: the Schwarz’s Bayesian Information Criterion is defined as
<span class="math display">\[
\text{BIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + (p+2)  \log(n)
\]</span>
Also in this case our goal is to minimize the BIC. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select the model with fewer predictors than AIC.</p></li>
</ul>
</div>
<div id="metrics-for-classification" class="section level4" number="7.2.3.2">
<h4><span class="header-section-number">7.2.3.2</span> Metrics for classification</h4>
<p>In the examples for this course, we have thus far focussed on regression models. For classification, different metrics for measuring the agreement between predicted and observed values are used. They will be introduced in a later chapter. If you’re curious already now, good overviews are provided in the following links:</p>
<ul>
<li><a href="https://bradleyboehmke.github.io/HOML/process.html#model-eval">Hands On Machine Learning in R, Bradley &amp; Boehmke</a> for brief definitions.</li>
<li><a href="https://en.wikipedia.org/wiki/Confusion_matrix">Wikipedia on Confusion Matrix</a></li>
<li><a href="https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b">Blogpost by M. Sunasra on Medium</a></li>
</ul>
</div>
<div id="residual-analysis" class="section level4" number="7.2.3.3">
<h4><span class="header-section-number">7.2.3.3</span> Residual analysis</h4>
<p>Quantifying metrics is one part of model evaluation. The other part is to get an intuitive understanding of the model-observation agreement and where and why they fail. Getting there is an integral part of exploratory data analysis. One of the first steps after obtaining results from your initial model is to investigate its residuals (the difference between predicted and observed values) and their pattern. If you can detect a clear pattern or trend in your residuals, then your model has room for improvement.</p>
<p>Our example dataset contains time series of multiple variables. Above, we have not used time as a predictor but values in the dataframe are ordered by time along rows. An obvious first step is to look at residuals and their relationship with time (or simply row number in our case).</p>
<p>A handy function to add predictions and residuals from a fitted model to the data (must contain the variables used in the model), is <a href="https://broom.tidymodels.org/reference/data.frame_tidiers.html"><code>augment</code></a> from the tidyverse <a href="https://broom.tidymodels.org/index.html"><em>broom</em></a> package. It adds informations about observations to a dataset.</p>
<p>Let’s fit a linear regression model <code>GPP_NT_VUT_REF ~ PPFD_IN</code> and look at the residuals.</p>
<div class="sourceCode" id="cb805"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb805-1"><a href="ch-07.html#cb805-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb805-2"><a href="ch-07.html#cb805-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb805-3"><a href="ch-07.html#cb805-3" aria-hidden="true" tabindex="-1"></a>linmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> PPFD_IN, <span class="at">data =</span> df)</span>
<span id="cb805-4"><a href="ch-07.html#cb805-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb805-5"><a href="ch-07.html#cb805-5" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">augment</span>(linmod1, <span class="at">data =</span> df)</span>
<span id="cb805-6"><a href="ch-07.html#cb805-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb805-7"><a href="ch-07.html#cb805-7" aria-hidden="true" tabindex="-1"></a>df1 <span class="sc">%&gt;%</span> </span>
<span id="cb805-8"><a href="ch-07.html#cb805-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">id =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb805-9"><a href="ch-07.html#cb805-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(id, .resid)) <span class="sc">+</span> </span>
<span id="cb805-10"><a href="ch-07.html#cb805-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">alpha =</span> .<span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb805-11"><a href="ch-07.html#cb805-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Row number&quot;</span>) <span class="sc">+</span></span>
<span id="cb805-12"><a href="ch-07.html#cb805-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="sc">+</span></span>
<span id="cb805-13"><a href="ch-07.html#cb805-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Univariate linear regression&quot;</span>, <span class="at">subtitle =</span> <span class="st">&quot;GPP_NT_VUT_REF ~ PPFD_IN&quot;</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-369-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Apparently, there is a clear pattern in the residuals with an apparent autocorrelation (the residual in one data point is correlated with the residual in its preceeding data point). This suggests that there is temporal information that we did not account for in our model. This could either be from additional predictors, not included here, that have a temporal pattern (note that we only included one predictor in this model), or from the inherent dynamics of the system itself, which is not captured by the predictors (e.g., memory effects).</p>
<p>We can also plot residuals versus additional, potentially influential but not included predictors to guide the revision of the model. However, it is often not possible to determine effects by eye, especially if interactions between variables are important. We learn about methods to sequentially add predictors to a linear regression model in the ‘Application’ session (see Chapter <a href="ch-08.html#ch-08">8</a>).</p>
</div>
</div>
<div id="model-interpretation" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Model interpretation</h3>
<p>Until now, we have seen how to build and train models, tune the hyperparameters and evaluate models. ML models work so well because they can effectively make use of large amounts of data and are flexible enough to model non-linear relationships and interactions, and predict rare and faint phenomena. This is their great advantage over classical statistical methods. However, this flexibility is underpinned by a high model complexity and a large number of parameters. This complexity also contrasts with traditional statistical methods, e.g., linear regression where fitted coefficients (<span class="math inline">\(\beta_i\)</span> in Chapter <a href="ch-06.html#ch-06">6</a>) can directly be interpreted and yield information about the sensitivity of the target variable to the different predictors and even about the statistical significance of their effect (we haven’t looked at that part). In contrast, the complexity of ML models renders their interpretation difficult. The ML algorithms’ predictive power comes at the cost of reduced <em>model interpretability</em>. ML models often appear to be “black boxes” which may limit their usefulness in typical applications in research, where we’re often not only interested in <em>predicting</em>, but also in <em>understanding</em> how the model arrived at its predictions. We are often interested in identifying patterns in the data that would otherwise not be visible, but that the algorithm apparently identified and learned.</p>
<p>In order to interpret a ML model and understand its inner workings, we have to “ask specific questions” that can be translated into an evaluation of the trained model that then yields the answer we want. There are basically two types of questions we can ask:</p>
<ul>
<li>How “important” is each predictor variable in our model? This is answered by <em>variable importance analysis</em>.</li>
<li>What’s the functional relationship between the target variable and each predictor? This is answered by <em>partial residual analysis</em>.</li>
</ul>
<div id="variable-importance" class="section level4" number="7.2.4.1">
<h4><span class="header-section-number">7.2.4.1</span> Variable importance</h4>
<p><strong>Model specific approach (for linear regression)</strong></p>
<p>Some of the approaches for evaluating feature importances are <em>model-specific</em>. For instance, the absolute value of <em>t</em>-statistic, in case of linear models, as a measure of feature importance. Such model-specific interpretation tools are limited to their respective model classes. There can be some advantages to using these model-specific approaches as they are more closely linked to the model and its performance, and can thus directly use parameters (coefficients) of the fitted model.</p>
<p>For linear regression models, we can quantify the significance of its coefficients <span class="math inline">\(\beta\)</span>, by testing a null-hypothesis that the coefficient is, in reality, zero. As an example, let’s assume that our data are generated from the following linear model
<span class="math display">\[
y = \beta_0 + \beta_1 x + \epsilon
\]</span>
where we assume that our errors <span class="math inline">\(\epsilon\)</span> are independent and come from a normal distribution. Given only one realization of our data, our aim is to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Let’s generate random data for a univariate linear relationship where the true coefficients are <span class="math inline">\(\beta_1 = 3\)</span> and <span class="math inline">\(\beta_0 =2\)</span>. Note that in practice we do not know the real coefficients and we don’t know what the true predictors are. Let’s consider an additional predictor that is available in our data but does not contribute to the actual data generation process (<code>x2</code>). We include it in the model and want to know whether the respective coefficient it yields is significant. How can we determine information about its <em>importance</em>? As we’ve seen before, the <code>summary()</code> function generates a human-readable output. What information does it provide about variable importance?</p>
<div class="sourceCode" id="cb806"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb806-1"><a href="ch-07.html#cb806-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generate random data</span></span>
<span id="cb806-2"><a href="ch-07.html#cb806-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb806-3"><a href="ch-07.html#cb806-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb806-4"><a href="ch-07.html#cb806-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb806-5"><a href="ch-07.html#cb806-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb806-6"><a href="ch-07.html#cb806-6" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># sample size</span></span>
<span id="cb806-7"><a href="ch-07.html#cb806-7" aria-hidden="true" tabindex="-1"></a>b_1 <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb806-8"><a href="ch-07.html#cb806-8" aria-hidden="true" tabindex="-1"></a>b_0 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb806-9"><a href="ch-07.html#cb806-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb806-10"><a href="ch-07.html#cb806-10" aria-hidden="true" tabindex="-1"></a>df_demo <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="fu">rnorm</span>(n), <span class="at">x2 =</span> <span class="fu">rnorm</span>(n)) <span class="sc">%&gt;%</span></span>
<span id="cb806-11"><a href="ch-07.html#cb806-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> b_1 <span class="sc">*</span> x1 <span class="sc">+</span> b_0 <span class="sc">+</span> <span class="fu">rnorm</span>(n ,<span class="at">mean =</span> <span class="dv">0</span> , <span class="at">sd =</span> <span class="dv">1</span>))  <span class="co"># no x2 here</span></span>
<span id="cb806-12"><a href="ch-07.html#cb806-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb806-13"><a href="ch-07.html#cb806-13" aria-hidden="true" tabindex="-1"></a><span class="do">## fit model</span></span>
<span id="cb806-14"><a href="ch-07.html#cb806-14" aria-hidden="true" tabindex="-1"></a>linmod_demo <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> df_demo)  <span class="co"># x2 here because it&#39;s in the data - is it significant?</span></span>
<span id="cb806-15"><a href="ch-07.html#cb806-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb806-16"><a href="ch-07.html#cb806-16" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_demo)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = df_demo)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8730 -0.6607 -0.1245  0.6214  2.0798 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.13507    0.09614  22.208   &lt;2e-16 ***
## x1           2.86683    0.10487  27.337   &lt;2e-16 ***
## x2           0.02381    0.09899   0.241     0.81    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9513 on 97 degrees of freedom
## Multiple R-squared:  0.8853, Adjusted R-squared:  0.8829 
## F-statistic: 374.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>You see that the linear regression fit yields estimates of the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that are relatively close to the real ones (2.13507 for <span class="math inline">\(\beta_0\)</span> instead of 2, 2.86683 for <span class="math inline">\(\beta_1\)</span> instead of 3). We also see that the estimate (<code>Estimate</code>) for (the fake) coefficient <code>x2</code> is close to zero. The standard error of the coefficient estimate <code>Std. Error</code> is an estimate of the standard deviation of coefficient estimates we would get if the above random data generation process was repeated many times. It decreases with the sample size (<code>n</code> in the code above). The <em>t</em>-statistic (<code>t value</code>) is <span class="math inline">\(\frac{\mbox{Estimate}}{\mbox{Std.Error}}\)</span>. Assuming a t-distribution of coefficient estimates, the <em>p</em>-value (<code>Pr(&gt;|t|)</code>) quantifies the probability of our coefficient estimate (considering its <em>t</em>-statistic) if the true coefficient was zero - our null hypothesis. In our example, the <em>p</em>-value for the <code>x2</code> estimate is 0.81. This is not significant at any significance level (indicated by the <code>*</code> to the right of the reported p-value, key given by <code>Signif. codes</code>). In contrast, the estimates for coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are highly significant. Their very low <em>p</em>-values indicate that it is highly unlikely that their true value was zero.</p>
<p>This is illustrated below.</p>
<div class="sourceCode" id="cb808"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb808-1"><a href="ch-07.html#cb808-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb808-2"><a href="ch-07.html#cb808-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb808-3"><a href="ch-07.html#cb808-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb808-4"><a href="ch-07.html#cb808-4" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">7</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb808-5"><a href="ch-07.html#cb808-5" aria-hidden="true" tabindex="-1"></a>t_dist <span class="ot">&lt;-</span> <span class="fu">dt</span>(grid, <span class="at">df =</span> n<span class="dv">-3</span>)           <span class="co"># 3 is the number of estimated coefficients</span></span>
<span id="cb808-6"><a href="ch-07.html#cb808-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb808-7"><a href="ch-07.html#cb808-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()<span class="sc">+</span></span>
<span id="cb808-8"><a href="ch-07.html#cb808-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> grid, <span class="at">y =</span> t_dist,<span class="at">color =</span> <span class="st">&#39;Null Distribution&#39;</span>), <span class="at">lwd=</span><span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb808-9"><a href="ch-07.html#cb808-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="fl">22.208</span>, <span class="at">linetype =</span> <span class="st">&#39; t value </span><span class="sc">\n</span><span class="st"> for intercept&#39;</span>), <span class="at">color=</span><span class="st">&#39;red&#39;</span>) <span class="sc">+</span></span>
<span id="cb808-10"><a href="ch-07.html#cb808-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">name =</span> <span class="st">&#39;&#39;</span>,<span class="at">values =</span> <span class="st">&#39;black&#39;</span>) <span class="sc">+</span></span>
<span id="cb808-11"><a href="ch-07.html#cb808-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(<span class="at">name=</span><span class="st">&#39;&#39;</span>, <span class="at">values =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb808-12"><a href="ch-07.html#cb808-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">25</span>)) <span class="sc">+</span></span>
<span id="cb808-13"><a href="ch-07.html#cb808-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&#39;t value&#39;</span>, <span class="at">y =</span> <span class="st">&#39;Density&#39;</span>, <span class="at">title =</span> <span class="st">&#39;Students t-distribution&#39;</span>) <span class="sc">+</span></span>
<span id="cb808-14"><a href="ch-07.html#cb808-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&#39;top&#39;</span>) <span class="sc">+</span></span>
<span id="cb808-15"><a href="ch-07.html#cb808-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">1</span>),</span>
<span id="cb808-16"><a href="ch-07.html#cb808-16" aria-hidden="true" tabindex="-1"></a>         <span class="at">linetype =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">2</span>))</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-371-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb809"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb809-1"><a href="ch-07.html#cb809-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()<span class="sc">+</span></span>
<span id="cb809-2"><a href="ch-07.html#cb809-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> grid, <span class="at">y =</span> t_dist, <span class="at">color =</span> <span class="st">&#39;Null Distribution&#39;</span>), <span class="at">lwd=</span><span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb809-3"><a href="ch-07.html#cb809-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="fl">0.241</span>, <span class="at">linetype =</span> <span class="st">&#39; t value </span><span class="sc">\n</span><span class="st"> for x2&#39;</span>), <span class="at">color=</span><span class="st">&#39;red&#39;</span>) <span class="sc">+</span></span>
<span id="cb809-4"><a href="ch-07.html#cb809-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">name =</span> <span class="st">&#39;&#39;</span>,<span class="at">values =</span> <span class="st">&#39;black&#39;</span>) <span class="sc">+</span></span>
<span id="cb809-5"><a href="ch-07.html#cb809-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(<span class="at">name=</span><span class="st">&#39;&#39;</span>, <span class="at">values =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb809-6"><a href="ch-07.html#cb809-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">25</span>)) <span class="sc">+</span></span>
<span id="cb809-7"><a href="ch-07.html#cb809-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&#39;t value&#39;</span>, <span class="at">y =</span> <span class="st">&#39;Density&#39;</span>, <span class="at">title =</span> <span class="st">&#39;Students t-distribution&#39;</span>) <span class="sc">+</span></span>
<span id="cb809-8"><a href="ch-07.html#cb809-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&#39;top&#39;</span>) <span class="sc">+</span></span>
<span id="cb809-9"><a href="ch-07.html#cb809-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">1</span>),</span>
<span id="cb809-10"><a href="ch-07.html#cb809-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">linetype =</span> <span class="fu">guide_legend</span>(<span class="at">order =</span> <span class="dv">2</span>))</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-371-2.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Variable importance in a fitted model can be assessed in R using the <strong>vip</strong> package. If we provide a linear regression model as argument, it automatically uses the absolute value of the <em>t</em>-statistic for quantifying the variable importance.</p>
<div class="sourceCode" id="cb810"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb810-1"><a href="ch-07.html#cb810-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span>
<span id="cb810-2"><a href="ch-07.html#cb810-2" aria-hidden="true" tabindex="-1"></a>linmod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> df)</span>
<span id="cb810-3"><a href="ch-07.html#cb810-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(linmod3)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-372-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p><strong>Checkpoint</strong></p>
<p>Verify that the bars plotted in the vip plot correspond to the t-statistic of the linear regression model.</p>
<p><strong>Solution</strong></p>
<div class="sourceCode" id="cb811"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb811-1"><a href="ch-07.html#cb811-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod3)  <span class="co"># compare with values under `t-value`</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ ., data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.7157 -1.6341 -0.0617  1.4759 12.9408 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -36.752729   6.271377  -5.860 5.03e-09 ***
## TA_F         -0.054296   0.015636  -3.473 0.000522 ***
## SW_IN_F       0.024095   0.005965   4.039 5.47e-05 ***
## LW_IN_F       0.044075   0.001932  22.814  &lt; 2e-16 ***
## VPD_F        -0.326812   0.026110 -12.517  &lt; 2e-16 ***
## PA_F          0.288201   0.064701   4.454 8.67e-06 ***
## P_F          -0.022461   0.010430  -2.153 0.031350 *  
## WS_F         -0.547352   0.083926  -6.522 7.91e-11 ***
## CO2_F_MDS    -0.004076   0.001407  -2.897 0.003787 ** 
## PPFD_IN       0.007245   0.002870   2.524 0.011632 *  
## USTAR         1.371836   0.530123   2.588 0.009699 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.486 on 3622 degrees of freedom
## Multiple R-squared:  0.6593, Adjusted R-squared:  0.6583 
## F-statistic: 700.8 on 10 and 3622 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The results of the feature importance plot using <code>vip()</code>, agree with the statistics obtaind from the <code>summary()</code> function. The variables <code>LW_IN_F</code> and <code>VPD_F</code> have largest <span class="math inline">\(|t|\)</span> statistic values (thus, the values on the tail ends of the t-distribution have low measure), which results in a small p-value, thus indicating that these are the most important features. On the other end, the variable <code>P_F</code> has the smallest <span class="math inline">\(|t|\)</span> statistc, which results in a large p-value, thus indicating that this variable is not as important.</p>
<p>To sum this up, variable importance can be measured for linear regression models using the <em>t</em>-statistic.</p>
<p>While <em>t</em>-statistic values provide information for feature importance analysis that is straight-forward to quantify and interpret, corresponding information has to be extracted by other means for other ML algorithms (e.g., accuracy degradation in random forests). This makes it difficult to compare their results across different classes of ML models.</p>
<p><strong>Model-Agnostic approaches</strong></p>
<p>Model-Agnostic approaches offer a solution to the intercomparability problem for feature importance analysis. These treat the ML algorithm as a “black-box,” and the separation of the specific model types from their interpretability measure enables us to compare the feature importance across different models and ML algorithms. <em>Permutation-based feature importance</em> is such an algorithm. Its idea is that if we <em>permute</em> (re-shuffle) the values of a feature in the training data, we effectively destroy the relationship between that particular feature and the target variable. Thus, if the feature is important, the model performance should degrade substantially. In contrast, for unimportant features, permuting their values should not degrade the model performance much.</p>
<p>Algorithmically speaking, this works as follows:</p>
<ul>
<li>We first compute the loss for the original model to establish a reference loss.</li>
<li>For predictor <span class="math inline">\(i\)</span> in <span class="math inline">\(\{1,2, …, p\}\)</span>
<ul>
<li>Randomly permute values of predictor <span class="math inline">\(i\)</span> in the dataset.</li>
<li>Train the model.</li>
<li>Compute the loss.</li>
<li>Compute the feature importance, e.g., as the ratio or difference of the loss with the permuted data and the reference loss.</li>
<li>Store this as the feature importance for feature <span class="math inline">\(i\)</span></li>
</ul></li>
<li>Sort the variable by their feature importance</li>
</ul>
<p>This can be easily implemented using the <code>vip</code> package along with a few extra parameters</p>
<p>Arguments to the function vip(), for permutation based feature importance
- <code>linmod3</code>: A fitted model object
- <code>train</code> : the training data to be used to compute the feature importances
- <code>method</code> : <code>"permute"</code> for permutation based method
- <code>target</code> : specifying the target variable for the train datset
- <code>metric</code> : <code>"RMSE"</code> root mean squared error for the regression based task
- <code>n_sim</code> : number of times the simulation is to be repeated, the result is averaged over all the simulations ; choose greater values to increase the stability of estimates
- <code>sample_frac</code> : specifies the proportion of the training data to be used in each simulation. Default value is NULL i.e. all of training data is used<br />
- <code>pred_wrapper</code> : Prediction function that requires two arguments, <code>object</code> and <code>newdata</code>. The output of this function should be determined by the metric being used
- A numeric vector of predicted outcomes for a regression task
- A vector of predicted class labels, or a matrix of predicted class probabilitites for a classification task
- <code>type</code> : Character string specifying how to compare the baseline and permuted performance metrics; default value is <code>'difference'</code>, and the other option is <code>'ratio'</code></p>
<p><em>Note: In case of large datasets, <code>n_sim</code> and <code>sample_frac</code> can be used to reduce the execution time of the algorithm, as permutation based approaches can become slow as the number of predictors grows.</em></p>
<div class="sourceCode" id="cb813"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb813-1"><a href="ch-07.html#cb813-1" aria-hidden="true" tabindex="-1"></a>pfun <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata) <span class="fu">predict</span>(object, <span class="at">newdata =</span> newdata) <span class="co"># inline function defined to output the predictions for newdata given a particular model object </span></span>
<span id="cb813-2"><a href="ch-07.html#cb813-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb813-3"><a href="ch-07.html#cb813-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb813-4"><a href="ch-07.html#cb813-4" aria-hidden="true" tabindex="-1"></a><span class="do">## variable importance plot using the permute method, with type == &#39;ratio&#39; </span></span>
<span id="cb813-5"><a href="ch-07.html#cb813-5" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb813-6"><a href="ch-07.html#cb813-6" aria-hidden="true" tabindex="-1"></a>  linmod3,</span>
<span id="cb813-7"><a href="ch-07.html#cb813-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> df,</span>
<span id="cb813-8"><a href="ch-07.html#cb813-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb813-9"><a href="ch-07.html#cb813-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb813-10"><a href="ch-07.html#cb813-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb813-11"><a href="ch-07.html#cb813-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb813-12"><a href="ch-07.html#cb813-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb813-13"><a href="ch-07.html#cb813-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun,</span>
<span id="cb813-14"><a href="ch-07.html#cb813-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;ratio&#39;</span></span>
<span id="cb813-15"><a href="ch-07.html#cb813-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-374-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb814"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb814-1"><a href="ch-07.html#cb814-1" aria-hidden="true" tabindex="-1"></a><span class="do">## variable importance plot using the permute method, with type == &#39;difference&#39; </span></span>
<span id="cb814-2"><a href="ch-07.html#cb814-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb814-3"><a href="ch-07.html#cb814-3" aria-hidden="true" tabindex="-1"></a>  linmod3,</span>
<span id="cb814-4"><a href="ch-07.html#cb814-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> df,</span>
<span id="cb814-5"><a href="ch-07.html#cb814-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb814-6"><a href="ch-07.html#cb814-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb814-7"><a href="ch-07.html#cb814-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb814-8"><a href="ch-07.html#cb814-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb814-9"><a href="ch-07.html#cb814-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb814-10"><a href="ch-07.html#cb814-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun, </span>
<span id="cb814-11"><a href="ch-07.html#cb814-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;difference&#39;</span> </span>
<span id="cb814-12"><a href="ch-07.html#cb814-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-374-2.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Now we can compare the importance plots obtained from the model-specific t-statistic approach and the model agnostic permuation based method. The feature importances obtained from both approaches differ a little, because of the underlying differences in the feature importance evaluation. Despite of the differences, both methods robustly return the key important features, particularly their ranks.</p>
<p><strong>Checkpoint</strong></p>
<p>Compute the variable importance for the best fit knn model, which we obtained earlier in the tutorial, using the <code>vip()</code> function in default mode. Repeat the same with permuatation based method, with type as ‘difference’ and ‘ratio.’ Do the two return the same top-4 features ? Verify.</p>
<p><strong>Solution</strong></p>
<div class="sourceCode" id="cb815"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb815-1"><a href="ch-07.html#cb815-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(knn_fit)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-375-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb816"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb816-1"><a href="ch-07.html#cb816-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb816-2"><a href="ch-07.html#cb816-2" aria-hidden="true" tabindex="-1"></a>  knn_fit,</span>
<span id="cb816-3"><a href="ch-07.html#cb816-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> df,</span>
<span id="cb816-4"><a href="ch-07.html#cb816-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb816-5"><a href="ch-07.html#cb816-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb816-6"><a href="ch-07.html#cb816-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb816-7"><a href="ch-07.html#cb816-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb816-8"><a href="ch-07.html#cb816-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb816-9"><a href="ch-07.html#cb816-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun, </span>
<span id="cb816-10"><a href="ch-07.html#cb816-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;difference&#39;</span> </span>
<span id="cb816-11"><a href="ch-07.html#cb816-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-375-2.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb817"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb817-1"><a href="ch-07.html#cb817-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(</span>
<span id="cb817-2"><a href="ch-07.html#cb817-2" aria-hidden="true" tabindex="-1"></a>  knn_fit,</span>
<span id="cb817-3"><a href="ch-07.html#cb817-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> df,</span>
<span id="cb817-4"><a href="ch-07.html#cb817-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;permute&quot;</span>,</span>
<span id="cb817-5"><a href="ch-07.html#cb817-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">&quot;GPP_NT_VUT_REF&quot;</span>,</span>
<span id="cb817-6"><a href="ch-07.html#cb817-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb817-7"><a href="ch-07.html#cb817-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">nsim =</span> <span class="dv">5</span>,</span>
<span id="cb817-8"><a href="ch-07.html#cb817-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_frac =</span> <span class="fl">0.8</span>,</span>
<span id="cb817-9"><a href="ch-07.html#cb817-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> pfun, </span>
<span id="cb817-10"><a href="ch-07.html#cb817-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&#39;ratio&#39;</span> </span>
<span id="cb817-11"><a href="ch-07.html#cb817-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-375-3.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="partial-dependence-plots" class="section level4" number="7.2.4.2">
<h4><span class="header-section-number">7.2.4.2</span> Partial dependence plots</h4>
<p>While variable importance analysis is helpful for understanding which predictors are helping us predict, <em>partial dependence plots</em> (PDPs) yield information on what the relationship between one individual predictor and the target variable looks like. It quantifies how the response of the target variable changes when we vary one particular predictor and hold all the remaining predictors at a constant value. If this is still a little muddy, the pseudo-code of this algorithm should make things clear (from <a href="https://bradleyboehmke.github.io/HOML/iml.html#partial-dependence">Bradley &amp; Boehmke</a>).</p>
<div class="sourceCode" id="cb818"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb818-1"><a href="ch-07.html#cb818-1" aria-hidden="true" tabindex="-1"></a>For a selected <span class="fu">predictor</span> (x)</span>
<span id="cb818-2"><a href="ch-07.html#cb818-2" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> Construct a grid of j evenly spaced values across the range of x<span class="sc">:</span> {x1, x2, ..., xj}</span>
<span id="cb818-3"><a href="ch-07.html#cb818-3" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> For i <span class="cf">in</span> {<span class="dv">1</span>,...,j} do</span>
<span id="cb818-4"><a href="ch-07.html#cb818-4" aria-hidden="true" tabindex="-1"></a>     <span class="sc">|</span> Copy the training data and replace the original values of x with the constant xi</span>
<span id="cb818-5"><a href="ch-07.html#cb818-5" aria-hidden="true" tabindex="-1"></a>     <span class="sc">|</span> Apply the fitted ML model to obtain vector of predictions <span class="cf">for</span> each data point.</span>
<span id="cb818-6"><a href="ch-07.html#cb818-6" aria-hidden="true" tabindex="-1"></a>     <span class="sc">|</span> Average predictions across all data points.</span>
<span id="cb818-7"><a href="ch-07.html#cb818-7" aria-hidden="true" tabindex="-1"></a>   End</span>
<span id="cb818-8"><a href="ch-07.html#cb818-8" aria-hidden="true" tabindex="-1"></a><span class="fl">3.</span> Plot the averaged predictions against x1, x2, ..., xj</span></code></pre></div>
<p>As the steps explain, PDPs plot the change in the average predicted value (<span class="math inline">\(\hat{y}\)</span>) as a function of a specified feature. As you will see in later chapters, PDPs become more useful when non-linear relationships are present. However, PDPs of linear models help illustrate how a fixed change in <span class="math inline">\(x_i\)</span> relates to a fixed linear change in <span class="math inline">\(\hat{y_i}\)</span> while taking into account the average effect of all the other features in the model. For linear regression models, the slope of the PDP is equal to the corresponding coefficient of the model.</p>
Considering Figure <a href="ch-07.html#fig:pdp">7.6</a>, to compute the PDP of target variable with respect to Gr_Liv_Area, we modify the values of this variable for all the rows in the data set, for each [j1, j2, …, j20] in the grid. Feed this modified dataset to the pre-trained model and compute the mean of our target variable (mean taken over all rows). Finally at the end of the algorithm we have 20 mean responses of the target variable for 20 different values of Gr_Liv_Area. Finally we plot the mean response of the target variable with respect to the changing variable Gr_Liv_Area to get the parital dependence plot.
<div class="figure" style="text-align: center"><span id="fig:pdp"></span>
<img src="figures/pdp-illustration.png" alt="Visualiation of a partial dependence plot (from [Bradley &amp; Boehmke](https://bradleyboehmke.github.io/HOML/images/pdp-illustration.png))." width="50%" />
<p class="caption">
Figure 7.6: Visualiation of a partial dependence plot (from <a href="https://bradleyboehmke.github.io/HOML/images/pdp-illustration.png">Bradley &amp; Boehmke</a>).
</p>
</div>
<p>Lucky for us, this code has already been implemented in the <code>pdp</code> package in R. We use the <code>partial()</code> function to plot the partial dependence plot for a single predictor variable, <code>PPFD_IN</code> or <code>VPD_F</code>, or for a subset of both these variables. Since our models are linear, we expect the PDP curves to be linear as well. The first curve shows a positive linear relationship between <code>PPFD_IN</code> and the target y_hat (<code>GPP_NT_VUT_REF</code>), i.e. as the <code>PPFD_IN</code> increases, the target increases. Whereas the second curve shows us that there is a negative linear dependence between <code>VPD_F</code> and <code>GPP_NT_VUT_REF</code>. The final plot shows a 2D partial dependence plot with <code>PPFD_IN</code> and <code>VPD_F</code>, and this follows a linear relationship too. As expected, we get the highest value for our target variable in the lower right corner and the smallest value in the upper left corner.</p>
<div class="sourceCode" id="cb819"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb819-1"><a href="ch-07.html#cb819-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pdp)</span>
<span id="cb819-2"><a href="ch-07.html#cb819-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb819-3"><a href="ch-07.html#cb819-3" aria-hidden="true" tabindex="-1"></a><span class="fu">partial</span>(linmod3, <span class="st">&quot;PPFD_IN&quot;</span>, <span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-377-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb820"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb820-1"><a href="ch-07.html#cb820-1" aria-hidden="true" tabindex="-1"></a><span class="fu">partial</span>(linmod3, <span class="st">&quot;VPD_F&quot;</span>, <span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-377-2.png" width="50%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb821"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb821-1"><a href="ch-07.html#cb821-1" aria-hidden="true" tabindex="-1"></a><span class="fu">partial</span>(linmod3, <span class="fu">c</span>(<span class="st">&quot;PPFD_IN&quot;</span>, <span class="st">&quot;VPD_F&quot;</span>), <span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-377-3.png" width="50%" style="display: block; margin: auto;" /></p>
<p>The <code>pdp</code> package works with various model classes. For classes that are not available, a prediction wrapper function has to be specified as described for example <a href="https://bradleyboehmke.github.io/HOML/iml.html#implementation-6">here</a>. If you want to learn more about PDP, have a look the online book by <a href="https://christophm.github.io/interpretable-ml-book/">Christoph Molnar</a>.</p>
<p>Note: The computational demand of partial dependence analysis rapidly increases as the number of search grid points (<code>j</code> in the algorithm description above) rises.</p>
<p><em>For the interested: If you do not want to average the y_hat, predictions over all the rows for each value of <span class="math inline">\(X_i\)</span>, look into Individual conditional expectation (ICE) curves, which rather than averaging the predicted values across all observations, observe and plot the individual observation-level predictions.</em></p>
</div>
</div>
</div>
<div id="exercise-6" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Exercise</h2>
<p><strong>1. Adjusted-<span class="math inline">\(R^2\)</span></strong></p>
<ol style="list-style-type: lower-alpha">
<li>Implement the formula for the adjusted-<span class="math inline">\(R^2\)</span> using simple “low-level” functions and <span class="math inline">\(R^2\)</span> defined as</li>
</ol>
<ul>
<li>the coefficient of determination and then</li>
<li>as the square of the Pearson’s correlation coefficient.</li>
</ul>
<ol start="2" style="list-style-type: lower-alpha">
<li>Compare the values of the two with what is returned by <code>summary(lm(..))$adj.r.squared</code>. Which one of the two is used by <code>summary(lm(..))</code>? For your investigations, generate correlated random data and apply functions on it.</li>
</ol>
<hr />
<p><strong>2. Cross validation</strong></p>
<p>Assess the generalisability of three alternatives of a linear regression model for <code>GPP_NT_VUT_REF</code>:</p>
<ol style="list-style-type: lower-alpha">
<li>with one predictor (<code>PPFD_IN</code>),</li>
<li>three predictors (<code>PPFD_IN</code>, <code>VPD_F</code>, and <code>TA_F,</code>),</li>
<li>all available predictors.</li>
</ol>
<p>Which model works best in terms of its generalisability, assessed by 5-fold cross-validation and using the RMSE as the loss function?</p>
<p>Use the caret function <code>train()</code> with RMSE as the loss function and use the data from file <code>ddf_ch_lae.RData</code> (avoid <code>TIMESTAMP</code> and <code>NEE_VUT_REF_QC</code> as predictors!).</p>
<p><strong>Hint</strong></p>
<p>To get the generalisation performance assessed during cross-validation (and not on the data held out from an initial split - which is not done here), you can use <code>summary(resamples(list(model1 = output_train1, ...)))</code>, where <code>output_train1</code> is the ouput of a <code>train()</code> function call with resampling. We see that by including all predictors, we get the best model performance in terms of it cross-validation results, i.e., the lowest root mean square error (RMSE) and the highest <span class="math inline">\(R^2\)</span>, averaged across folds.</p>
<hr />
<p><strong>3. Exploratory modelling</strong></p>
<p>Using the same data set as above (daily data for CH-Lae), find the model with the best performance, measured by evaluation against testing data held out from an initial split (30-70%). Use your own creativity to pre-process features and select variables, and chose between either a KNN or a multivariate linear regression, or any other ML model if you like (see models available with caret <a href="https://topepo.github.io/caret/available-models.html">here</a>). Use all methods you’ve learned so far and impress your peers. Who gets the best generalisable model? Document and justify all steps you take. Use data from file <code>ddf_ch_lae.RData</code> (avoid <code>TIMESTAMP</code> and <code>NEE_VUT_REF_QC</code> as predictors).</p>
<hr />
<p><strong>4. Resampling - Bonus problem</strong></p>
<ol style="list-style-type: lower-alpha">
<li>Define a resampling function of the form <code>my_resample_folds_nested(df, k)</code> generating <span class="math inline">\(k\)</span> folds and taking a data frame <code>df</code> as arguments. This function should return a nested data frame with <span class="math inline">\(k\)</span> rows and two columns <code>train</code> and <code>test</code> that contain nested dataframes for training and testing data with each fold. Use data from file <code>ddf_ch_lae.RData</code> (avoid <code>TIMESTAMP</code> and <code>NEE_VUT_REF_QC</code> as predictors).</li>
<li>Print the dimensions of the whole data frame and run head() of fold two of the column ‘train’ of the nested data frame (this is just to make the peer review easier).</li>
</ol>
<p><em>“Relief option:”</em> You may create two separate flat data frames, one with testing, and one with training data, where one column specifies which fold each row belongs to.</p>
<p><strong>Steps:</strong></p>
<ol style="list-style-type: decimal">
<li>Use the caret function <code>createFolds</code> to split a vector of row indices into a list of length corresponding to the number of folds, where each list element cotains the indices of the training resamples for the respective fold.</li>
<li>Use the dplyr function <code>slice()</code> to subset a dataframe by row numbers.</li>
<li>Use <code>group_by()</code> in combination with <code>nest()</code> for nesting data frames for each of the <span class="math inline">\(k\)</span> folds and end up with a dataframe that has <span class="math inline">\(k\)</span> rows.</li>
<li>Fit a linear model of the form <code>GPP_NT_VUT_REF ~ PPFD_IN</code>
<ul>
<li>directly on all the data in <code>df</code> and</li>
<li>on the resampled <strong>training</strong> folds.</li>
</ul></li>
<li>Evaluate the models by calculating metrics (squared Pearson’s correlation coefficient, and RMSE) of predictions made
<ul>
<li>for the full dataframe <code>df</code></li>
<li>for each respective <strong>testing</strong> fold separately. For this, report the mean across metrics calculated on each testing fold.</li>
</ul></li>
</ol>
<p>If this sounds confusing: Just implement a k-fold cross-validation as described also in the tutorial and report the cross-validation metrics.</p>
<p><strong>Hints:</strong>
<em>Feel free to come up with a solution that doesn’t make use of these hints.</em></p>
<ul>
<li>You may use <code>mutate( newcol = purrr::map( nested_col, ~function_name(.)) )</code> to apply a function <code>function_name()</code> on each of the nested data frames in column <code>nested_col</code>. The <code>.</code> refers to the position at which the argument <code>nested_col</code> goes.</li>
<li>You may use the broom function <code>augment()</code> to add a column <code>.fitted</code> to a data frame, given a model object that was generated using predictors that are available as columns in the respective data frame.</li>
<li>You may use <code>mutate( newcol = purrr::map2( arg1, arg2, ~function_name(.x, .y)) )</code> to apply a function <code>function_name()</code> that takes two arguments from other columns in the same data frame. The <code>.x</code> and <code>.y</code> refer to the position at which the two arguments go.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-06.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-08.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
