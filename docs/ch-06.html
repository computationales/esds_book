<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Supervised Machine Learning I | Environmental Systems Data Science</title>
  <meta name="description" content="Tutorial and exercises for Environmental System Data Science, ETH Zürich." />
  <meta name="generator" content="bookdown 0.22.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Supervised Machine Learning I | Environmental Systems Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Tutorial and exercises for Environmental System Data Science, ETH Zürich." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Supervised Machine Learning I | Environmental Systems Data Science" />
  
  <meta name="twitter:description" content="Tutorial and exercises for Environmental System Data Science, ETH Zürich." />
  

<meta name="author" content="Loïc Pellissier, Joshua Payne, Benjamin Stocker" />


<meta name="date" content="2021-10-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-05.html"/>
<link rel="next" href="model-formulation-1.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> pdf_document:</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-description"><i class="fa fa-check"></i>Course Description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objectives"><i class="fa fa-check"></i>Course Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#content"><i class="fa fa-check"></i>Content</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#useful-prerequisites"><i class="fa fa-check"></i>Useful Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-01.html"><a href="ch-01.html"><i class="fa fa-check"></i><b>2</b> Primers</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-01.html"><a href="ch-01.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ch-01.html"><a href="ch-01.html#key-points-from-the-lecture"><i class="fa fa-check"></i><b>2.1.1</b> Key Points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch-01.html"><a href="ch-01.html#tutorial"><i class="fa fa-check"></i><b>2.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch-01.html"><a href="ch-01.html#working-with-rstudio-on-renku"><i class="fa fa-check"></i><b>2.2.1</b> Working with RStudio on Renku</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-01.html"><a href="ch-01.html#git"><i class="fa fa-check"></i><b>2.2.2</b> Git</a></li>
<li class="chapter" data-level="2.2.3" data-path="ch-01.html"><a href="ch-01.html#libraries"><i class="fa fa-check"></i><b>2.2.3</b> Libraries</a></li>
<li class="chapter" data-level="2.2.4" data-path="ch-01.html"><a href="ch-01.html#r-scripts"><i class="fa fa-check"></i><b>2.2.4</b> R scripts</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch-01.html"><a href="ch-01.html#rmarkdown"><i class="fa fa-check"></i><b>2.2.5</b> RMarkdown</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch-01.html"><a href="ch-01.html#functions"><i class="fa fa-check"></i><b>2.2.6</b> Functions</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch-01.html"><a href="ch-01.html#tidy-data"><i class="fa fa-check"></i><b>2.2.7</b> Tidy data</a></li>
<li class="chapter" data-level="2.2.8" data-path="ch-01.html"><a href="ch-01.html#r-projects"><i class="fa fa-check"></i><b>2.2.8</b> R projects</a></li>
<li class="chapter" data-level="2.2.9" data-path="ch-01.html"><a href="ch-01.html#working-with-data-frames"><i class="fa fa-check"></i><b>2.2.9</b> Working with data frames</a></li>
<li class="chapter" data-level="2.2.10" data-path="ch-01.html"><a href="ch-01.html#r-objects"><i class="fa fa-check"></i><b>2.2.10</b> R objects</a></li>
<li class="chapter" data-level="2.2.11" data-path="ch-01.html"><a href="ch-01.html#data-visualisation"><i class="fa fa-check"></i><b>2.2.11</b> Data visualisation</a></li>
<li class="chapter" data-level="2.2.12" data-path="ch-01.html"><a href="ch-01.html#conditionals"><i class="fa fa-check"></i><b>2.2.12</b> Conditionals</a></li>
<li class="chapter" data-level="2.2.13" data-path="ch-01.html"><a href="ch-01.html#loops"><i class="fa fa-check"></i><b>2.2.13</b> Loops</a></li>
<li class="chapter" data-level="2.2.14" data-path="ch-01.html"><a href="ch-01.html#where-to-find-help"><i class="fa fa-check"></i><b>2.2.14</b> Where to find help</a></li>
<li class="chapter" data-level="2.2.15" data-path="ch-01.html"><a href="ch-01.html#key-points-from-the-tutorial"><i class="fa fa-check"></i><b>2.2.15</b> Key points from the tutorial</a></li>
<li class="chapter" data-level="2.2.16" data-path="ch-01.html"><a href="ch-01.html#further-reading"><i class="fa fa-check"></i><b>2.2.16</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-01.html"><a href="ch-01.html#exercise"><i class="fa fa-check"></i><b>2.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-02.html"><a href="ch-02.html"><i class="fa fa-check"></i><b>3</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-02.html"><a href="ch-02.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch-02.html"><a href="ch-02.html#learning-objectives"><i class="fa fa-check"></i><b>3.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.1.2" data-path="ch-02.html"><a href="ch-02.html#key-points-from-the-lecture-1"><i class="fa fa-check"></i><b>3.1.2</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-02.html"><a href="ch-02.html#tutorial-1"><i class="fa fa-check"></i><b>3.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ch-02.html"><a href="ch-02.html#libraries-1"><i class="fa fa-check"></i><b>3.2.1</b> Libraries</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-02.html"><a href="ch-02.html#variables-in-a-data-frame"><i class="fa fa-check"></i><b>3.2.2</b> Variables in a data frame</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-02.html"><a href="ch-02.html#time-objects"><i class="fa fa-check"></i><b>3.2.3</b> Time objects</a></li>
<li class="chapter" data-level="3.2.4" data-path="ch-02.html"><a href="ch-02.html#variable-re--definition"><i class="fa fa-check"></i><b>3.2.4</b> Variable (re-) definition</a></li>
<li class="chapter" data-level="3.2.5" data-path="ch-02.html"><a href="ch-02.html#selecting-cleaning-and-gap-filling"><i class="fa fa-check"></i><b>3.2.5</b> Selecting, cleaning and gap-filling</a></li>
<li class="chapter" data-level="3.2.6" data-path="ch-02.html"><a href="ch-02.html#functions-1"><i class="fa fa-check"></i><b>3.2.6</b> Functions</a></li>
<li class="chapter" data-level="3.2.7" data-path="ch-02.html"><a href="ch-02.html#data-overview"><i class="fa fa-check"></i><b>3.2.7</b> Data overview</a></li>
<li class="chapter" data-level="3.2.8" data-path="ch-02.html"><a href="ch-02.html#data-visualisation-i"><i class="fa fa-check"></i><b>3.2.8</b> Data visualisation I</a></li>
<li class="chapter" data-level="3.2.9" data-path="ch-02.html"><a href="ch-02.html#aggregating"><i class="fa fa-check"></i><b>3.2.9</b> Aggregating</a></li>
<li class="chapter" data-level="3.2.10" data-path="ch-02.html"><a href="ch-02.html#data-visualisation-ii"><i class="fa fa-check"></i><b>3.2.10</b> Data visualisation II</a></li>
<li class="chapter" data-level="3.2.11" data-path="ch-02.html"><a href="ch-02.html#functional-programming-i"><i class="fa fa-check"></i><b>3.2.11</b> Functional programming I</a></li>
<li class="chapter" data-level="3.2.12" data-path="ch-02.html"><a href="ch-02.html#strings"><i class="fa fa-check"></i><b>3.2.12</b> Strings</a></li>
<li class="chapter" data-level="3.2.13" data-path="ch-02.html"><a href="ch-02.html#combining-relational-data"><i class="fa fa-check"></i><b>3.2.13</b> Combining relational data</a></li>
<li class="chapter" data-level="3.2.14" data-path="ch-02.html"><a href="ch-02.html#key-points-from-the-tutorial-1"><i class="fa fa-check"></i><b>3.2.14</b> Key points from the tutorial</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-02.html"><a href="ch-02.html#exercise-1"><i class="fa fa-check"></i><b>3.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-03.html"><a href="ch-03.html"><i class="fa fa-check"></i><b>4</b> Data variety</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-03.html"><a href="ch-03.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-03.html"><a href="ch-03.html#overview"><i class="fa fa-check"></i><b>4.1.1</b> Overview</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-03.html"><a href="ch-03.html#learning-objectives-1"><i class="fa fa-check"></i><b>4.1.2</b> Learning objectives</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-03.html"><a href="ch-03.html#key-points-from-the-lecture-2"><i class="fa fa-check"></i><b>4.1.3</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-03.html"><a href="ch-03.html#tutorial-2"><i class="fa fa-check"></i><b>4.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-03.html"><a href="ch-03.html#overview-1"><i class="fa fa-check"></i><b>4.2.1</b> Overview</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-03.html"><a href="ch-03.html#modis-remote-download"><i class="fa fa-check"></i><b>4.2.2</b> MODIS remote download</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-03.html"><a href="ch-03.html#points-on-the-globe"><i class="fa fa-check"></i><b>4.2.3</b> Points on the globe</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-03.html"><a href="ch-03.html#shapefiles"><i class="fa fa-check"></i><b>4.2.4</b> Shapefiles</a></li>
<li class="chapter" data-level="4.2.5" data-path="ch-03.html"><a href="ch-03.html#rasters"><i class="fa fa-check"></i><b>4.2.5</b> Rasters</a></li>
<li class="chapter" data-level="4.2.6" data-path="ch-03.html"><a href="ch-03.html#key-points-from-the-tutorial-2"><i class="fa fa-check"></i><b>4.2.6</b> Key points from the tutorial</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-03.html"><a href="ch-03.html#exercise-2"><i class="fa fa-check"></i><b>4.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-04.html"><a href="ch-04.html"><i class="fa fa-check"></i><b>5</b> Data Scraping</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-04.html"><a href="ch-04.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-04.html"><a href="ch-04.html#key-points-from-the-lecture-3"><i class="fa fa-check"></i><b>5.1.1</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-04.html"><a href="ch-04.html#tutorial-3"><i class="fa fa-check"></i><b>5.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-04.html"><a href="ch-04.html#r-packages-and-functions"><i class="fa fa-check"></i><b>5.2.1</b> R-Packages and Functions</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-04.html"><a href="ch-04.html#the-fishbase-website"><i class="fa fa-check"></i><b>5.2.2</b> The FishBase website</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-04.html"><a href="ch-04.html#accessing-fishbase"><i class="fa fa-check"></i><b>5.2.3</b> Accessing FishBase</a></li>
<li class="chapter" data-level="5.2.4" data-path="ch-04.html"><a href="ch-04.html#scraping-numbers"><i class="fa fa-check"></i><b>5.2.4</b> Scraping Numbers</a></li>
<li class="chapter" data-level="5.2.5" data-path="ch-04.html"><a href="ch-04.html#scraping-text-snippetrs"><i class="fa fa-check"></i><b>5.2.5</b> Scraping Text Snippetrs</a></li>
<li class="chapter" data-level="5.2.6" data-path="ch-04.html"><a href="ch-04.html#scraping-tables"><i class="fa fa-check"></i><b>5.2.6</b> Scraping Tables</a></li>
<li class="chapter" data-level="5.2.7" data-path="ch-04.html"><a href="ch-04.html#the-fishbase-package"><i class="fa fa-check"></i><b>5.2.7</b> The Fishbase Package</a></li>
<li class="chapter" data-level="5.2.8" data-path="ch-04.html"><a href="ch-04.html#summary"><i class="fa fa-check"></i><b>5.2.8</b> Summary</a></li>
<li class="chapter" data-level="5.2.9" data-path="ch-04.html"><a href="ch-04.html#case-study"><i class="fa fa-check"></i><b>5.2.9</b> Case Study</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-04.html"><a href="ch-04.html#exercise-3"><i class="fa fa-check"></i><b>5.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-05.html"><a href="ch-05.html"><i class="fa fa-check"></i><b>6</b> Catch-up</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-05.html"><a href="ch-05.html#loops-in-r"><i class="fa fa-check"></i><b>6.1</b> Loops in R</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ch-05.html"><a href="ch-05.html#some-simple-examples"><i class="fa fa-check"></i><b>6.1.1</b> Some simple examples</a></li>
<li class="chapter" data-level="6.1.2" data-path="ch-05.html"><a href="ch-05.html#nested-loops"><i class="fa fa-check"></i><b>6.1.2</b> Nested loops</a></li>
<li class="chapter" data-level="6.1.3" data-path="ch-05.html"><a href="ch-05.html#exercise-4"><i class="fa fa-check"></i><b>6.1.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ch-05.html"><a href="ch-05.html#functional-programming-using-purr"><i class="fa fa-check"></i><b>6.2</b> Functional programming using purr</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-05.html"><a href="ch-05.html#shortcuts-in-a-purrr-function"><i class="fa fa-check"></i><b>6.2.1</b> Shortcuts in a <code>purrr</code> function</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-05.html"><a href="ch-05.html#workflow-nested-data-map-and-mutate"><i class="fa fa-check"></i><b>6.2.2</b> Workflow: nested data, map and mutate</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-05.html"><a href="ch-05.html#string-manipulations"><i class="fa fa-check"></i><b>6.3</b> String Manipulations</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-05.html"><a href="ch-05.html#introduction-to-strings"><i class="fa fa-check"></i><b>6.3.1</b> Introduction to strings</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-05.html"><a href="ch-05.html#matching-and-extracting-patterns"><i class="fa fa-check"></i><b>6.3.2</b> Matching and extracting patterns</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-05.html"><a href="ch-05.html#advanced-example"><i class="fa fa-check"></i><b>6.3.3</b> Advanced example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-05.html"><a href="ch-05.html#web-scraping-in-a-nut-shell"><i class="fa fa-check"></i><b>6.4</b> Web-scraping in a nut-shell</a></li>
<li class="chapter" data-level="6.5" data-path="ch-05.html"><a href="ch-05.html#tidyverses-filter-and-select"><i class="fa fa-check"></i><b>6.5</b> Tidyverse’s filter and select</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ch-05.html"><a href="ch-05.html#introduction-4"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="ch-05.html"><a href="ch-05.html#select"><i class="fa fa-check"></i><b>6.5.2</b> Select()</a></li>
<li class="chapter" data-level="6.5.3" data-path="ch-05.html"><a href="ch-05.html#filter"><i class="fa fa-check"></i><b>6.5.3</b> Filter()</a></li>
<li class="chapter" data-level="6.5.4" data-path="ch-05.html"><a href="ch-05.html#exercises"><i class="fa fa-check"></i><b>6.5.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5.5" data-path="ch-05.html"><a href="ch-05.html#solutions"><i class="fa fa-check"></i><b>6.5.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ch-05.html"><a href="ch-05.html#preparing-data-for-ggplot"><i class="fa fa-check"></i><b>6.6</b> Preparing data for ggplot()</a></li>
<li class="chapter" data-level="6.7" data-path="ch-05.html"><a href="ch-05.html#base-r-functions"><i class="fa fa-check"></i><b>6.7</b> Base R functions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-06.html"><a href="ch-06.html"><i class="fa fa-check"></i><b>7</b> Supervised Machine Learning I</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-06.html"><a href="ch-06.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-06.html"><a href="ch-06.html#learning-objectives-2"><i class="fa fa-check"></i><b>7.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-06.html"><a href="ch-06.html#important-points-from-the-lecture"><i class="fa fa-check"></i><b>7.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-06.html"><a href="ch-06.html#tutorial-4"><i class="fa fa-check"></i><b>7.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-06.html"><a href="ch-06.html#overfitting"><i class="fa fa-check"></i><b>7.2.1</b> Overfitting</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-06.html"><a href="ch-06.html#motivation"><i class="fa fa-check"></i><b>7.2.2</b> Modelling challenge</a></li>
<li class="chapter" data-level="7.2.3" data-path="ch-06.html"><a href="ch-06.html#a-selection-of-ml-models"><i class="fa fa-check"></i><b>7.2.3</b> A selection of ML models</a></li>
<li class="chapter" data-level="7.2.4" data-path="ch-06.html"><a href="ch-06.html#data-splitting"><i class="fa fa-check"></i><b>7.2.4</b> Data splitting</a></li>
<li class="chapter" data-level="7.2.5" data-path="ch-06.html"><a href="ch-06.html#preprocessing"><i class="fa fa-check"></i><b>7.2.5</b> Pre-processing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-06.html"><a href="ch-06.html#model-formulation"><i class="fa fa-check"></i><b>7.3</b> Model formulation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ch-06.html"><a href="ch-06.html#formula-notation"><i class="fa fa-check"></i><b>7.3.1</b> Formula notation</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-06.html"><a href="ch-06.html#exercise-5"><i class="fa fa-check"></i><b>7.4</b> Exercise</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-06.html"><a href="ch-06.html#reading-and-cleaning"><i class="fa fa-check"></i><b>7.4.1</b> Reading and cleaning</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-06.html"><a href="ch-06.html#data-splitting-1"><i class="fa fa-check"></i><b>7.4.2</b> Data splitting</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch-06.html"><a href="ch-06.html#linear-model"><i class="fa fa-check"></i><b>7.4.3</b> Linear model</a></li>
<li class="chapter" data-level="7.4.4" data-path="ch-06.html"><a href="ch-06.html#pre-processing"><i class="fa fa-check"></i><b>7.4.4</b> Pre-processing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-formulation-1.html"><a href="model-formulation-1.html"><i class="fa fa-check"></i><b>8</b> Model formulation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="model-formulation-1.html"><a href="model-formulation-1.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="model-formulation-1.html"><a href="model-formulation-1.html#leaerning-objectives"><i class="fa fa-check"></i><b>8.1.1</b> Leaerning objectives</a></li>
<li class="chapter" data-level="8.1.2" data-path="model-formulation-1.html"><a href="model-formulation-1.html#key-points-from-the-lecture-4"><i class="fa fa-check"></i><b>8.1.2</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="model-formulation-1.html"><a href="model-formulation-1.html#tutorial-5"><i class="fa fa-check"></i><b>8.2</b> Tutorial</a></li>
<li class="chapter" data-level="8.3" data-path="model-formulation-1.html"><a href="model-formulation-1.html#the-generic-train"><i class="fa fa-check"></i><b>8.3</b> The generic <code>train()</code></a></li>
<li class="chapter" data-level="8.4" data-path="model-formulation-1.html"><a href="model-formulation-1.html#recipes"><i class="fa fa-check"></i><b>8.4</b> Recipes</a></li>
<li class="chapter" data-level="8.5" data-path="model-formulation-1.html"><a href="model-formulation-1.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>8.5</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="8.6" data-path="model-formulation-1.html"><a href="model-formulation-1.html#resampling"><i class="fa fa-check"></i><b>8.6</b> Resampling</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="model-formulation-1.html"><a href="model-formulation-1.html#putting-it-all-together"><i class="fa fa-check"></i><b>8.6.1</b> Putting it all together</a></li>
<li class="chapter" data-level="8.6.2" data-path="model-formulation-1.html"><a href="model-formulation-1.html#model-evaluation"><i class="fa fa-check"></i><b>8.6.2</b> Model evaluation</a></li>
<li class="chapter" data-level="8.6.3" data-path="model-formulation-1.html"><a href="model-formulation-1.html#model-interpretation"><i class="fa fa-check"></i><b>8.6.3</b> Model interpretation</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="model-formulation-1.html"><a href="model-formulation-1.html#exercise-6"><i class="fa fa-check"></i><b>8.7</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-07.html"><a href="ch-07.html"><i class="fa fa-check"></i><b>9</b> Supervised Machine Learning II</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch-07.html"><a href="ch-07.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch-07.html"><a href="ch-07.html#leaerning-objectives-1"><i class="fa fa-check"></i><b>9.1.1</b> Leaerning objectives</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-07.html"><a href="ch-07.html#key-points-from-the-lecture-5"><i class="fa fa-check"></i><b>9.1.2</b> Key points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-07.html"><a href="ch-07.html#tutorial-6"><i class="fa fa-check"></i><b>9.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch-07.html"><a href="ch-07.html#model-formulation-2"><i class="fa fa-check"></i><b>9.2.1</b> Model formulation</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-06.html"><a href="ch-06.html#training"><i class="fa fa-check"></i><b>9.2.2</b> Model training</a></li>
<li class="chapter" data-level="9.2.3" data-path="ch-07.html"><a href="ch-07.html#model-evaluation-1"><i class="fa fa-check"></i><b>9.2.3</b> Model evaluation</a></li>
<li class="chapter" data-level="9.2.4" data-path="ch-07.html"><a href="ch-07.html#bonus-model-interpretation"><i class="fa fa-check"></i><b>9.2.4</b> Bonus: Model interpretation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-07.html"><a href="ch-07.html#exercise-7"><i class="fa fa-check"></i><b>9.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-08.html"><a href="ch-08.html"><i class="fa fa-check"></i><b>10</b> Application 1: Variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch-08.html"><a href="ch-08.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="ch-08.html"><a href="ch-08.html#application"><i class="fa fa-check"></i><b>10.2</b> Application</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ch-08.html"><a href="ch-08.html#warm-up-1-nested-for-loop"><i class="fa fa-check"></i><b>10.2.1</b> Warm-up 1: Nested for-loop</a></li>
<li class="chapter" data-level="10.2.2" data-path="ch-08.html"><a href="ch-08.html#warm-up-2-find-the-best-single-predictor"><i class="fa fa-check"></i><b>10.2.2</b> Warm-up 2: Find the best single predictor</a></li>
<li class="chapter" data-level="10.2.3" data-path="ch-08.html"><a href="ch-08.html#full-stepwise-regression"><i class="fa fa-check"></i><b>10.2.3</b> Full stepwise regression</a></li>
<li class="chapter" data-level="10.2.4" data-path="ch-08.html"><a href="ch-08.html#bonus-stepwise-regression-out-of-the-box"><i class="fa fa-check"></i><b>10.2.4</b> Bonus: Stepwise regression out-of-the-box</a></li>
<li class="chapter" data-level="10.2.5" data-path="ch-08.html"><a href="ch-08.html#bonus-best-subset-selection"><i class="fa fa-check"></i><b>10.2.5</b> Bonus: Best Subset Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-09.html"><a href="ch-09.html"><i class="fa fa-check"></i><b>11</b> Supervised Neural Networks I</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ch-09.html"><a href="ch-09.html#introduction-9"><i class="fa fa-check"></i><b>11.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ch-09.html"><a href="ch-09.html#learning-objectives-3"><i class="fa fa-check"></i><b>11.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.1.2" data-path="ch-09.html"><a href="ch-09.html#important-points-from-the-lecture-1"><i class="fa fa-check"></i><b>11.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ch-09.html"><a href="ch-09.html#tutorial-7"><i class="fa fa-check"></i><b>11.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ch-09.html"><a href="ch-09.html#set-up"><i class="fa fa-check"></i><b>11.2.1</b> Set-up</a></li>
<li class="chapter" data-level="11.2.2" data-path="ch-09.html"><a href="ch-09.html#keras-for-linear-models"><i class="fa fa-check"></i><b>11.2.2</b> Keras for linear models</a></li>
<li class="chapter" data-level="11.2.3" data-path="ch-09.html"><a href="ch-09.html#tuning-learning-rate"><i class="fa fa-check"></i><b>11.2.3</b> Tuning learning rate</a></li>
<li class="chapter" data-level="11.2.4" data-path="ch-09.html"><a href="ch-09.html#logistic-regression"><i class="fa fa-check"></i><b>11.2.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-09.html"><a href="ch-09.html#exercise-8"><i class="fa fa-check"></i><b>11.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-10.html"><a href="ch-10.html"><i class="fa fa-check"></i><b>12</b> Supervised Neural Networks II</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ch-10.html"><a href="ch-10.html#introduction-10"><i class="fa fa-check"></i><b>12.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ch-10.html"><a href="ch-10.html#learning-objectives-4"><i class="fa fa-check"></i><b>12.1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.1.2" data-path="ch-10.html"><a href="ch-10.html#important-points-from-the-lecture-2"><i class="fa fa-check"></i><b>12.1.2</b> Important points from the lecture</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ch-10.html"><a href="ch-10.html#tutorial-8"><i class="fa fa-check"></i><b>12.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ch-10.html"><a href="ch-10.html#import-libraries-1"><i class="fa fa-check"></i><b>12.2.1</b> Import libraries</a></li>
<li class="chapter" data-level="12.2.2" data-path="ch-10.html"><a href="ch-10.html#construct-a-toy-dataset"><i class="fa fa-check"></i><b>12.2.2</b> Construct a toy dataset</a></li>
<li class="chapter" data-level="12.2.3" data-path="ch-10.html"><a href="ch-10.html#build-and-train-nn"><i class="fa fa-check"></i><b>12.2.3</b> Build and train NN</a></li>
<li class="chapter" data-level="12.2.4" data-path="ch-10.html"><a href="ch-10.html#model-performance"><i class="fa fa-check"></i><b>12.2.4</b> Model performance</a></li>
<li class="chapter" data-level="12.2.5" data-path="ch-10.html"><a href="ch-10.html#influence-of-nn-architecture"><i class="fa fa-check"></i><b>12.2.5</b> Influence of NN architecture</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ch-10.html"><a href="ch-10.html#exercise-9"><i class="fa fa-check"></i><b>12.3</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ch-11.html"><a href="ch-11.html"><i class="fa fa-check"></i><b>13</b> Application 2: Neural Networks and Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ch-11.html"><a href="ch-11.html#introduction-11"><i class="fa fa-check"></i><b>13.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ch-11.html"><a href="ch-11.html#learning-goals"><i class="fa fa-check"></i><b>13.1.1</b> Learning Goals</a></li>
<li class="chapter" data-level="13.1.2" data-path="ch-11.html"><a href="ch-11.html#key-points-from-previous-lectures"><i class="fa fa-check"></i><b>13.1.2</b> Key Points from Previous Lectures</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ch-11.html"><a href="ch-11.html#application-1"><i class="fa fa-check"></i><b>13.2</b> Application</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ch-11.html"><a href="ch-11.html#problem-statement"><i class="fa fa-check"></i><b>13.2.1</b> Problem Statement</a></li>
<li class="chapter" data-level="13.2.2" data-path="ch-11.html"><a href="ch-11.html#data-preparation"><i class="fa fa-check"></i><b>13.2.2</b> Data preparation</a></li>
<li class="chapter" data-level="13.2.3" data-path="ch-11.html"><a href="ch-11.html#center-and-scale"><i class="fa fa-check"></i><b>13.2.3</b> Center and scale</a></li>
<li class="chapter" data-level="13.2.4" data-path="ch-11.html"><a href="ch-11.html#building-a-simple-model-with-keras-subtask-1"><i class="fa fa-check"></i><b>13.2.4</b> Building a simple model with keras ( SubTask 1)</a></li>
<li class="chapter" data-level="13.2.5" data-path="ch-11.html"><a href="ch-11.html#cross-validation"><i class="fa fa-check"></i><b>13.2.5</b> Cross validation</a></li>
<li class="chapter" data-level="13.2.6" data-path="ch-11.html"><a href="ch-11.html#parameter-tuning"><i class="fa fa-check"></i><b>13.2.6</b> Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-12.html"><a href="ch-12.html"><i class="fa fa-check"></i><b>14</b> Supervised Deep Learning I</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ch-12.html"><a href="ch-12.html#introduction-12"><i class="fa fa-check"></i><b>14.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="ch-12.html"><a href="ch-12.html#learning-objectives-5"><i class="fa fa-check"></i><b>14.1.1</b> Learning objectives</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ch-12.html"><a href="ch-12.html#tutorial-9"><i class="fa fa-check"></i><b>14.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ch-12.html"><a href="ch-12.html#building-blocks-of-cnns"><i class="fa fa-check"></i><b>14.2.1</b> Building Blocks of CNNs</a></li>
<li class="chapter" data-level="14.2.2" data-path="ch-12.html"><a href="ch-12.html#build-the-model"><i class="fa fa-check"></i><b>14.2.2</b> Build the model</a></li>
<li class="chapter" data-level="14.2.3" data-path="ch-12.html"><a href="ch-12.html#compile-and-train-the-model"><i class="fa fa-check"></i><b>14.2.3</b> Compile and train the model</a></li>
<li class="chapter" data-level="14.2.4" data-path="ch-12.html"><a href="ch-12.html#reduce-overfitting"><i class="fa fa-check"></i><b>14.2.4</b> Reduce Overfitting</a></li>
<li class="chapter" data-level="14.2.5" data-path="ch-12.html"><a href="ch-12.html#visualizing-a-cnn"><i class="fa fa-check"></i><b>14.2.5</b> Visualizing a CNN</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ch-12.html"><a href="ch-12.html#exercise-10"><i class="fa fa-check"></i><b>14.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ch-12.html"><a href="ch-12.html#import-libraries-and-data"><i class="fa fa-check"></i><b>14.3.1</b> Import libraries and data</a></li>
<li class="chapter" data-level="14.3.2" data-path="ch-12.html"><a href="ch-12.html#tasks"><i class="fa fa-check"></i><b>14.3.2</b> Tasks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-13.html"><a href="ch-13.html"><i class="fa fa-check"></i><b>15</b> Supervised Deep Learning II</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ch-13.html"><a href="ch-13.html#introduction-13"><i class="fa fa-check"></i><b>15.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="ch-13.html"><a href="ch-13.html#learning-objectives-6"><i class="fa fa-check"></i><b>15.1.1</b> Learning objectives</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ch-13.html"><a href="ch-13.html#tutorial-10"><i class="fa fa-check"></i><b>15.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ch-13.html"><a href="ch-13.html#dataset"><i class="fa fa-check"></i><b>15.2.1</b> Dataset</a></li>
<li class="chapter" data-level="15.2.2" data-path="ch-13.html"><a href="ch-13.html#naive-models-old-world"><i class="fa fa-check"></i><b>15.2.2</b> Naive models (Old World)</a></li>
<li class="chapter" data-level="15.2.3" data-path="ch-13.html"><a href="ch-13.html#neural-networks-new-world"><i class="fa fa-check"></i><b>15.2.3</b> Neural Networks (New World)</a></li>
<li class="chapter" data-level="15.2.4" data-path="ch-13.html"><a href="ch-13.html#model-comparison"><i class="fa fa-check"></i><b>15.2.4</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ch-13.html"><a href="ch-13.html#exercise-11"><i class="fa fa-check"></i><b>15.3</b> Exercise</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="ch-13.html"><a href="ch-13.html#import-libraries-2"><i class="fa fa-check"></i><b>15.3.1</b> Import libraries</a></li>
<li class="chapter" data-level="15.3.2" data-path="ch-13.html"><a href="ch-13.html#load-data"><i class="fa fa-check"></i><b>15.3.2</b> Load data</a></li>
<li class="chapter" data-level="15.3.3" data-path="ch-13.html"><a href="ch-13.html#preprocess-ndvi-images"><i class="fa fa-check"></i><b>15.3.3</b> Preprocess NDVI images</a></li>
<li class="chapter" data-level="15.3.4" data-path="ch-13.html"><a href="ch-13.html#part-1"><i class="fa fa-check"></i><b>15.3.4</b> Part 1</a></li>
<li class="chapter" data-level="15.3.5" data-path="ch-13.html"><a href="ch-13.html#part-2"><i class="fa fa-check"></i><b>15.3.5</b> Part 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Environmental Systems Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-06" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Supervised Machine Learning I</h1>
<div id="introduction-5" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Introduction</h2>
<div id="learning-objectives-2" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Learning objectives</h3>
<p>After this learning unit, you will be able to …</p>
<ul>
<li>Differentiate machine learning from classical programming</li>
<li>Describe the different variants of machine learning</li>
<li>Conceptualize model training as an optimization problem</li>
<li>Describe overfitting and how it can be measured (training vs. validation error).</li>
<li>Formulate a model in R.</li>
<li>Discuss why, when, and how to pre-process data.</li>
<li>Measure and minimize loss for regression and classification (video)</li>
<li>Describe the fundamentals of gradient descent (video)</li>
</ul>
</div>
<div id="important-points-from-the-lecture" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Important points from the lecture</h3>
<p>Machine learning refers to a class of algorithms that automatically generate statistical models of data. There are two main types of machine learning:</p>
<p><strong>Unsupervised machine learning</strong>: A class of algorithms that automatically detect patterns in data without using labels or ‘learning without a teacher’. Examples are: PCAs, k-means clustering, autoencoders, self-organizing maps, etc.</p>
<p><strong>Supervised machine learning</strong>: A class of algorithms that automatically learn an input-output relationships based on example input-output pairs. Examples include: support vector machines, random forests, decision trees, neural networks, etc. Supervised machine learning requires three ingredients: (1) Input data (2) Output data (3) A measure of model performance (a.k.a. “loss”). Supervised machine learning be used for regression (predict a continuous label) or classification (predict a categorical label).</p>
<p><strong>Loss</strong> is a concept central to many supervised machine learning algorithms. It measures how well our predicted model values fit the actual observed model values, a higher value indicates a higher loss, essentially meaning the model fits less well. Ideally, loss should be minimised. Loss can be used to update our model parameters in the next iteration of model training, this is called learning.</p>
</div>
</div>
<div id="tutorial-4" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Tutorial</h2>
<div id="overfitting" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Overfitting</h3>
<p>Machine learning (ML) may appear magical. The ability of ML algorithms to detect patterns and make predictions is fascinating. However, several challenges have to be met in the process of formulating, training, and evaluating the models. In this practical we will discuss some basics of supervised ML and how to achieve best predictive results.</p>
<p>In general, the aim of supervised ML is to find a model <span class="math inline">\(\hat{Y} = f(X)\)</span> that is <em>trained</em> (calibrated) using observed relationships between a set of <em>features</em> (also known as <em>predictors</em>, or <em>labels</em>, or <em>independent variables</em>) <span class="math inline">\(X\)</span> and the <em>target</em> variable <span class="math inline">\(Y\)</span>. Note, that <span class="math inline">\(Y\)</span> is observed. The hat on <span class="math inline">\(\hat{Y}\)</span> denotes an estimate. Some algorithms can even handle predictions of multiple target variables simultaneously (e.g., neural networks). ML algorithms consist of (more or less) flexible mathematical models with a certain structure and set of parameters. At the simple extreme end of the model spectrum is the univariate linear regression. You may not want to call this a ML algorithm because there is no iterative learning involved. Nevertheless, also univariate linear regression provides a prediction <span class="math inline">\(\hat{Y} = f(X)\)</span>, just like other (proper) ML algorithms do. The functional form of a linear regression is not particularly flexible (just a straight line for the best fit between predictors and targets) and it has only two parameters (slope and intercept). At the other extreme end are, for example, deep neural networks. They are extremely flexible, can learn highly non-linear relationships and deal with interactions between a large number of predictors. They also contain very large numbers of parameters, typically on the order of thousands. You can imagine that this allows these types of algorithms to very effectively learn from the data, but also bears the risk of <em>overfitting</em>.</p>
<p>What is overfitting? The following example illustrates it. Let’s assume that there is some true underlying relationship between a predictor <span class="math inline">\(x\)</span> and the target variable <span class="math inline">\(y\)</span>. We don’t know this relationship (in the code below, this is <code>true_fun()</code>) and the observations contain a (normally distributed) error (<code>y = true_fun(x) + 0.1 * rnorm(n_samples)</code>). Based on our training data (<code>df_train</code>), we fit three polynomial models of degree 1, 4, and 15 to the observations. A polynomial of degree N is given by: <span class="math display">\[
y = \sum_{n=0}^N a_n x^n
\]</span> <span class="math inline">\(a_n\)</span> are the coefficients, i.e., model parameters. The goal of the training is to get the coefficients <span class="math inline">\(a_n\)</span>. From the above definition, the polynomial of degree 15 has 16 parameters, while the polynomial of degree 1 has two parameters (and corresponds to a simple linear regression). You can imagine that the polynomial of degree 15 is much more flexible and should thus yield the closest fit to the training data. This is indeed the case.</p>
<p><img src="esds_book_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We can use the same fitted models on unseen data - the <em>validation data</em>. This is what’s done below. Again, the same true underlying relationship is used, but we sample a new set of data points in x and add a new sample of errors on top of the true relationship.</p>
<p><img src="esds_book_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>You see that, using the validation set, we find that “poly4” actually performs the best - it has a much lower RMSE that “poly15”. Apparently, “poly15” was overfitted. Apparently, it indeed used its flexibility to fit not only the shape of the true underlying relationship, but also the observation errors on top of it. This has obviously the implication that, when this model is used to make predictions for data that was not used for training (calibration), it will yield misguided predictions that are affected by the errors in the training set. In the above pictures we can also conclude that “poly1” was underfitted.</p>
<p>It gets even worse when applying the fitted polynomial models to data that extends beyond the range in <span class="math inline">\(x\)</span> that was used for model training. Here, we’re extending just 50% to the left and to the right.</p>
<p><img src="esds_book_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>You see that the RMSE for “poly15” literally explodes. The model is hopelessly overfitted and completely useless for prediction, although it looked like it fit the data best when we considered at the training results. This is a fundamental challenge in ML - finding the model with the best <em>generalisability</em>. That is, a model that not only fits the training data well, but also performs well on unseen data.</p>
<p>The phenomenon of fitting/overfitting as a function of the model “flexibility” is also referred to as <em>bias vs. variance trade-off</em>. The bias describes how well a model matches the training set (average error). A model with low bias will match the data set closely and vice versa. The variance describes how much a model changes when you train it using different portions of your data set. “poly15” has a high variance, but much of its variance is the result of misled training on observation errors. On the other extreme, “poly1” has a high bias. It’s not affected by the noise in observations, but its predictions are also far off the observations. In ML, we are challenged to balance this trade-off. In Figure <a href="#fig:tradeoff"><strong>??</strong></a> you can see a schematic illustration of the bias–variance trade-off.</p>
<p>The next chapters introduce methods to achieve the best model generalisability and to find the sweet spot between high bias and high variance. The steps to get there include the splitting of data into training and testing sets, pre-processing of data and model training which “steers” the model towards what is considered a good model fit in terms of its generalisation power.</p>
<p>You have learned in video 6a about the basic setup of supervised ML, with input data containing the features (or predictors) <span class="math inline">\(X\)</span>, predicted (<span class="math inline">\(\hat{Y}\)</span>) and observed target values (<span class="math inline">\(Y\)</span>, also known as <em>labels</em>). In video 6b (title 6c: loss and it’s minimization), you learned about the loss function which quantifies the agreement between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span> and defines the objective of the model training. Here, you’ll learn how all of this can be implemented in R. Depending on your application or research question, it may also be of interest to evaluate the relationships embodied in <span class="math inline">\(f(X)\)</span> or to quantify the importance of different predictors in our model. This is referred to as <em>model interpretation</em> and is introduced in the respectively named subsection. Finally, we’ll get into <em>feature selection</em> in the next Application session.</p>
<p>The topic of supervised machine learning methods covers enough material to fill two sessions. Therefore, we split this part in two. Model training, implementing the an entire modelling workflow, model evaluation and interpretation will be covered in the next session’s tutorial (Supervised Machine Learning Methods II).</p>
<p>Of course, a plethora of algorithms exist that do the job of <span class="math inline">\(Y = f(X)\)</span>. Each of them has its own strengths and limitations. It is beyond the scope of this course to introduce a larger number of ML algorithms. Subsequent sessions will focus primarily on Artificial Neural Networks (ANN) - a type of ML algorithm that has gained popularity for its capacity to efficiently learn patterns in large data sets. For illustration purposes in this and the next chapter, we will briefly introduce two simple alternative “ML” methods, linear regression and K-nearest-neighbors. They have quite different characteristics and are therefore great for illustration purposes in this chapter.</p>
</div>
<div id="motivation" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Modelling challenge</h3>
<p>The environment determines ecosystem-atmosphere exchange fluxes of water vapour and CO2. Temporally changing mass exchange fluxes can be continuously measured with the eddy covariance technique, while abiotic variables (meteorological variables, soil moisture) can be measured in parallel. This offers an opportunity for building models that predict mass exchange fluxes from the environment.</p>
<p>In this tutorial, we formulate a model for predicting ecosystem gross primary production (photosynthesis) from environmental covariates. This is to say that <code>GPP_NT_VUT_REF</code> is the target variable, and other available variables available in the dataset can be used as predictors.</p>
<p>Data is provided here at daily resolution for a site (‘CH-Dav’) located in the Swiss alps (Davos). This is one of the longest-running eddy covariance sites globally and measures fluxes in a evergreen coniferous forest with cold winters and temperate, relatively moist summers.</p>
<p>For more information of the variables in the dataset, see <a href="http://fluxnet.fluxdata.org/data/fluxnet2015-dataset/">FLUXNET 2015 website</a>, and <a href="https://www.nature.com/articles/s41597-020-0534-3">Pastorello et al., 2020</a> for a comprehensive documentation of variable definitions and methods.</p>
<p><strong>Available variables:</strong></p>
<ul>
<li><code>TIMESTAMP</code>: Day of measurement.</li>
<li><code>TA_F</code>: Air temperature. The meaning of suffix <code>_F</code> is described in <a href="https://www.nature.com/articles/s41597-020-0534-3">Pastorello et al., 2020</a>.</li>
<li><code>SW_IN_F</code>: Shortwave incoming radiation</li>
<li><code>LW_IN_F</code>: Longwave incoming radiation</li>
<li><code>VPD_F</code>: Vapour pressure deficit (relates to the humidity of the air)</li>
<li><code>PA_F</code>: Atmospheric pressure</li>
<li><code>P_F</code>: Precipitation</li>
<li><code>WS_F</code>: Wind speed</li>
<li><code>GPP_NT_VUT_REF</code>: Gross primary production - <strong>the target variable</strong></li>
<li><code>NEE_VUT_REF_QC</code>: Quality control information for <code>GPP_NT_VUT_REF</code>. Specifies the fraction of high-quality underlying high-frequency data from which the daily data is derived. 0.8 = 80% underlying high-quality data, remaining 20% of the high-frequency data is gap-filled.</li>
</ul>
</div>
<div id="a-selection-of-ml-models" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> A selection of ML models</h3>
<div id="linear-models" class="section level4" number="7.2.3.1">
<h4><span class="header-section-number">7.2.3.1</span> Linear Models</h4>
<p>The simplest form of a linear model (LM) is the univariate linear regression, where we assume a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \;\;\; i = 1, 2, ...n \;,
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the i-th observation of the target variable, and <span class="math inline">\(X_i\)</span> is the i-th value of the (single) predictor variable. The errors <span class="math inline">\(\epsilon_i\)</span> are assumed to be independent from each other (no autocorrelation), normally distributed, have mean of zero and a constant variance. <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are constant coefficients (model parameters). Fitting a linear regression is finding the values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> so that the sum of the square errors is minimized, that is:</p>
<p><span class="math display">\[
\sum_i \epsilon_i^2 = \sum_i (Y_i - \beta_0 - \beta_1 X_i)^2 = \text{min}.
\]</span></p>
<p>Since the expected value of <span class="math inline">\(\epsilon\)</span> is zero (because it’s normally distributed with mean zero), predictions of a linear regression model are obtained by <span class="math inline">\(Y_\text{new} = \beta_0 + \beta_1 X_\text{new}\)</span>.</p>
<p>It’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of <span class="math inline">\(p\)</span> predictor variables:
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \; ... \; + \beta_p X_p + \epsilon \;.
\]</span>
Note that here, <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(\epsilon\)</span> are vectors of length corresponding to the number of observations in our data set (<span class="math inline">\(n\)</span> - as above). Analogously, calibrating the <span class="math inline">\(p\)</span> coefficients <span class="math inline">\(\beta_1, \beta_2, ..., \beta_p\)</span> is to minimize the sum of square errors <span class="math inline">\(\sum_i \epsilon_i^2\)</span>. While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and so on.</p>
</div>
<div id="k-nearest-neighbours" class="section level4" number="7.2.3.2">
<h4><span class="header-section-number">7.2.3.2</span> K-nearest neighbours</h4>
<p>As the name suggests, the K-nearest neighbour (KNN) uses the <span class="math inline">\(k\)</span> observations that are “nearest” to the new record for which we want to make a prediction. It then calculates their average (in regression) or most frequent value (in classification) as the prediction. “Nearest” is determined by some distance metric evaluated based on the values of the predictors. In our example (<code>GPP_NT_VUT_REF ~ .</code>), KNN would determine the <span class="math inline">\(k\)</span> days where conditions, given by our set of predictors, were most similar (nearest) to the day for which we seek a prediction. Then, it calculates the prediction as the average (mean) GPP value of these days. Determining “nearest” neighbors is commonly based on either the Euclidean or Manhattan distances between two data points <span class="math inline">\(x_a\)</span> and <span class="math inline">\(x_b\)</span>, considering all <span class="math inline">\(p\)</span> predictors <span class="math inline">\(j\)</span>.</p>
<p>Euclidean distance:
<span class="math display">\[
\sqrt{ \sum_{j=1}^p (x_{a,j} - x_{b,j})^2  } \\
\]</span>
Manhattan distance:
<span class="math display">\[
\sum_{j=1}^p | x_{a,j} - x_{b,j} |
\]</span></p>
<p>In two-dimensional space, the Euclidean distance measures the length of a straight line between two points (remember Pythagoras!). The Manhattan distance is called this way because it measures the distance you would have to walk to get from point <span class="math inline">\(a\)</span> to point <span class="math inline">\(b\)</span> in Manhattan, New York, where you cannot cut corners but have to follow a rectangular grid of streets. <span class="math inline">\(|x|\)</span> is the positive value of <span class="math inline">\(x\)</span> ( <span class="math inline">\(|-x| = x\)</span>).</p>
<p>KNN is a simple algorithm that uses knowledge of the “local” data structure for prediction. A drawback is that the model training has to be done for each prediction step and the computation time of the training increases with <span class="math inline">\(x \times p\)</span>. KNNs are used, for example, to impute values (fill missing values) and have the advantage that predicted values are always within the range of observed values of the target variable.</p>
</div>
<div id="random-forest" class="section level4" number="7.2.3.3">
<h4><span class="header-section-number">7.2.3.3</span> Random forest</h4>
<p>Random forest (RF) models are based on <em>decision trees</em>, where binary decisions for predicting the target’s values are based on thresholds of the predictors’ values. The <em>depth</em> of a decision tree refers to the number of such decisions. The deeper a tree, the more likely the model will overfit.</p>
<p>Just as forests are made up by trees, <em>random Forest</em> models make use of random subsets of the original data and of available predictions and respective decision trees. Predictions are then made by averaging predictions of individual <em>base learners</em> (the decision trees). The number of predictors considered at each decision step is a tunable parameter (a <em>hyperparameter</em>, typically called <span class="math inline">\(m_{try}\)</span>). Introducing this randomness is effective because decision trees tend to overfit and because of the <em>wisdom of the crowd</em> - i.e., the power of aggregating individual predictions with their random error (and without systematic bias) for generating accurate and relatively precise predictions.</p>
<p>Random forest models have gained particular popularity and are widely applied in environmental sciences not only for their power, but also for their ease of use. No pre-processing (centering, scaling) is necessary, they can deal with skewed data, and can effectively learn interactions between predictors.</p>
<p>You can learn more on how random forests work in the book <a href="https://bradleyboehmke.github.io/HOML/random-forest.html">Hands On Machine-Learning in R</a>.</p>
<p>Before we move on to the actual implementation of LM, KNN and RF, we first have to understand how we can properly prepare our data in order to fit the respective model’s requirements and to validate model performance.</p>
</div>
</div>
<div id="data-splitting" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Data splitting</h3>
<div id="reading-and-wrangling-data" class="section level4" number="7.2.4.1">
<h4><span class="header-section-number">7.2.4.1</span> Reading and wrangling data</h4>
<p>There is a difference between data wrangling and pre-processing as part of the modelling workflow. <em>Data wrangling</em> can be considered to encompass the steps to prepare the data set prior to modelling, including, combining variables from different sources, removal of bad or missing data, and aggregating to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample). See the <a href="https://github.com/Quartz/bad-data-guide">Quartz Guide to Bad Data</a> for an overview of how to deal with different types of bad data.</p>
<p>In contrast, <em>data pre-processing</em> refers to the additional steps that are either required by the ML algorithm (e.g. centering and scaling for KNN or neural networks) or the transformation of variables guided by the resulting improvement of the predictive power of the ML model. In other words, pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data.</p>
<p>Let’s read the data, select relevant variables, convert the time stamp column to a time object and interpret missing values (encoded <code>-9999</code> in the file).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="ch-06.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="ch-06.html#cb1-2" aria-hidden="true" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb1-3"><a href="ch-06.html#cb1-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-4"><a href="ch-06.html#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="do">## select only the variables we are interested in</span></span>
<span id="cb1-5"><a href="ch-06.html#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(TIMESTAMP,</span>
<span id="cb1-6"><a href="ch-06.html#cb1-6" aria-hidden="true" tabindex="-1"></a>         GPP_NT_VUT_REF,    <span class="co"># the target</span></span>
<span id="cb1-7"><a href="ch-06.html#cb1-7" aria-hidden="true" tabindex="-1"></a>         NEE_VUT_REF_QC,    <span class="co"># quality control info</span></span>
<span id="cb1-8"><a href="ch-06.html#cb1-8" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ends_with</span>(<span class="st">&quot;_F&quot;</span>),   <span class="co"># includes all all meteorological variables</span></span>
<span id="cb1-9"><a href="ch-06.html#cb1-9" aria-hidden="true" tabindex="-1"></a>         <span class="sc">-</span><span class="fu">contains</span>(<span class="st">&quot;JSB&quot;</span>)   <span class="co"># weird useless variable</span></span>
<span id="cb1-10"><a href="ch-06.html#cb1-10" aria-hidden="true" tabindex="-1"></a>         ) <span class="sc">%&gt;%</span></span>
<span id="cb1-11"><a href="ch-06.html#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="ch-06.html#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="do">## convert to a nice date object</span></span>
<span id="cb1-13"><a href="ch-06.html#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">TIMESTAMP =</span> lubridate<span class="sc">::</span><span class="fu">ymd</span>(TIMESTAMP)) <span class="sc">%&gt;%</span></span>
<span id="cb1-14"><a href="ch-06.html#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="ch-06.html#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="do">## set all -9999 to NA</span></span>
<span id="cb1-16"><a href="ch-06.html#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na_if</span>(<span class="sc">-</span><span class="dv">9999</span>) <span class="sc">%&gt;%</span></span>
<span id="cb1-17"><a href="ch-06.html#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="ch-06.html#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## drop QC variables (no longer needed), except NEE_VUT_REF_QC</span></span>
<span id="cb1-19"><a href="ch-06.html#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="fu">ends_with</span>(<span class="st">&quot;_QC&quot;</span>), NEE_VUT_REF_QC)</span></code></pre></div>
<p>The column <code>NEE_VUT_REF_QC</code> provides information about the fraction of gap-filled half-hourly data used to calculate daily aggregates. Let’s use only <code>GPP_NT_VUT_REF</code> data, where at least 80% of the underlying half-hourly data was good quality measured data, and not gap-filled. Make sure to not actually remove the respective rows, but rather replace values with NA.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="ch-06.html#cb2-1" aria-hidden="true" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> ddf <span class="sc">%&gt;%</span> </span>
<span id="cb2-2"><a href="ch-06.html#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">GPP_NT_VUT_REF =</span> <span class="fu">ifelse</span>(NEE_VUT_REF_QC <span class="sc">&lt;</span> <span class="fl">0.8</span>, <span class="cn">NA</span>, GPP_NT_VUT_REF))</span></code></pre></div>
<p>At this stage, we won’t use <code>NEE_VUT_REF_QC</code> any longer. So we can drop it.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="ch-06.html#cb3-1" aria-hidden="true" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> ddf <span class="sc">%&gt;%</span> </span>
<span id="cb3-2"><a href="ch-06.html#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>NEE_VUT_REF_QC)</span></code></pre></div>
</div>
<div id="splitting-into-testing-and-training-sets" class="section level4" number="7.2.4.2">
<h4><span class="header-section-number">7.2.4.2</span> Splitting into testing and training sets</h4>
<p>The introductory example impressively demonstrated the importance of validating the fitted model with data that was <strong>not</strong> used for training. Thus, we can test the model’s <em>generalisability.</em> The essential step that enables us to assess the model’s <em>generalization error</em> is to hold out part of the data from training, and set it aside (leaving it absolutely untouched) for <em>testing</em>.</p>
<p>There is no fixed rule for how much data are to be used for training and testing, respectively. We have to balance the trade-off between:</p>
<ul>
<li>Spending too much data for training will leave us with too little data for testing and the test results may not be robust. In this case, the sample size for getting robust validation statistics is not sufficiently large and we don’t know for sure whether we are safe from an over-fit model.</li>
<li>Spending too much data for validation will leave us with too little data for training. In this case, the ML algorithm may not be successful at finding real relationships due to insufficient amounts of training data.</li>
</ul>
<p>Typical splits are between 60-80% for training. However, in cases where the number of data points is very large, the gains from having more training data are marginal, but come at the cost of adding to the already high computational burden of model training.</p>
<p>In environmental sciences, the number of predictors is often smaller than the sample size (<span class="math inline">\(p &lt; n\)</span>), because its typically easier to collect repeated observations of a particular variable than to expand the set of variables being observed. Nevertheless, in cases where the number <span class="math inline">\(p\)</span> gets large, it is important, and for some algorithms mandatory, to maintain <span class="math inline">\(p &lt; n\)</span> for model training.</p>
<p>An important aspect to consider when splitting the data is to make sure that all “states” of the system for which we have data are approximately equally represented in training and testing sets. This is to make sure that the algorithm learns relationships <span class="math inline">\(f(X)\)</span> also under rare conditions <span class="math inline">\(X\)</span>, for example meteorological extreme events.</p>
<p>Several alternative functions for the data splitting step are available from different packages in R. We will use the the <strong>rsample</strong> package because it allows to additionally make sure that data from the full range of a given variable’s values (<code>VPD_F</code> in the example below) are well covered in both training and testing sets.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="ch-06.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rsample)</span>
<span id="cb4-2"><a href="ch-06.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb4-3"><a href="ch-06.html#cb4-3" aria-hidden="true" tabindex="-1"></a>split     <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ddf, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;VPD_F&quot;</span>)</span>
<span id="cb4-4"><a href="ch-06.html#cb4-4" aria-hidden="true" tabindex="-1"></a>ddf_train <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb4-5"><a href="ch-06.html#cb4-5" aria-hidden="true" tabindex="-1"></a>ddf_test  <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
<p>Plot the distribution of values in the training and testing sets.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="ch-06.html#cb5-1" aria-hidden="true" tabindex="-1"></a>ddf_train <span class="sc">%&gt;%</span> </span>
<span id="cb5-2"><a href="ch-06.html#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">split =</span> <span class="st">&quot;train&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb5-3"><a href="ch-06.html#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(ddf_test <span class="sc">%&gt;%</span> </span>
<span id="cb5-4"><a href="ch-06.html#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">split =</span> <span class="st">&quot;test&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb5-5"><a href="ch-06.html#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">9</span>, <span class="at">names_to =</span> <span class="st">&quot;variable&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;value&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb5-6"><a href="ch-06.html#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> ..density.., <span class="at">color =</span> split)) <span class="sc">+</span></span>
<span id="cb5-7"><a href="ch-06.html#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb5-8"><a href="ch-06.html#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>variable, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</div>
<div id="preprocessing" class="section level3" number="7.2.5">
<h3><span class="header-section-number">7.2.5</span> Pre-processing</h3>
<div id="the-use-of-recipes" class="section level4" number="7.2.5.1">
<h4><span class="header-section-number">7.2.5.1</span> The use of recipes</h4>
<p>Skewed data, outliers, and values covering multiple orders of magnitude can create difficulties for certain ML algorithms, e.g., or K-nearest neighbours, or neural networks. Other algorithms, like tree-based methods (e.g., Random Forest), are more robust against such issues.</p>
<p>When defining any pre-processing step, it should be specified as a “recipe” or “blueprint”, and not actually executed on the data itself before we start with the model training. Such a “recipe” can then be applied to any new data, while the parameters of the data pre-processing transformations are different each time. We will introduce an example below where we divide all values in the testing and training dataset by the standard deviation of the respective dataset. Note that, because the two datasets hold different values they also have different standard deviations. The same “recipe” (i.e., division by standard deviation) will be applied to both datasets but the transformation is done with a different parameter (i.e., the different standard deviations in either dataset).</p>
<p>The reason for not actually executing the same data transformation on both dataset is the risk of <em>data leakage</em>. This happens when information from the validation data somehow finds its way into the training step. Don’t worry if this sounds incomprehensible. We’ll learn more about it below. For now, you can focus on the types of pre-processing steps, what they do, and how they are implemented as part of a pre-processing “recipe” in R.</p>
<p>Many pre-processing steps can be done either by hand or using different R packages. Below, we give you a mix of both approaches to emphasize on how pre-processing is done. The <a href="https://recipes.tidymodels.org/">recipes</a> package offers a powerful way to specify pre-processing steps in R and is gaining traction as part of the <a href="https://www.tidymodels.org/">tidymodels</a> ecosystem. Moreover, it is compatible with the <a href="https://topepo.github.io/caret/">caret</a> package that will be used to formulate the different ML models.</p>
</div>
<div id="target-engineering" class="section level4" number="7.2.5.2">
<h4><span class="header-section-number">7.2.5.2</span> Target engineering</h4>
<p>Target engineering refers to pre-processing of the target variable. Its application can enable improved predictions, particularly for models that make assumptions about errors (e.g., normally distributed errors in linear regression) and when the target variable follows a “special” distribution (e.g., heavily skewed distribution, or where the target variable is a fraction that is naturally bounded by 0 and 1).</p>
<p>A simple log-transformation of the target variable can often resolve issues with skewed distributions. An implication of a log-transformation is that errors in predicting values in the upper end of the observed range do not affect the model disproportionately compared to errors in the lower range.</p>
<p>In our data set of half-hourly ecosystem flux and meteorological measurements, the variable <code>WS_F</code> (wind speed) is skewed. The target variable that we have considered so far (<code>GPP_NT_VUT_REF</code>) is not skewed. In a case where we would consider <code>WS_F</code> to be our target variable, we would thus consider applying a log-transformation.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="ch-06.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)  <span class="co"># to combine two plots into separate panels of a single plot</span></span>
<span id="cb6-2"><a href="ch-06.html#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="ch-06.html#cb6-3" aria-hidden="true" tabindex="-1"></a>gg1 <span class="ot">&lt;-</span> ddf_train <span class="sc">%&gt;%</span> </span>
<span id="cb6-4"><a href="ch-06.html#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> WS_F, <span class="at">y =</span> ..density..)) <span class="sc">+</span></span>
<span id="cb6-5"><a href="ch-06.html#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb6-6"><a href="ch-06.html#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Original&quot;</span>)</span>
<span id="cb6-7"><a href="ch-06.html#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="ch-06.html#cb6-8" aria-hidden="true" tabindex="-1"></a>gg2 <span class="ot">&lt;-</span> ddf_train <span class="sc">%&gt;%</span> </span>
<span id="cb6-9"><a href="ch-06.html#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(WS_F), <span class="at">y =</span> ..density..)) <span class="sc">+</span></span>
<span id="cb6-10"><a href="ch-06.html#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb6-11"><a href="ch-06.html#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Log-transformed &#39;by hand&#39;&quot;</span>)</span>
<span id="cb6-12"><a href="ch-06.html#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="ch-06.html#cb6-13" aria-hidden="true" tabindex="-1"></a>gg1 <span class="sc">+</span> gg2   <span class="co"># the + is from the patchwork library</span></span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Now, how can we do this using the <em>recipes</em> package? The syntax is that we define a model with target and predictor variables, the data the recipe should be applied to, and the steps of the recipe:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="ch-06.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(recipes)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;recipes&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stringr&#39;:
## 
##     fixed</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     step</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="ch-06.html#cb11-1" aria-hidden="true" tabindex="-1"></a>recipe_example <span class="ot">&lt;-</span> <span class="fu">recipe</span>(WS_F <span class="sc">~</span> ., <span class="at">data =</span> ddf) <span class="sc">%&gt;%</span> <span class="fu">step_log</span>(<span class="fu">all_outcomes</span>())</span>
<span id="cb11-2"><a href="ch-06.html#cb11-2" aria-hidden="true" tabindex="-1"></a>recipe_example</span></code></pre></div>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          8
## 
## Operations:
## 
## Log transformation on all_outcomes()</code></pre>
<p>The formula, specified as an argument in the <code>recipe()</code> function, defines variable “roles” (in recipes-speak). What’s left of the <code>~</code> is interpreted as an “outcome” (the target variable). By writing <code>~ .</code>, we specify all remaining variables in the data frame as “predictors”. As an argument in the <code>step_log()</code> function, we write <code>all_outcomes()</code> to declare that the log transformation is applied only to the “outcome” (target) variable.</p>
<p>The output of <code>recipe_example</code> tells us that the recipe holds a log-transformation on all outcome variables (here, 1). As it is in the real world, writing a recipe does not mean to actually cook it, respectively in the <em>recipes</em> terminology to bake it. There are two more steps involved to get there. This might seem a nuisance at first but their separation is actually quite beautiful and translates the conception of the pre-processing as a “blueprint” into the way we write the code. You’ll understand why this is so useful throughout the next Chapters.</p>
<p>The full routine from writing the recipe to preparing and baking it is as follows:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="ch-06.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">## First, we have to prepare everything, taking the prepared recipe and our ingredients (data)</span></span>
<span id="cb13-2"><a href="ch-06.html#cb13-2" aria-hidden="true" tabindex="-1"></a>prep_example <span class="ot">&lt;-</span> <span class="fu">prep</span>(recipe_example, <span class="at">training =</span> ddf_train)</span>
<span id="cb13-3"><a href="ch-06.html#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="ch-06.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Second, we have to bake our mixture, to get a nice meal </span></span>
<span id="cb13-5"><a href="ch-06.html#cb13-5" aria-hidden="true" tabindex="-1"></a>df_baked <span class="ot">&lt;-</span> <span class="fu">bake</span>(prep_example, <span class="at">new_data =</span> ddf_train)</span></code></pre></div>
<p>The log-transformed data from the recipe-routine should look the same as when simply using the <em>log()</em> command as was done above.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="ch-06.html#cb14-1" aria-hidden="true" tabindex="-1"></a>gg3 <span class="ot">&lt;-</span> df_baked <span class="sc">%&gt;%</span> </span>
<span id="cb14-2"><a href="ch-06.html#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> WS_F, <span class="at">y =</span> ..density..)) <span class="sc">+</span></span>
<span id="cb14-3"><a href="ch-06.html#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb14-4"><a href="ch-06.html#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;log-transformed &#39;by recipe&#39;&quot;</span>)</span>
<span id="cb14-5"><a href="ch-06.html#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="ch-06.html#cb14-6" aria-hidden="true" tabindex="-1"></a>(gg1 <span class="sc">+</span> gg2) <span class="sc">/</span> (<span class="fu">plot_spacer</span>() <span class="sc">+</span> gg3)</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Yes! You can see, both approaches to do a log-transformation resulted in a more-or-less normal distribution of our data.</p>
<p>Of course, for a log-transformation this recipe-routine may seem a bit over-the-top. But depending on your data and the distribution that is needed for your model to work, different transformations can be done. For example, the log-transformation does not necessarily result in a perfect normal distribution. Therefore, one can use the <em>Box-Cox</em> transformation which is less straight-forward to do. Fortunately, the <em>recipes</em> package already provides the function <em>step_BoxCox()</em>, so that we do not need to implement a <em>Box-Cox</em> transformation by hand.</p>
</div>
<div id="standardization" class="section level4" number="7.2.5.3">
<h4><span class="header-section-number">7.2.5.3</span> Standardization</h4>
<p>Several algorithms explicitly require data to be standardized. That is, values of all predictors vary within a comparable range. The necessity of this step becomes obvious when considering neural networks, the activation functions of each node have to deal with standardized inputs. In other words, inputs have to vary over the same range, expecting a mean of zero and standard deviation of one. To get a quick overview of the distribution of all variables (columns) in our data frame, we can use the <em>skimr</em> package.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="ch-06.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(skimr)</span>
<span id="cb15-2"><a href="ch-06.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">skim</span>(ddf_train)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-13">Table 7.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">ddf_train</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">4604</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">9</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Date</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: Date</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">min</th>
<th align="left">max</th>
<th align="left">median</th>
<th align="right">n_unique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">TIMESTAMP</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">1997-01-01</td>
<td align="left">2014-12-29</td>
<td align="left">2005-12-29</td>
<td align="right">4604</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">GPP_NT_VUT_REF</td>
<td align="right">287</td>
<td align="right">0.94</td>
<td align="right">3.18</td>
<td align="right">2.75</td>
<td align="right">-4.23</td>
<td align="right">0.75</td>
<td align="right">2.82</td>
<td align="right">5.35</td>
<td align="right">12.26</td>
<td align="left">▁▇▆▃▁</td>
</tr>
<tr class="even">
<td align="left">TA_F</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">3.43</td>
<td align="right">6.72</td>
<td align="right">-21.92</td>
<td align="right">-1.65</td>
<td align="right">3.38</td>
<td align="right">8.62</td>
<td align="right">20.69</td>
<td align="left">▁▂▇▇▂</td>
</tr>
<tr class="odd">
<td align="left">SW_IN_F</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">150.04</td>
<td align="right">84.51</td>
<td align="right">3.30</td>
<td align="right">78.19</td>
<td align="right">136.54</td>
<td align="right">215.36</td>
<td align="right">365.89</td>
<td align="left">▆▇▆▅▂</td>
</tr>
<tr class="even">
<td align="left">LW_IN_F</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">268.97</td>
<td align="right">42.46</td>
<td align="right">138.12</td>
<td align="right">238.09</td>
<td align="right">272.01</td>
<td align="right">303.33</td>
<td align="right">364.79</td>
<td align="left">▁▃▇▇▂</td>
</tr>
<tr class="odd">
<td align="left">VPD_F</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.87</td>
<td align="right">2.41</td>
<td align="right">0.00</td>
<td align="right">0.99</td>
<td align="right">2.24</td>
<td align="right">4.06</td>
<td align="right">15.98</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="even">
<td align="left">PA_F</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">83.56</td>
<td align="right">0.73</td>
<td align="right">80.49</td>
<td align="right">83.16</td>
<td align="right">83.68</td>
<td align="right">84.07</td>
<td align="right">85.63</td>
<td align="left">▁▂▅▇▁</td>
</tr>
<tr class="odd">
<td align="left">P_F</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.31</td>
<td align="right">5.78</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1.70</td>
<td align="right">92.10</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">WS_F</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.98</td>
<td align="right">0.65</td>
<td align="right">0.33</td>
<td align="right">1.53</td>
<td align="right">1.92</td>
<td align="right">2.32</td>
<td align="right">6.20</td>
<td align="left">▃▇▂▁▁</td>
</tr>
</tbody>
</table>
<p>We see for example, that typical values of <code>LW_IN_F</code> are by a factor 100 larger than values of <code>VPD_F</code>. KNN uses the distance from neighbouring points for predictions. Obviously, in this case here, any distance would be dominated by <code>LW_IN_F</code> and distances in the “direction” of <code>VPD_F</code>, even when relatively large, would not be influential, neither for a Euclidean nor a Manhattan distance (see <a href="ch-01.html#introduction">2.1</a>). Therefore, it is reasonable to scale distances in a dataset via standardization so that such absolute differences become irrelevant. Note that the activation functions of neural networks take inputs values in a given range (e.g., 0-1) and thus, the data also has to be standardized prior to model training.</p>
<p>Standardization is done by dividing each variable, that is all values in one column, by the standard deviation of that variable, and then subtracting its mean. This way, the resulting standardized values are centered around 0, and scaled such that a value of 1 means that the data point is one standard deviation above the mean of the respective variable (column).</p>
<p>When applied to all predictors individually, the absolute values of their variations can be directly compared and only then it can be meaningfully used for determining the distance. Standardization can be done not only by centering and scaling (as described above), but also by <em>scaling to within range</em>, where values are scaled such that the minimum value within each variable (column) is 0 and the maximum is 1.</p>
<p>In order to avoid <em>data leakage</em>, centering and scaling has to be done separately for each split into training and validation data (more on that later). In other words, don’t center and scale the entire data frame with the mean and standard deviation derived from the entire data frame, but instead center and scale with mean and standard deviation derived from the training portion of the data, and apply that also to the validation portion, when evaluating.</p>
<p>The <em>caret</em> package takes care of this. The R package <a href="https://topepo.github.io/caret/"><strong>caret</strong></a> provides a unified interface for using different ML algorithms implemented in separate packages. The preprocessing steps applied with each resampling fold can be specified using the function <code>preProcess()</code>. More on resampling in Chapter <a href="ch-06.html#training">7.4.3.1</a>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="ch-06.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb16-2"><a href="ch-06.html#cb16-2" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(ddf_train, <span class="at">method =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</span>
<span id="cb16-3"><a href="ch-06.html#cb16-3" aria-hidden="true" tabindex="-1"></a>pp</span></code></pre></div>
<pre><code>## Created from 4317 samples and 9 variables
## 
## Pre-processing:
##   - centered (8)
##   - ignored (1)
##   - scaled (8)</code></pre>
<p>As seen above for the feature engineering example, this does not return a standardized version of the data frame <code>ddf</code>. Rather, it returns the information that allows us to apply the same standardization also to other data sets. In other words, we use the distribution of values in the data set to which we applied the function to determine the centering and scaling (here: mean and standard deviation). Note that the “ignored” variable is the <code>TIMESTAMP</code> which is formated as date and thus cannot be scaled and centered.</p>
<p>Using the <em>recipes</em> package, the code goes as follows:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="ch-06.html#cb18-1" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">recipe</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> ddf_train) <span class="sc">%&gt;%</span> </span>
<span id="cb18-2"><a href="ch-06.html#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_center</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb18-3"><a href="ch-06.html#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_scale</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>())</span>
<span id="cb18-4"><a href="ch-06.html#cb18-4" aria-hidden="true" tabindex="-1"></a>pp</span></code></pre></div>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          8
## 
## Operations:
## 
## Centering for all_numeric(), -all_outcomes()
## Scaling for all_numeric(), -all_outcomes()</code></pre>
<p>Here, we used selectors to apply the recipe step to several variables at once. The first selector, <code>all_numeric()</code>, selects all variables that are either integers or real values. The second selector, <code>-all_outcomes()</code> removes any outcome (target) variables from this recipe step. As you can see from the output of <code>pp</code>, both recipes hold the same information to be applied when training a ML model.</p>
</div>
<div id="zero-variance-predictors" class="section level4" number="7.2.5.4">
<h4><span class="header-section-number">7.2.5.4</span> Zero-variance predictors</h4>
<p>Sometimes, the data generation process yields variables that have the same value in each observation. This can be due to failure of the measurement device or another bug in the data collection pipeline. Either way, this may cause some algorithms to crash or become unstable. Such “zero-variance” predictors are usually removed altogether. The same applies also to variables with “<em>near</em>-zero variance”. That is, variables where only a few unique values occur in the entire data set with a high frequency. The danger is that, when data is split into training and testing sets, the variable may effectively become a “zero-variance” variable within the training subset.</p>
<p>We can test for zero-variance or near-zero variance predictors by quantifying the following metrics:</p>
<ul>
<li>Frequency ratio: Ratio of the frequency of the most common predictor over the second most common predictor. This should be near 1 for well-behaved predictors and get very large for problematic ones.</li>
<li>Percent unique values: The number of unique values divided by the total number of rows in the data set (times 100). For problematic variables, this ratio gets small (approaches 1/100).</li>
</ul>
<p>The function <code>nearZeroVar</code> of the caret package flags suspicious variables (<code>zeroVar = TRUE</code> or <code>nzv = TRUE</code>). In our data set, we don’t find any:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="ch-06.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nearZeroVar</span>(ddf, <span class="at">saveMetrics=</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>##                freqRatio percentUnique zeroVar   nzv
## TIMESTAMP       1.000000    100.000000   FALSE FALSE
## GPP_NT_VUT_REF  1.000000     93.732887   FALSE FALSE
## TA_F            1.000000     87.085488   FALSE FALSE
## SW_IN_F         1.500000     98.843931   FALSE FALSE
## LW_IN_F         1.000000     97.916033   FALSE FALSE
## VPD_F           1.142857     62.062671   FALSE FALSE
## PA_F            1.090909     38.469729   FALSE FALSE
## P_F            10.268072      5.978096   FALSE FALSE
## WS_F            1.083333     35.853362   FALSE FALSE</code></pre>
<p>Using the <em>recipes</em> package, we can add a step that removes zero-variance predictors by. Note that the following code picks up the <code>recipe_example</code> holding the log-transformation from above and extends the entire recipe by a removal of zero-variance predictors.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="ch-06.html#cb22-1" aria-hidden="true" tabindex="-1"></a>recipe_example <span class="ot">&lt;-</span> recipe_example <span class="sc">%&gt;%</span> <span class="fu">step_zv</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb22-2"><a href="ch-06.html#cb22-2" aria-hidden="true" tabindex="-1"></a>recipe_example</span></code></pre></div>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          8
## 
## Operations:
## 
## Log transformation on all_outcomes()
## Zero variance filter on all_predictors()</code></pre>
</div>
<div id="one-hot-encoding" class="section level4" number="7.2.5.5">
<h4><span class="header-section-number">7.2.5.5</span> One-hot encoding</h4>
<p>For ML algorithms that require that all predictors be numerical (e.g., neural networks, or KNN), categorical predictors have to be pre-processed and converted into new numerical predictors. The most common transformation is <em>one-hot encoding</em>, where a categorical feature (predictor) with <span class="math inline">\(N\)</span> levels is replaced by <span class="math inline">\(N\)</span> new features that contain either zeros or ones depending whether the value of the categorical feature corresponds to the respective column. This creates perfect colinearity between these new columns which allows to drop one column. In other words, one needs <span class="math inline">\(N-1\)</span> new features to describe a categorical variable with <span class="math inline">\(N\)</span> levels. This is referred to as <em>dummy encoding</em>. Check out Figure <a href="ch-06.html#fig:ohe">7.1</a> for a one-hot encoding visualization.</p>
<div class="figure"><span style="display:block;" id="fig:ohe"></span>
<img src="figures/06_figure_ohe.png" alt="Visualization of one-hot encoding from [Kaggle](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding)." width="354" />
<p class="caption">
Figure 7.1: Visualization of one-hot encoding from <a href="https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding">Kaggle</a>.
</p>
</div>
<p>Let’s have a hands-on example to get a better graps on this. The following code chunk loads the <code>iris</code> dataset which holds three different levels for the categorical feature <code>Species</code>:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="ch-06.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(datasets)</span>
<span id="cb24-2"><a href="ch-06.html#cb24-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> iris <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Sepal.Length, Sepal.Width, Species) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(Sepal.Width)</span>
<span id="cb24-3"><a href="ch-06.html#cb24-3" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> <span class="fu">head</span>() <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Sepal.Length</th>
<th align="right">Sepal.Width</th>
<th align="left">Species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5.0</td>
<td align="right">2.0</td>
<td align="left">versicolor</td>
</tr>
<tr class="even">
<td align="right">6.0</td>
<td align="right">2.2</td>
<td align="left">versicolor</td>
</tr>
<tr class="odd">
<td align="right">6.2</td>
<td align="right">2.2</td>
<td align="left">versicolor</td>
</tr>
<tr class="even">
<td align="right">6.0</td>
<td align="right">2.2</td>
<td align="left">virginica</td>
</tr>
<tr class="odd">
<td align="right">4.5</td>
<td align="right">2.3</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">5.5</td>
<td align="right">2.3</td>
<td align="left">versicolor</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="ch-06.html#cb25-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>Species <span class="sc">%&gt;%</span> <span class="fu">levels</span>()</span></code></pre></div>
<pre><code>## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>
<p>Now, let’s use the handy <em>recipes</em> package to turn <code>Species</code> into several new columns via the <em>step_dummy()</em> command:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="ch-06.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="do">## To get a feeling for our dataset, we can also print a summary of our recipe</span></span>
<span id="cb27-2"><a href="ch-06.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Note that the formula below does not define any target variable and that the type of &#39;Species&#39; is nominal</span></span>
<span id="cb27-3"><a href="ch-06.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">recipe</span>( <span class="sc">~</span> ., <span class="at">data =</span> df) <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## # A tibble: 3 x 4
##   variable     type    role      source  
##   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
## 1 Sepal.Length numeric predictor original
## 2 Sepal.Width  numeric predictor original
## 3 Species      nominal predictor original</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="ch-06.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Let&#39;s bake our recipe!</span></span>
<span id="cb29-2"><a href="ch-06.html#cb29-2" aria-hidden="true" tabindex="-1"></a>baked_df <span class="ot">&lt;-</span> <span class="fu">recipe</span>( <span class="sc">~</span> ., <span class="at">data =</span> iris) <span class="sc">%&gt;%</span> </span>
<span id="cb29-3"><a href="ch-06.html#cb29-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(Species, <span class="at">one_hot =</span> T) <span class="sc">%&gt;%</span> </span>
<span id="cb29-4"><a href="ch-06.html#cb29-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">prep</span>(., <span class="at">data =</span> df) <span class="sc">%&gt;%</span></span>
<span id="cb29-5"><a href="ch-06.html#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bake</span>(., <span class="at">new_data =</span> df)</span>
<span id="cb29-6"><a href="ch-06.html#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="ch-06.html#cb29-7" aria-hidden="true" tabindex="-1"></a>baked_df <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(Sepal.Width) <span class="sc">%&gt;%</span>  <span class="fu">head</span>() <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Sepal.Length</th>
<th align="right">Sepal.Width</th>
<th align="right">Species_setosa</th>
<th align="right">Species_versicolor</th>
<th align="right">Species_virginica</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5.0</td>
<td align="right">2.0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">6.0</td>
<td align="right">2.2</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">6.2</td>
<td align="right">2.2</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">6.0</td>
<td align="right">2.2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">4.5</td>
<td align="right">2.3</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">5.5</td>
<td align="right">2.3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Compare the output of the dataframe before and after baking. Can you see how <code>Species</code> has been split into three numerical features and how the describe the different levels? Also, do you understand now, why this process is called 1-hot encoding?</p>
</div>
<div id="dealing-with-missingness-and-bad-data" class="section level4" number="7.2.5.6">
<h4><span class="header-section-number">7.2.5.6</span> Dealing with missingness and bad data</h4>
<p>Several ML algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. Data may be missing for several reasons. Some yield random patterns of missing data, others not. In the latter case, we can speak of <em>informative missingness</em> (Kuhn &amp; Johnson, 2003) and its information can be used for predictions. For categorical data, we may replace such data with <code>"none"</code> (instead of <code>NA</code>), while randomly missing data may be dropped altogether. Some ML algorithms (mainly tree-based methods, e.g., random forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand.</p>
<p>Visualising missing data is essential for making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal). The cells with missing data in a data frame can be eaily visualised e.g. with <code>vis_miss()</code> from the <em>visdat</em> package.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="ch-06.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(visdat)</span>
<span id="cb30-2"><a href="ch-06.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vis_miss</span>(</span>
<span id="cb30-3"><a href="ch-06.html#cb30-3" aria-hidden="true" tabindex="-1"></a>  ddf,</span>
<span id="cb30-4"><a href="ch-06.html#cb30-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">cluster =</span> <span class="cn">FALSE</span>, </span>
<span id="cb30-5"><a href="ch-06.html#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">warn_large_data =</span> <span class="cn">FALSE</span></span>
<span id="cb30-6"><a href="ch-06.html#cb30-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="esds_book_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>The question about what is “bad data” and whether or when it should be removed is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human writing the paper, it’s often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during its process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions.</p>
</div>
<div id="more-pre-processing" class="section level4" number="7.2.5.7">
<h4><span class="header-section-number">7.2.5.7</span> More pre-processing</h4>
<p>Depending on the algorithm and the data, additional pre-processing steps may be required. You can find more information about this in the great and freely available online tutorial <a href="https://bradleyboehmke.github.io/HOML/engineering.html#target-engineering">Hands-On Machine Learning in R</a>.</p>
<p>One such additional pre-processing step is <em>imputation</em>, where missing values are imputed (gap-filled), for example by the mean of each variable respectively. Also imputation is prone to cause data leakage and must therefore be implemented as part of the resampling and training workflow. The <em>recipes</em> package offers a great way to deal with imputation (and also all other pre-processing steps). <a href="https://bradleyboehmke.github.io/HOML/engineering.html#impute">Here</a> is a link to learn more about it.</p>
</div>
</div>
</div>
<div id="model-formulation" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Model formulation</h2>
<p>The aim of supervised ML is to find a model <span class="math inline">\(\hat{Y} = f(X)\)</span> so that <span class="math inline">\(\hat{Y}\)</span> agrees well with observations <span class="math inline">\(Y\)</span>. We typically start with a research question where <span class="math inline">\(Y\)</span> is given - naturally - by the problem we are addressing and we have a data set at hand where one or multiple predictors (or features) <span class="math inline">\(X\)</span> are recorded along with <span class="math inline">\(Y\)</span>. From our data, we have information about how GPP (ecosystem-level photosynthesis) depends on set of abiotic factors, mostly meteorological measurements.</p>
<div id="formula-notation" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Formula notation</h3>
<p>In R, it is common to use the <em>formula</em> notation to specify the target and predictor variables. You have probably encountered formulas before, e.g., for a linear regression using the <code>lm()</code> function. To specify a linear regression model for <code>GPP_NT_VUT_REF</code> with three predictors <code>SW_F_IN</code>, <code>VPD_F</code>, and <code>TA_F</code>, we write:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="ch-06.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_F_IN <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, <span class="at">data =</span> ddf)</span></code></pre></div>
<p>As mentioned throughout this tutorial, the <em>caret</em> package provides a unified interface to define different ML models and pre-processing recipes. In the next tutorial, we will learn more about how to do so. In this tutorial’s exercise you will deepen your understanding of data wrangling, splitting and pre-processing as well as setting up LM using the formula notation.</p>
</div>
</div>
<div id="exercise-5" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Exercise</h2>
<p>Now that you are familiar with the basic steps for supervised machine learning, you can get your hands on the data yourself and implement code for addressing the modelling task outlined in Chapter <a href="ch-06.html#motivation">7.2.2</a>.</p>
<div id="reading-and-cleaning" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Reading and cleaning</h3>
<p>Read the CSV file <code>"FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv"</code>, select all variables with name ending with <code>"_F"</code>, the variables <code>"TIMESTAMP"</code>, <code>"GPP_NT_VUT_REF"</code>, and <code>"NEE_VUT_REF_QC"</code>, and drop all variables that contain <code>"JSB"</code> in their name. Then convert the variable <code>"TIMESTAMP"</code> to a date-time object with the function <code>ymd()</code> from the <em>lubridate</em> package, and interpret all values <code>-9999</code> as missing values. Then, set all values of <code>"GPP_NT_VUT_REF"</code> to missing if the corresponding quality control variable indicates that less than 90% are measured data points. Finally, drop the variable <code>"NEE_VUT_REF_QC"</code> - we won’t use it anymore.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="ch-06.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
<div id="data-splitting-1" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Data splitting</h3>
<p>Split the data into a training and testing set, that contain 70% and 30% of the total available data, respectively.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="ch-06.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
<div id="linear-model" class="section level3" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Linear model</h3>
<div id="training" class="section level4" number="7.4.3.1">
<h4><span class="header-section-number">7.4.3.1</span> Training</h4>
<p>Fit a linear regression model using the base-R function <code>lm()</code> and the training set. The target variable is <code>"GPP_NT_VUT_REF"</code>, and predictor variables are all available meterological variables in the dataset. Answer the following questions:</p>
<ul>
<li>What is the <span class="math inline">\(R^2\)</span> of predicted vs. observed <code>"GPP_NT_VUT_REF"</code>?</li>
<li>Is the linear regression slope significantly different from zero for all predictors?</li>
<li>Is a linear regression model where insignificant predictors are removed better than a model that includes all predictors?</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="ch-06.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
<div id="prediction" class="section level4" number="7.4.3.2">
<h4><span class="header-section-number">7.4.3.2</span> Prediction</h4>
<p>With the model containing all predictors and fitted on <code>ddf_train</code>, make predictions using first <code>ddf_train</code> and then <code>ddf_test</code>. Compute the <span class="math inline">\(R^2\)</span> and the root-mean-square error, and visualise modelled vs. observed values to evaluate both predictions.</p>
<p>Do you expect the linear regression model trained on <code>ddf_train</code> to predict substantially better on <code>ddf_train</code> than on <code>ddf_test</code>? Why (not)?</p>
<p>Hints:</p>
<ul>
<li>To calculate predictions, use the generic function <code>predict()</code> with the argument <code>newdata = ...</code>.</li>
<li>The <span class="math inline">\(R^2\)</span> can be extracted from the model object as <code>summary(model_object)$r.squared</code>, or is (as the RMSE) given in the metrics data frame returned by <code>metrics()</code> from the <em>yardstick</em> library.</li>
<li>For visualisation the model performance, consider a scatterplot, or (better) a plot that reveals the density of overlapping points. (We’re plotting information from over 4000 data points here!)</li>
</ul>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="ch-06.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>
</div>
</div>
<div id="pre-processing" class="section level3" number="7.4.4">
<h3><span class="header-section-number">7.4.4</span> Pre-processing</h3>
<p>To get a better understanding of writing and baking recipes for pre-processing, load the dataset <code>starwars</code> from the <code>dplyr</code> package. Note that for this part of the exercise you do not need to split the data, just work with the full dataset. Do the following tasks:</p>
<p><strong>Encoding of factor levels:</strong></p>
<ul>
<li>Load the dataset and select the features height, mass and species</li>
<li>Drop all na entries from the dataset</li>
<li>Apply an one-hot encoding and a dummy encoding on the feature species</li>
<li>Answer the following questions:
<ul>
<li>How many columns does the un-processed original data frame have and how many columns are in the freshly “baked” data frames created by the One-hot encoding and how many in the one from the dummy-encoding? Explain the differences.</li>
<li>Which column was created by the One-hot-encoding but not by the dummy-encoding?</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="ch-06.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span>
<span id="cb36-2"><a href="ch-06.html#cb36-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> starwars</span></code></pre></div>
<p><strong>Sequential pre-processing:</strong></p>
<ul>
<li>Load the dataset and select the features height, mass and species</li>
<li>Drop all na entries from the dataset</li>
<li>Write and bake a recipe with the following steps (use the dataset after 2. for this):
<ul>
<li>Filter out zero or near-zero variance features</li>
<li>Standardize (center and scale) numeric features</li>
<li>Dummy encode categorical features.</li>
</ul></li>
<li>Visualise the distribution of the numerical</li>
<li>Answer the question: Does the order in which pre-processing steps are defined in the recipe matter?</li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="ch-06.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="do">## write your code here</span></span></code></pre></div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-05.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-formulation-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
