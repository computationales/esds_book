# 04_Data_Scraping

---

## 1. Introduction and learning objectives

This exercise covers _data scraping_ in R and yes, this is not a typo it is indeed spelt _scraping_ (to gather or extract) not _scrapping_ (to get rid of). The goal is to download and convert online data into a structured format that can be easily accessed and modified in R. As a case study, we will scrape data from a website containing various kinds of information about fish species, called [FishBase](https://www.fishbase.in/search.php). We will first extract the useful information about fish species and their families from the scrapped data. Then, we will learn how to read a table in HTML document and how to generate a new table with the extracted information. Later, we will clean our data and plot the relationship among different species with respect to its proportion and surface area. At the end we will run the model and make the predictions about fish richness.  


---

## 2. Summary of the theory

Let us first have a brief review of the main concept of the lecture.

The web is the largest source of information and often free to access. In some cases the information is already presented in a nice format and can be easily copied into an excel spreadsheet. 
However, this is often not the case and data-sets are only present in a format that are not easy to download or modify. Copying data from different sub pages and blocks manually is a tedious, slow and error prone approach. We therefore need a different technique to access such data in a more automated and efficient way. 
Web scraping is a popular technique to extract information from the web. It allows us to get unstructured data from the web and convert it into readable and structured formats. This structured data can then be further used as training, validation or test data sets for our machine learning algorithms. 

<div style="border: 2px black solid; border-radius: 7px; padding:10px">
We will now review some of the most important concepts and protocols used on websites.

- HTTP: The  <b>H</b>yper<b>t</b>ext <b>T</b>ransfer <b>P</b>rotocol is widely used protocol in information systems. Hypertext documents include hyperlinks to other resources that a user can easily access just by a mouse click or by tapping the screen in a web browser.

- HTML: The <b>H</b>yper<b>t</b>ext <b>M</b>arkup <b>L</b>anguage is markup language as the name suggests and is often used to build websites and describes their content and structure. HTML is organized using tags, which are surrounded by < > symbols. One problem when scraping HTML based websites is that not all the websites have the same html structure, different sites vary in their structure, request tags and HTML tags.

- XML: The e<b>X</b>tensible <b>M</b>arkup <b>L</b>anguage has certain similarities to HTML but the format is generally easier to read for machines. While HTML has a number of pre-defined tags, those are extensible in XML. This means that it allows to define and control the meaning of the elements contained in a document or text.

- API: An <b>A</b>pplication <b>P</b>rogramming <b>I</b>nterface is a set of procedures and communication protocols that provide access to the data of an application, operating system or other services. Both APIs and web scraping are used to retrieve data from websites, but their methodology differs substantially. APIs give us direct access to the data we would want, but they are limited to the corresponding website. As a result, we might find us in a scenario where there might not be an API to access the data we want. In these scenarios, web scrapping would allow us to access the data as long as it is available on a website. Hence APIs are very source/website specific and we can only do what is already implemented but in a clean fashion, while scrapping is more flexible and can be applied (nearly) everywhere but we have to handle all the formatting, inconsistencies, extraction, etc. by ourself.
</div>

---

## 3. Case Study - Web Scraping on FishBase

We will now demonstrate the principles of web scraping using a simple case study. We will extract fish occurrence data from an online database and perform some basic correlations with the obtained data. We will quickly introduce our data source, the FishBase database.

### 3.1 The FishBase website

[FishBase](https://www.fishbase.in/search.php) is a global species database of fish species. It provides data of various fish species, including information on taxonomy, geographical distribution, biometrics and morphology, behaviour and habitats, ecology and population dynamics as well as reproductive, metabolic and genetic data. Different tools, such as trophic pyramids, identification keys, biogeographical modelling and fishery statistics can be accessed on the website. Furthermore, direct species level links to information in other databases such as LarvalBase, GenBank, the IUCN Red List and the Catalog of Fishes exist. As of November 2018, FishBase included descriptions of 34,000 species and subspecies.
![Screenshot of the FishBase webpage for the species *Coregonus lavaretus*, a member of the family Salmonidae. It is widespread in freshwater systems from central and northwest Europe to Siberia.](../data/images/screen.png)
<b>Figure1.</b> Screenshot of the FishBase webpage for the species  *Coregonus lavaretus*, a member of the family Salmonidae. It is widespread in freshwater systems from central and northwest Europe to Siberia.

As shown in figure 1, the website contains lots of information for each species. This information is stored in various different data types, such as: 
- numbers in different formats and units (temperature ranges, latitudinal distribution)
- text blocks (description of the distribution)
- tables (fecundity, larvae information)
- pictures and videos (of the species, embedded into HTML code)

For a better understanding of the following R code, you are strongly encouraged to have a look at the [FishBase](https://fishbase.org) website in your browser.

In the next sections you will learn how to deal with the complex data types. A careful approach is required when extracting and downloading information from such datasets. It is an imporant first step as it will save you time looking for the important (or relevant) information and will make the application of machine learning algorithms more straight forward.


---

## Hands on

### 3.2 Preliminaries (Packages)

We will now load the packages necessary to perform the web scraping on FishBase, namely <b>RCurl</b> and <b>XML</b>. Since we will also use the <b>raster</b> and <b>rgdal</b> packages for spatial visualisation later in this tutorial, we will load them here in the beginning.

The package RCurl provides functions to allow us to compose general HTTP requests and provides convenient functions to fetch URLs via get and post requests and process the results returned by the web server. The package XML give us approaches for both <b>reading</b> (get request) and <b>creating</b> (post request) XML (and HTML) documents, both locally and on the web via HTTP. 


---

Before we proceed further, we would like to introduce function **lapply()** and **sapply()**.

**lapply()** : lapply returns a list of the same length as X, each element of which is the result of applying FUN (Function applied to each element of x) to the corresponding element of X. X is a vector (atomic or list) or an expression object. `lapply()` takes vector or data frame as input and gives output in list. The _**l**_ in `lapply()` stands for list.  

`lapply(X, FUN)`

Arguments:

-`X`: A vector or an object

-`FUN`: Function applied to each element of x

**For example :** A simple example is to change the string value of a matrix to lower case with tolower function. We construct a matrix fish. The name is in upper case format.


```R
fish <- c("FAMILY","SPECIES")
fish_lower <-lapply(fish, tolower)
str(fish_lower)
```

    List of 2
     $ : chr "family"
     $ : chr "species"


**sapply()** : sapply() function takes list, vector or data frame as input and gives output in vector or matrix. It is useful for operations on list objects and returns a list object of the same length as the original set. `sapply()` function does the same job as `lapply()` function but returns a vector.

---


In the next two lines we load the packages and save them in the vector 'lib_vec'.


```R
lib_vec <- c("RCurl", "XML", "raster", "rgdal", "rfishbase", "tidyverse", "sf")
sapply(lib_vec, library, character.only = TRUE)
```


<table>
<caption>A matrix: 23 × 7 of type chr</caption>
<thead>
	<tr><th scope=col>RCurl</th><th scope=col>XML</th><th scope=col>raster</th><th scope=col>rgdal</th><th scope=col>rfishbase</th><th scope=col>tidyverse</th><th scope=col>sf</th></tr>
</thead>
<tbody>
	<tr><td>sf       </td><td>sf       </td><td>sf       </td><td>sf       </td><td>sf       </td><td>sf       </td><td>sf       </td></tr>
	<tr><td>forcats  </td><td>forcats  </td><td>forcats  </td><td>forcats  </td><td>forcats  </td><td>forcats  </td><td>forcats  </td></tr>
	<tr><td>stringr  </td><td>stringr  </td><td>stringr  </td><td>stringr  </td><td>stringr  </td><td>stringr  </td><td>stringr  </td></tr>
	<tr><td>dplyr    </td><td>dplyr    </td><td>dplyr    </td><td>dplyr    </td><td>dplyr    </td><td>dplyr    </td><td>dplyr    </td></tr>
	<tr><td>purrr    </td><td>purrr    </td><td>purrr    </td><td>purrr    </td><td>purrr    </td><td>purrr    </td><td>purrr    </td></tr>
	<tr><td>readr    </td><td>readr    </td><td>readr    </td><td>readr    </td><td>readr    </td><td>readr    </td><td>readr    </td></tr>
	<tr><td>tidyr    </td><td>tidyr    </td><td>tidyr    </td><td>tidyr    </td><td>tidyr    </td><td>tidyr    </td><td>tidyr    </td></tr>
	<tr><td>tibble   </td><td>tibble   </td><td>tibble   </td><td>tibble   </td><td>tibble   </td><td>tibble   </td><td>tibble   </td></tr>
	<tr><td>ggplot2  </td><td>ggplot2  </td><td>ggplot2  </td><td>ggplot2  </td><td>ggplot2  </td><td>ggplot2  </td><td>ggplot2  </td></tr>
	<tr><td>tidyverse</td><td>tidyverse</td><td>tidyverse</td><td>tidyverse</td><td>tidyverse</td><td>tidyverse</td><td>tidyverse</td></tr>
	<tr><td>rfishbase</td><td>rfishbase</td><td>rfishbase</td><td>rfishbase</td><td>rfishbase</td><td>rfishbase</td><td>rfishbase</td></tr>
	<tr><td>rgdal    </td><td>rgdal    </td><td>rgdal    </td><td>rgdal    </td><td>rgdal    </td><td>rgdal    </td><td>rgdal    </td></tr>
	<tr><td>raster   </td><td>raster   </td><td>raster   </td><td>raster   </td><td>raster   </td><td>raster   </td><td>raster   </td></tr>
	<tr><td>sp       </td><td>sp       </td><td>sp       </td><td>sp       </td><td>sp       </td><td>sp       </td><td>sp       </td></tr>
	<tr><td>XML      </td><td>XML      </td><td>XML      </td><td>XML      </td><td>XML      </td><td>XML      </td><td>XML      </td></tr>
	<tr><td>RCurl    </td><td>RCurl    </td><td>RCurl    </td><td>RCurl    </td><td>RCurl    </td><td>RCurl    </td><td>RCurl    </td></tr>
	<tr><td>stats    </td><td>stats    </td><td>stats    </td><td>stats    </td><td>stats    </td><td>stats    </td><td>stats    </td></tr>
	<tr><td>graphics </td><td>graphics </td><td>graphics </td><td>graphics </td><td>graphics </td><td>graphics </td><td>graphics </td></tr>
	<tr><td>grDevices</td><td>grDevices</td><td>grDevices</td><td>grDevices</td><td>grDevices</td><td>grDevices</td><td>grDevices</td></tr>
	<tr><td>utils    </td><td>utils    </td><td>utils    </td><td>utils    </td><td>utils    </td><td>utils    </td><td>utils    </td></tr>
	<tr><td>datasets </td><td>datasets </td><td>datasets </td><td>datasets </td><td>datasets </td><td>datasets </td><td>datasets </td></tr>
	<tr><td>methods  </td><td>methods  </td><td>methods  </td><td>methods  </td><td>methods  </td><td>methods  </td><td>methods  </td></tr>
	<tr><td>base     </td><td>base     </td><td>base     </td><td>base     </td><td>base     </td><td>base     </td><td>base     </td></tr>
</tbody>
</table>



To get information about the packages we can type the following command in the console.


```R
help(package = "XML")
```


    Documentation for package ‘XML’
    
    
    		Information on package ‘XML’
    
    Description:
    
    Package:              XML
    Version:              3.99-0.5
    Authors@R:            c(person("CRAN Team", role = c('ctb', 'cre'),
                          email = "CRAN@r-project.org", comment = "de facto
                          maintainer since 2013"), person("Duncan", "Temple
                          Lang", role = c("aut"), email =
                          "duncan@r-project.org", comment = c(ORCID =
                          "0000-0003-0159-1546")), person("Tomas",
                          "Kalibera", role = "ctb"))
    Title:                Tools for Parsing and Generating XML Within R and
                          S-Plus
    Depends:              R (>= 4.0.0), methods, utils
    Suggests:             bitops, RCurl
    SystemRequirements:   libxml2 (>= 2.6.3)
    Description:          Many approaches for both reading and creating XML
                          (and HTML) documents (including DTDs), both local
                          and accessible via HTTP or FTP.  Also offers
                          access to an 'XPath' "interpreter".
    URL:                  http://www.omegahat.net/RSXML
    License:              BSD_3_clause + file LICENSE
    Collate:              AAA.R DTD.R DTDClasses.R DTDRef.R SAXMethods.R
                          XMLClasses.R applyDOM.R assignChild.R catalog.R
                          createNode.R dynSupports.R error.R flatTree.R
                          nodeAccessors.R parseDTD.R schema.R summary.R
                          tangle.R toString.R tree.R version.R
                          xmlErrorEnums.R xmlEventHandler.R xmlEventParse.R
                          xmlHandler.R xmlInternalSource.R xmlOutputDOM.R
                          xmlNodes.R xmlOutputBuffer.R xmlTree.R
                          xmlTreeParse.R htmlParse.R hashTree.R zzz.R
                          supports.R parser.R libxmlFeatures.R xmlString.R
                          saveXML.R namespaces.R readHTMLTable.R
                          reflection.R xmlToDataFrame.R bitList.R compare.R
                          encoding.R fixNS.R xmlRoot.R serialize.R
                          xmlMemoryMgmt.R keyValueDB.R solrDocs.R
                          XMLRErrorInfo.R xincludes.R namespaceHandlers.R
                          tangle1.R htmlLinks.R htmlLists.R
                          getDependencies.R getRelativeURL.R xmlIncludes.R
                          simplifyPath.R
    NeedsCompilation:     yes
    Packaged:             2020-07-23 13:36:18 UTC; ripley
    Author:               CRAN Team [ctb, cre] (de facto maintainer since
                          2013), Duncan Temple Lang [aut]
                          (<https://orcid.org/0000-0003-0159-1546>), Tomas
                          Kalibera [ctb]
    Maintainer:           CRAN Team <CRAN@r-project.org>
    Repository:           RSPM
    Date/Publication:     2020-07-23 13:37:06 UTC
    Encoding:             UTF-8
    Built:                R 4.0.0; x86_64-pc-linux-gnu; 2020-10-03 10:19:39
                          UTC; unix
    
    Index:
    
    [.XMLNode               Convenience accessors for the children of
                            XMLNode objects.
    [<-.XMLNode             Assign sub-nodes to an XML node
    addChildren             Add child nodes to an XML node
    addNode                 Add a node to a tree
    append.xmlNode          Add children to an XML node
    asXMLNode               Converts non-XML node objects to XMLTextNode
                            objects
    asXMLTreeNode           Convert a regular XML node to one for use in a
                            "flat" tree
    catalogLoad             Manipulate XML catalog contents
    catalogResolve          Look up an element via the XML catalog
                            mechanism
    coerce,XMLHashTreeNode,XMLHashTree-method
                            Transform between XML representations
    compareXMLDocs          Indicate differences between two XML documents
    docName                 Accessors for name of XML document
    Doctype                 Constructor for DTD reference
    Doctype-class           Class to describe a reference to an XML DTD
    dtdElement              Gets the definition of an element or entity
                            from a DTD.
    dtdElementValidEntry    Determines whether an XML element allows a
                            particular type of sub-element.
    dtdIsAttribute          Query if a name is a valid attribute of a DTD
                            element.
    dtdValidElement         Determines whether an XML tag is valid within
                            another.
    ensureNamespace         Ensure that the node has a definition for
                            particular XML namespaces
    ExternalReference-class
                            Classes for working with XML Schema
    findXInclude            Find the XInclude node associated with an XML
                            node
    free                    Release the specified object and clean up its
                            memory usage
    genericSAXHandlers      SAX generic callback handler list
    getChildrenStrings      Get the individual
    getEncoding             Determines the encoding for an XML document or
                            node
    getHTMLLinks            Get links or names of external files in HTML
                            document
    getLineNumber           Determine the location - file & line number of
                            an (internal) XML node
    getNodeSet              Find matching nodes in an internal XML tree/DOM
    getRelativeURL          Compute name of URL relative to a base URL
    getSibling              Manipulate sibling XML nodes
    getXIncludes            Find the documents that are XInclude'd in an
                            XML document
    getXMLErrors            Get XML/HTML document parse errors
    isXMLString             Facilities for working with XML strings
    length.XMLNode          Determine the number of children in an XMLNode
                            object.
    libxmlVersion           Query the version and available features of the
                            libxml library.
    makeClassTemplate       Create S4 class definition based on XML node(s)
    names.XMLNode           Get the names of an XML nodes children.
    newXMLDoc               Create internal XML node or document object
    newXMLNamespace         Add a namespace definition to an XML node
    parseDTD                Read a Document Type Definition (DTD)
    parseURI                Parse a URI string into its elements
    parseXMLAndAdd          Parse XML content and add it to a node
    print.XMLAttributeDef   Methods for displaying XML objects
    processXInclude         Perform the XInclude substitutions
    readHTMLList            Read data in an HTML list or all lists in a
                            document
    readHTMLTable           Read data from one or more HTML tables
    readKeyValueDB          Read an XML property-list style document
    readSolrDoc             Read the data from a Solr document
    removeXMLNamespaces     Remove namespace definitions from a XML node or
                            document
    replaceNodeWithChildren
                            Replace an XML node with it child nodes
    saveXML                 Output internal XML Tree
    SAXState-class          A virtual base class defining methods for SAX
                            parsing
    setXMLNamespace         Set the name space on a node
    startElement.SAX        Generic Methods for SAX callbacks
    supportsExpat           Determines which native XML parsers are being
                            used.
    toHTML                  Create an HTML representation of the given R
                            object, using internal C-level nodes
    toString.XMLNode        Creates string representation of XML node
    xmlApply                Applies a function to each of the children of
                            an XMLNode
    XMLAttributes-class     Class '"XMLAttributes"'
    xmlAttributeType        The type of an XML attribute for element from
                            the DTD
    xmlAttrs                Get the list of attributes of an XML node.
    xmlChildren             Gets the sub-nodes within an XMLNode object.
    xmlCleanNamespaces      Remove redundant namespaces on an XML document
    xmlClone                Create a copy of an internal XML document or
                            node
    XMLCodeFile-class       Simple classes for identifying an XML document
                            containing R code
    xmlContainsEntity       Checks if an entity is defined within a DTD.
    xmlDOMApply             Apply function to nodes in an XML tree/DOM.
    xmlElementsByTagName    Retrieve the children of an XML node with a
                            specific tag name
    xmlElementSummary       Frequency table of names of elements and
                            attributes in XML content
    xmlEventHandler         Default handlers for the SAX-style event XML
                            parser
    xmlEventParse           XML Event/Callback element-wise Parser
    xmlGetAttr              Get the value of an attribute in an XML node
    xmlHandler              Example XML Event Parser Handler Functions
    xmlHashTree             Constructors for trees stored as flat list of
                            nodes with information about parents and
                            children.
    XMLInternalDocument-class
                            Class to represent reference to C-level data
                            structure for an XML document
    xmlName                 Extraces the tag name of an XMLNode object.
    xmlNamespace            Retrieve the namespace value of an XML node.
    xmlNamespaceDefinitions
                            Get definitions of any namespaces defined in
                            this XML node
    xmlNode                 Create an XML node
    XMLNode-class           Classes to describe an XML node object.
    xmlOutputBuffer         XML output streams
    xmlParent               Get parent node of XMLInternalNode or ancestor
                            nodes
    xmlParseDoc             Parse an XML document with options controlling
                            the parser.
    xmlParserContextFunction
                            Identifies function as expecting an
                            xmlParserContext argument
    xmlRoot                 Get the top-level XML node.
    xmlSchemaValidate       Validate an XML document relative to an XML
                            schema
    xmlSearchNs             Find a namespace definition object by searching
                            ancestor nodes
    xmlSerializeHook        Functions that help serialize and deserialize
                            XML internal objects
    xmlSize                 The number of sub-elements within an XML node.
    xmlSource               Source the R code, examples, etc. from an XML
                            document
    xmlStopParser           Terminate an XML parser
    xmlStructuredStop       Condition/error handler functions for XML
                            parsing
    xmlToDataFrame          Extract data from a simple XML document
    xmlToList               Convert an XML node/document to a more R-like
                            list
    xmlToS4                 General mechanism for mapping an XML node to an
                            S4 object
    xmlTree                 An internal, updatable DOM object for building
                            XML trees
    xmlTreeParse            XML Parser
    xmlValue                Extract or set the contents of a leaf XML node


For data scraping in R one can also use 'rjson' to convert R objects into JSON objects and vice-versa. Another option as we will see later is to use APIs.

---

### 3.3 Reading the content of FishBase

We are now ready to extract data from https://www.fishbase.de. We want to get data about the *Coregonus lavaretus*. We can create the object x and assign the value same as the species to keep the flexibility. If we want to get data about other species we can generate a new object or can also assign new value to the object _x_ with the desired species name. 


```R
x <- "Coregonus-lavaretus"
```

Next, we use the function `paste()` to convert its arguments to character strings and concatenate them to get the link of the webpage from which we are going to extract the data. We concatenate the URL in order to get the webpage with the summary of the species *Coregonus-lavaretus*. We do not put any separation between the arguments, so we use `sep = ""`.


```R
# to keep the code 'flexible', you can see here that x is used to define the species
# this way we can easily change the species without having to always and the latin name to the code

url1 <- paste("http://www.fishbase.de/summary/", x ,".html",sep="")
```

For url1 we could also directly use http://www.fishbase.de/summary/Coregonus-lavaretus, but then we would loose flexibility if we want to look for information about other species.

<div style="background-color:rgba(0, 200, 0, 0.5);border-radius: 7px; padding:10px">
    <b>Checkpoint</b><br/>
    You can try to do the same for different species by changing the value of object x or by directly providing the address of any other species into the url. 
</div>




```R
# your code 
```


```R
### Solution

# set the object x1 with the species name
x1<- "Salvelinus alpinus"

# concatenate the url
url11 <- paste("http://www.fishbase.de/summary/",x1,".html",sep="")
```

---

Before we continue with the tutorial let's give a brief introduction to HTML (HyperText Markup Language).

**HTML** : HTML is a primary markup language for creating websites. It consists of a series of codes used to structure texts, images, and other content to be displayed in the browser. Each HTML document is made of elements that are specified using tags. HTML elements and HTML tags are often confused. The _tags_ are used to open and close the object, whereas the _element_ includes both tags and its content. 
   
Let’s consider an example with the <h1> tag: 

<h1> Title of the document </h1> - is an element, and <h1>, </h1> - are tags. 

<> symbol is used to open a tag or an element and </> is used to close it. 
HTML document has a hierarchical or tree like structure with different types of nodes as discribed in the figure. The rectangular boxes are referred as nodes. The Text node is also called as a child node of Element Node and also a leaf node as it only contains the text and no links to further nodes. Both of the Element nodes that are attached to Root Element node are called Sibling nodes of Root Element node. When we do scrapping we go through such hierarchical structure to get the content (here text). Keep in mind that every html document can vary with respect to its structure. This example is to just to provide you some knowledge about HTML document and its structure.

![alt](../data/images/node-tree.png)

---

The next step is to use the `htmlParse()` function to read the html document into an R object.
To get help on functions we can type the following command in the console, remember how you also used this for getting information on the packages.


```R
help("htmlParse")
```



<table width="100%" summary="page for xmlTreeParse {XML}"><tr><td>xmlTreeParse {XML}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>XML Parser</h2>

<h3>Description</h3>

<p>Parses an XML or HTML file or string containing XML/HTML content, and generates an R 
structure representing the XML/HTML tree.  Use <code>htmlTreeParse</code> when the content is known
to be (potentially malformed) HTML.
This function has numerous parameters/options and operates quite differently
based on  their values.
It can create trees in R or using internal C-level nodes, both of
which are useful in different contexts.
It can perform conversion of the nodes into R objects using
caller-specified  handler functions and this can be used to 
map the XML document directly into R data structures,
by-passing the conversion to an R-level tree which would then
be processed recursively or with multiple descents to extract the
information of interest.
</p>
<p><code>xmlParse</code> and <code>htmlParse</code> are equivalent to the
<code>xmlTreeParse</code> and <code>htmlTreeParse</code> respectively,
except they both use a default value for the <code>useInternalNodes</code> parameter 
of <code>TRUE</code>, i.e. they working with and return internal
nodes/C-level nodes.  These can then be searched using
XPath expressions via <code>xpathApply</code> and 
<code>getNodeSet</code>.
</p>
<p><code>xmlSchemaParse</code> is a convenience function for parsing an XML schema.
</p>


<h3>Usage</h3>

<pre>
xmlTreeParse(file, ignoreBlanks=TRUE, handlers=NULL, replaceEntities=FALSE,
             asText=FALSE, trim=TRUE, validate=FALSE, getDTD=TRUE,
             isURL=FALSE, asTree = FALSE, addAttributeNamespaces = FALSE,
             useInternalNodes = FALSE, isSchema = FALSE,
             fullNamespaceInfo = FALSE, encoding = character(),
             useDotNames = length(grep("^\\.", names(handlers))) &gt; 0,
             xinclude = TRUE, addFinalizer = TRUE, error = xmlErrorCumulator(),
             isHTML = FALSE, options = integer(), parentFirst = FALSE)

xmlInternalTreeParse(file, ignoreBlanks=TRUE, handlers=NULL, replaceEntities=FALSE,
             asText=FALSE, trim=TRUE, validate=FALSE, getDTD=TRUE,
             isURL=FALSE, asTree = FALSE, addAttributeNamespaces = FALSE,
             useInternalNodes = TRUE, isSchema = FALSE,
             fullNamespaceInfo = FALSE, encoding = character(),
             useDotNames = length(grep("^\\.", names(handlers))) &gt; 0,
             xinclude = TRUE, addFinalizer = TRUE, error = xmlErrorCumulator(),
             isHTML = FALSE, options = integer(), parentFirst = FALSE)

xmlNativeTreeParse(file, ignoreBlanks=TRUE, handlers=NULL, replaceEntities=FALSE,
             asText=FALSE, trim=TRUE, validate=FALSE, getDTD=TRUE,
             isURL=FALSE, asTree = FALSE, addAttributeNamespaces = FALSE,
             useInternalNodes = TRUE, isSchema = FALSE,
             fullNamespaceInfo = FALSE, encoding = character(),
             useDotNames = length(grep("^\\.", names(handlers))) &gt; 0,
             xinclude = TRUE, addFinalizer = TRUE, error = xmlErrorCumulator(),
             isHTML = FALSE, options = integer(), parentFirst = FALSE)


htmlTreeParse(file, ignoreBlanks=TRUE, handlers=NULL, replaceEntities=FALSE,
             asText=FALSE, trim=TRUE, validate=FALSE, getDTD=TRUE,
             isURL=FALSE, asTree = FALSE, addAttributeNamespaces = FALSE,
             useInternalNodes = FALSE, isSchema = FALSE,
             fullNamespaceInfo = FALSE, encoding = character(),
             useDotNames = length(grep("^\\.", names(handlers))) &gt; 0,
             xinclude = TRUE, addFinalizer = TRUE, error = htmlErrorHandler,
             isHTML = TRUE, options = integer(), parentFirst = FALSE)

htmlParse(file, ignoreBlanks = TRUE, handlers = NULL, replaceEntities = FALSE, 
          asText = FALSE, trim = TRUE, validate = FALSE, getDTD = TRUE, 
           isURL = FALSE, asTree = FALSE, addAttributeNamespaces = FALSE, 
            useInternalNodes = TRUE, isSchema = FALSE, fullNamespaceInfo = FALSE, 
             encoding = character(), 
             useDotNames = length(grep("^\\.", names(handlers))) &gt; 0, 
              xinclude = TRUE, addFinalizer = TRUE, 
               error = htmlErrorHandler, isHTML = TRUE,
                options = integer(), parentFirst = FALSE) 

xmlSchemaParse(file, asText = FALSE, xinclude = TRUE, error = xmlErrorCumulator())
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>file</code></td>
<td>
<p> The name of the file containing the XML contents.
This can contain \~ which is expanded to the user's
home directory.
It can also be a URL. See <code>isURL</code>.
Additionally, the file can be compressed (gzip)
and is read directly without the user having
to de-compress (gunzip) it.</p>
</td></tr>
<tr valign="top"><td><code>ignoreBlanks</code></td>
<td>
<p> logical value indicating whether
text elements made up entirely of white space should be included
in the resulting &lsquo;tree&rsquo;. </p>
</td></tr>
<tr valign="top"><td><code>handlers</code></td>
<td>
<p>Optional collection of functions
used to map the different XML nodes to R
objects. Typically, this is a named list of functions,
and a closure can be used to provide local data.
This provides a way of filtering the tree as it is being
created in R, adding or removing nodes, and generally processing
them as they are constructed in the C code.
</p>
<p>In a recent addition to the package (version 0.99-8),
if this is specified as a single function object,
we call that function for each node (of any type) in the underlying DOM tree.
It is invoked with the new node and its parent node.
This applies to regular nodes and also comments, processing
instructions, CDATA nodes, etc.  So this function must be
sufficiently general to handle them all.
</p>
</td></tr>
<tr valign="top"><td><code>replaceEntities</code></td>
<td>

<p>logical value indicating whether to substitute entity references
with their text directly. This should be left as False.
The text still appears as the value of the node, but there
is more information about its source, allowing the parse to be reversed
with full reference information.
</p>
</td></tr>
<tr valign="top"><td><code>asText</code></td>
<td>
<p>logical value indicating that the first argument,
&lsquo;file&rsquo;, 
should be treated as the XML text to parse, not the name of 
a file. This allows the contents of documents to be retrieved 
from different sources (e.g. HTTP servers, XML-RPC, etc.) and still
use this parser.</p>
</td></tr>
<tr valign="top"><td><code>trim</code></td>
<td>

<p>whether to strip white space from the beginning and end of text strings.
</p>
</td></tr>
<tr valign="top"><td><code>validate</code></td>
<td>

<p>logical indicating whether to use a validating parser or not, or in other words
check the contents against the DTD specification. If this is true, warning
messages will be displayed about errors in the DTD and/or document, but the parsing 
will proceed except for the presence of terminal errors.
This is ignored when parsing an HTML document.
</p>
</td></tr>
<tr valign="top"><td><code>getDTD</code></td>
<td>

<p>logical flag indicating whether the DTD (both internal and external)
should be returned along with the document nodes. This changes the 
return type.
This is ignored when parsing an HTML document.
</p>
</td></tr>
<tr valign="top"><td><code>isURL</code></td>
<td>

<p>indicates whether the <code>file</code>  argument refers to a URL
(accessible via ftp or http) or a regular file on the system.
If <code>asText</code> is TRUE, this should not be specified.
The function attempts to determine whether the 
data source is a URL by using <code>grep</code>
to look for http or ftp at the start of the string.
The libxml parser handles the connection to servers,
not the R facilities (e.g. <code>scan</code>).
</p>
</td></tr>
<tr valign="top"><td><code>asTree</code></td>
<td>
<p>this only applies when on passes a value for
the  <code>handlers</code> argument and is used then to determine
whether the DOM tree should be returned or the <code>handlers</code>
object.
</p>
</td></tr>
<tr valign="top"><td><code>addAttributeNamespaces</code></td>
<td>
<p>a logical value indicating whether to
return the namespace in the names of the attributes within a node
or to omit them. If this is <code>TRUE</code>, an attribute such as
<code>xsi:type="xsd:string"</code> is reported with the name
<code>xsi:type</code>.
If it is <code>FALSE</code>, the name of the attribute is <code>type</code>.</p>
</td></tr>
<tr valign="top"><td><code>useInternalNodes</code></td>
<td>
<p>a logical value indicating whether 
to call the converter functions with objects of class
<code>XMLInternalNode</code> rather than <code>XMLNode</code>.
This should make things faster as we do not convert  the 
contents of the internal nodes to R explicit objects.
Also, it allows one to access the parent and ancestor nodes.
However, since the objects refer to volatile C-level objects,
one cannot store these nodes for use in further computations within R.
They &ldquo;disappear&rdquo; after the processing the XML document is completed.
</p>
<p>If this argument is <code>TRUE</code> and no handlers are provided, the
return value is a reference to the internal C-level document pointer.
This can be used to do post-processing via XPath expressions using
<code>getNodeSet</code>.
</p>
<p>This is ignored when parsing an HTML document.
</p>
</td></tr>
<tr valign="top"><td><code>isSchema</code></td>
<td>
<p>a logical value indicating whether the document
is an XML schema (<code>TRUE</code>) and should be parsed as such using
the built-in schema parser in libxml.</p>
</td></tr>
<tr valign="top"><td><code>fullNamespaceInfo</code></td>
<td>
<p>a logical value indicating whether
to provide the namespace URI and prefix on each node
or just the prefix.  The latter (<code>FALSE</code>) is
currently the default as that was the original way the
package behaved.   However, using
<code>TRUE</code> is more informative and we will make this
the default in the future.
</p>
<p>This is ignored when parsing an HTML document.
</p>
</td></tr>
<tr valign="top"><td><code>encoding</code></td>
<td>
<p> a character string (scalar) giving the encoding for the
document.  This is optional as the document should contain its own
encoding information. However, if it doesn't, the caller can specify
this for the parser.  If the XML/HTML document does specify its own
encoding that value is used regardless of any value specified by the
caller. (That's just the way it goes!) So this is to be used
as a safety net in case the document does not have an encoding and
the caller happens to know theactual encoding.
</p>
</td></tr>
<tr valign="top"><td><code>useDotNames</code></td>
<td>
<p>a logical value
indicating whether to use the
newer format for identifying general element function handlers
with the '.' prefix, e.g. .text, .comment, .startElement.
If this is <code>FALSE</code>, then the older format
text, comment, startElement, ...
are used. This causes problems when there are indeed nodes
named text or comment or startElement as a
node-specific handler are confused with the corresponding
general handler of the same name. Using <code>TRUE</code>
means that your list of handlers should have names that use
the '.' prefix for these general element handlers.
This is the preferred way to write new code.
</p>
</td></tr>
<tr valign="top"><td><code>xinclude</code></td>
<td>
<p>a logical value indicating whether
to process nodes of the form <code>&lt;xi:include xmlns:xi="http://www.w3.org/2001/XInclude"&gt;</code>
to insert content from other parts of (potentially different)
documents. <code>TRUE</code> means resolve the external references;
<code>FALSE</code> means leave the node as is.
Of course, one can process these nodes oneself after document has
been parse using handler functions or working on the DOM.
Please note that the syntax for inclusion using XPointer
is not the same as XPath and the results can be a little
unexpected and confusing. See the libxml2 documentation for more details.
</p>
</td></tr>
<tr valign="top"><td><code>addFinalizer</code></td>
<td>
<p>a logical value indicating whether the
default finalizer routine should be registered to
free the internal xmlDoc when R no longer has a reference to this
external pointer object. This is only relevant when
<code>useInternalNodes</code> is <code>TRUE</code>.
</p>
</td></tr>
<tr valign="top"><td><code>error</code></td>
<td>
<p>a function that is invoked when the XML parser reports
an error.
When an error is encountered, this is called with 7 arguments.
See <code>xmlStructuredStop</code> for information about these
</p>
<p>If parsing completes and no document is generated, this function is
called again with only argument which is a character vector of
length 0.  This gives the function an opportunity to report all the 
errors and raise an exception rather than doing this when it sees
th first one.
</p>
<p>This function can do what it likes with the information.
It can raise an R error or let parser continue and potentially
find further errors.
</p>
<p>The default value of this argument supplies a function that 
cumulates the errors
</p>
<p>If this is <code>NULL</code>, the default error handler function in the
package  <code>xmlStructuredStop</code> is invoked and this will 
raise an error in R at that time in R.
</p>
</td></tr>
<tr valign="top"><td><code>isHTML</code></td>
<td>
<p>a logical value that allows this function to be used for parsing HTML documents.
This causes validation and processing of a DTD to be turned off.
This is currently experimental so that we can implement
<code>htmlParse</code> with this same function.</p>
</td></tr>
<tr valign="top"><td><code>options</code></td>
<td>
<p>an integer value or vector of values that are combined
(OR'ed) together
to specify options for the XML parser. This is the same as the
<code>options</code> parameter for <code>xmlParseDoc</code>.
</p>
</td></tr>
<tr valign="top"><td><code>parentFirst</code></td>
<td>
<p>a logical value for use when we have handler
functions and are traversing the tree.
This controls whether we process
the node before processing its children, or process the children
before their parent node.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>handlers</code> argument is used similarly
to those specified in xmlEventParse.
When an XML tag (element) is processed,
we look for a function in this collection 
with the same name as the tag's name. 
If this is not found, we look for one named
<code>startElement</code>. If this is not found, we use the default
built in converter.
The same works for comments, entity references, cdata, processing instructions,
etc.
The default entries should be named
<code>comment</code>, <code>startElement</code>,
<code>externalEntity</code>,
<code>processingInstruction</code>,
<code>text</code>, <code>cdata</code> and <code>namespace</code>.
All but the last should take the XMLnode as their first argument.
In the future, other information may be passed via ...,
for example, the depth in the tree, etc.
Specifically, the second argument will be the parent node into which they
are being added, but this is not currently implemented,
so should have a default value (<code>NULL</code>).
</p>
<p>The <code>namespace</code> function is called with a single argument which
is an object of class <code>XMLNameSpace</code>.  This contains
</p>

<dl>
<dt>id</dt><dd><p>the namespace identifier as used to
qualify tag names;</p>
</dd> 
<dt>uri</dt><dd><p>the value of the namespace identifier,
i.e. the URI
identifying the namespace.</p>
</dd>
<dt>local</dt><dd><p>a logical value indicating whether the definition
is local to the document being parsed.</p>
</dd>
</dl>

<p>One should note that the <code>namespace</code> handler is called before the
node in which the namespace definition occurs and its children are
processed.  This is different than the other handlers which are called
after the child nodes have been processed.
</p>
<p>Each of these functions can return arbitrary values that are then
entered into the tree in place of the default node passed to the
function as the first argument.  This allows the caller to generate
the nodes of the resulting document tree exactly as they wish.  If the
function returns <code>NULL</code>, the node is dropped from the resulting
tree. This is a convenient way to discard nodes having processed their
contents.
</p>


<h3>Value</h3>

<p>By default ( when <code>useInternalNodes</code> is <code>FALSE</code>, 
<code>getDTD</code> is <code>TRUE</code>,  and no
handler functions are provided), the return value is, an object of
(S3) class <code>XMLDocument</code>.
This has two fields named <code>doc</code> and <code>dtd</code>
and are of class <code>DTDList</code> and <code>XMLDocumentContent</code> respectively.
</p>
<p>If <code>getDTD</code> is <code>FALSE</code>,  only the <code>doc</code> object is returned.
</p>
<p>The <code>doc</code> object has three fields of its own:
<code>file</code>, <code>version</code> and <code>children</code>.
</p>
<table summary="R valueblock">
<tr valign="top"><td><code><code>file</code></code></td>
<td>
<p>The (expanded) name of the file  containing the XML.</p>
</td></tr>
<tr valign="top"><td><code><code>version</code></code></td>
<td>
<p>A string identifying the  version of XML used by the document.</p>
</td></tr>
<tr valign="top"><td><code><code>children</code></code></td>
<td>

<p>A list of the XML nodes at the top of the document.
Each of these is of class <code>XMLNode</code>.
These are made up of 4 fields.
</p>

<ul>
<li><p><code>name</code>The name of the element.
</p>
</li>
<li><p><code>attributes</code>For regular elements, a named list
of XML attributes converted from the 
&lt;tag x=&quot;1&quot; y=&quot;abc&quot;&gt;
</p>
</li>
<li><p><code>children</code>List of sub-nodes.
</p>
</li>
<li><p><code>value</code>Used only for text entries.
</p>
</li></ul>

<p>Some nodes specializations of <code>XMLNode</code>, such as 
<code>XMLComment</code>, <code>XMLProcessingInstruction</code>,
<code>XMLEntityRef</code> are used.
</p>
<p>If the value of the argument getDTD is TRUE and the document refers
to a DTD via a top-level DOCTYPE element, the DTD and its information
will be available in the <code>dtd</code> field.  The second element is a
list containing the external and internal DTDs. Each of these
contains 2 lists - one for element definitions and another for entities. See
<code>parseDTD</code>. 
</p>
<p>If a list of functions is given via <code>handlers</code>, 
this list is returned. Typically, these handler functions
share state via a closure and the resulting updated data structures
which contain the extracted and processed values from the XML
document can be retrieved via a function in this handler list.
</p>
<p>If <code>asTree</code> is <code>TRUE</code>, then the converted tree is returned.
What form this takes depends on what the handler functions have
done to process the XML tree.
</p>
<p>If <code>useInternalNodes</code> is <code>TRUE</code> and no handlers are
specified, an object of S3 class <code>XMLInternalDocument</code> is
returned. This can be used in much the same ways as an
<code>XMLDocument</code>, e.g. with <code>xmlRoot</code>,
<code>docName</code> and so on to traverse the tree.
It can also be used with XPath queries via <code>getNodeSet</code>,
<code>xpathApply</code> and <code>doc["xpath-expression"]</code>.
</p>
<p>If internal nodes are used and the internal tree returned directly,
all the nodes are returned as-is and no attempt to 
trim white space, remove &ldquo;empty&rdquo; nodes (i.e. containing only white
space), etc. is done. This is potentially quite expensive and so is
not done generally, but should  be done during the processing
of the nodes.  When using XPath queries, such nodes are easily
identified and/or ignored and so do not cause any difficulties.
They do become an issue when dealing with a node's chidren
directly and so one can use simple filtering techniques such as
<code> xmlChildren(node)[ ! xmlSApply(node, inherits,  "XMLInternalTextNode")]</code>
and even check the <code>xmlValue</code> to determine if it contains only
white space.
<code> xmlChildren(node)[ ! xmlSApply(node, function(x) inherit(x,
              "XMLInternalTextNode")] &amp;&amp; trim(xmlValue(x)) == "")</code>
</p>
</td></tr> </table>


<h3>Note</h3>

<p>Make sure  that the necessary 3rd party libraries are available.</p>


<h3>Author(s)</h3>

<p>Duncan Temple Lang &lt;duncan@wald.ucdavis.edu&gt;</p>


<h3>References</h3>

<p><a href="http://xmlsoft.org">http://xmlsoft.org</a>, <a href="http://www.w3.org/xml">http://www.w3.org/xml</a></p>


<h3>See Also</h3>

 <p>xmlEventParse,
<code>free</code> for releasing the memory when
an <code>XMLInternalDocument</code> object is returned.
</p>


<h3>Examples</h3>

<pre>
 fileName &lt;- system.file("exampleData", "test.xml", package="XML")
   # parse the document and return it in its standard format.

 xmlTreeParse(fileName)

   # parse the document, discarding comments.

 xmlTreeParse(fileName, handlers=list("comment"=function(x,...){NULL}), asTree = TRUE)

   # print the entities
 invisible(xmlTreeParse(fileName,
            handlers=list(entity=function(x) {
                                    cat("In entity",x$name, x$value,"\n")
                                    x}
                                  ), asTree = TRUE
                          )
          )

 # Parse some XML text.
 # Read the text from the file
 xmlText &lt;- paste(readLines(fileName), "\n", collapse="")

 print(xmlText)
 xmlTreeParse(xmlText, asText=TRUE)


    # with version 1.4.2 we can pass the contents of an XML
    # stream without pasting them.
 xmlTreeParse(readLines(fileName), asText=TRUE)


 # Read a MathML document and convert each node
 # so that the primary class is 
 #   &lt;name of tag&gt;MathML
 # so that we can use method  dispatching when processing
 # it rather than conditional statements on the tag name.
 # See plotMathML() in examples/.
 fileName &lt;- system.file("exampleData", "mathml.xml",package="XML")
m &lt;- xmlTreeParse(fileName, 
                  handlers=list(
                   startElement = function(node){
                   cname &lt;- paste(xmlName(node),"MathML", sep="",collapse="")
                   class(node) &lt;- c(cname, class(node)); 
                   node
                }))



  # In this example, we extract _just_ the names of the
  # variables in the mtcars.xml file. 
  # The names are the contents of the &lt;variable&gt;
  # tags. We discard all other tags by returning NULL
  # from the startElement handler.
  #
  # We cumulate the names of variables in a character
  # vector named `vars'.
  # We define this within a closure and define the 
  # variable function within that closure so that it
  # will be invoked when the parser encounters a &lt;variable&gt;
  # tag.
  # This is called with 2 arguments: the XMLNode object (containing
  # its children) and the list of attributes.
  # We get the variable name via call to xmlValue().

  # Note that we define the closure function in the call and then 
  # create an instance of it by calling it directly as
  #   (function() {...})()

  # Note that we can get the names by parsing
  # in the usual manner and the entire document and then executing
  # xmlSApply(xmlRoot(doc)[[1]], function(x) xmlValue(x[[1]]))
  # which is simpler but is more costly in terms of memory.
 fileName &lt;- system.file("exampleData", "mtcars.xml", package="XML")
 doc &lt;- xmlTreeParse(fileName,  handlers = (function() { 
                                 vars &lt;- character(0) ;
                                list(variable=function(x, attrs) { 
                                                vars &lt;&lt;- c(vars, xmlValue(x[[1]])); 
                                                NULL}, 
                                     startElement=function(x,attr){
                                                   NULL
                                                  }, 
                                     names = function() {
                                                 vars
                                             }
                                    )
                               })()
                     )

  # Here we just print the variable names to the console
  # with a special handler.
 doc &lt;- xmlTreeParse(fileName, handlers = list(
                                  variable=function(x, attrs) {
                                             print(xmlValue(x[[1]])); TRUE
                                           }), asTree=TRUE)


  # This should raise an error.
  try(xmlTreeParse(
            system.file("exampleData", "TestInvalid.xml", package="XML"),
            validate=TRUE))

## Not run: 
 # Parse an XML document directly from a URL.
 # Requires Internet access.
 xmlTreeParse("http://www.omegahat.net/Scripts/Data/mtcars.xml", asText=TRUE)

## End(Not run)

  counter = function() {
              counts = integer(0)
              list(startElement = function(node) {
                                     name = xmlName(node)
                                     if(name %in% names(counts))
                                          counts[name] &lt;&lt;- counts[name] + 1
                                     else
                                          counts[name] &lt;&lt;- 1
                                  },
                    counts = function() counts)
            }

   h = counter()
   xmlParse(system.file("exampleData", "mtcars.xml", package="XML"),  handlers = h)
   h$counts()



 f = system.file("examples", "index.html", package = "XML")
 htmlTreeParse(readLines(f), asText = TRUE)
 htmlTreeParse(readLines(f))

  # Same as 
 htmlTreeParse(paste(readLines(f), collapse = "\n"), asText = TRUE)


 getLinks = function() { 
       links = character() 
       list(a = function(node, ...) { 
                   links &lt;&lt;- c(links, xmlGetAttr(node, "href"))
                   node 
                }, 
            links = function()links)
     }

 h1 = getLinks()
 htmlTreeParse(system.file("examples", "index.html", package = "XML"),
               handlers = h1)
 h1$links()

 h2 = getLinks()
 htmlTreeParse(system.file("examples", "index.html", package = "XML"),
              handlers = h2, useInternalNodes = TRUE)
 all(h1$links() == h2$links())

  # Using flat trees
 tt = xmlHashTree()
 f = system.file("exampleData", "mtcars.xml", package="XML")
 xmlTreeParse(f, handlers = list(.startElement = tt[[".addNode"]]))
 xmlRoot(tt)



 doc = xmlTreeParse(f, useInternalNodes = TRUE)

 sapply(getNodeSet(doc, "//variable"), xmlValue)

 #free(doc) 


  # character set encoding for HTML
 f = system.file("exampleData", "9003.html", package = "XML")
   # we specify the encoding
 d = htmlTreeParse(f, encoding = "UTF-8")
   # get a different result if we do not specify any encoding
 d.no = htmlTreeParse(f)
   # document with its encoding in the HEAD of the document.
 d.self = htmlTreeParse(system.file("exampleData", "9003-en.html",package = "XML"))
   # XXX want to do a test here to see the similarities between d and
   # d.self and differences between d.no


  # include
 f = system.file("exampleData", "nodes1.xml", package = "XML")
 xmlRoot(xmlTreeParse(f, xinclude = FALSE))
 xmlRoot(xmlTreeParse(f, xinclude = TRUE))

 f = system.file("exampleData", "nodes2.xml", package = "XML")
 xmlRoot(xmlTreeParse(f, xinclude = TRUE))

  # Errors
  try(xmlTreeParse("&lt;doc&gt;&lt;a&gt; &amp; &lt; &lt;?pi &gt; &lt;/doc&gt;"))

    # catch the error by type.
 tryCatch(xmlTreeParse("&lt;doc&gt;&lt;a&gt; &amp; &lt; &lt;?pi &gt; &lt;/doc&gt;"),
                "XMLParserErrorList" = function(e) {
                     cat("Errors in XML document\n", e$message, "\n")
                                                    })

    #  terminate on first error            
  try(xmlTreeParse("&lt;doc&gt;&lt;a&gt; &amp; &lt; &lt;?pi &gt; &lt;/doc&gt;", error = NULL))

    #  see xmlErrorCumulator in the XML package 


  f = system.file("exampleData", "book.xml", package = "XML")
  doc.trim = xmlInternalTreeParse(f, trim = TRUE)
  doc = xmlInternalTreeParse(f, trim = FALSE)
  xmlSApply(xmlRoot(doc.trim), class)
      # note the additional XMLInternalTextNode objects
  xmlSApply(xmlRoot(doc), class)


  top = xmlRoot(doc)
  textNodes = xmlSApply(top, inherits, "XMLInternalTextNode")
  sapply(xmlChildren(top)[textNodes], xmlValue)


     # Storing nodes
   f = system.file("exampleData", "book.xml", package = "XML")
   titles = list()
   xmlTreeParse(f, handlers = list(title = function(x)
                                  titles[[length(titles) + 1]] &lt;&lt;- x))
   sapply(titles, xmlValue)
   rm(titles)
</pre>

<hr /><div style="text-align: center;">[Package <em>XML</em> version 3.99-0.5 ]</div>


We can now use the function `getURLContent()` to retrieve the source of a webpage, which is especially useful for retrieving pages for data processing (i.e. scraping). We will apply the function `htmlParse()` to obtain an R object.


```R
fishbase <- htmlParse(getURLContent(url1, followlocation=TRUE)) 
```

Below we can see a print screen of part of the object _fishbase_. ![alt](../data/images/fishbase.png) 


In the next figure we can see another extract. 
This printscreen of part of the object fishbase is related to the maximal length.![alt](../data/images/fishbase_maxle.png)

Highlighted in <font color=red> red </font> we can see the opening and closing tags in the HTML extract, these tag block are used to identify points of interest in this example.
So we need to extract the <b>span/div</b>  blocks.

In order to extract the information inside these two blocks, we are going to use the function `getNodeSet()` to find XML nodes that match a particular criterion. <b>Span</b> is used for a small chunk of HTML inside a line and **div** to group larger chunks of code. This will make finding and extracting information easier.



```R
fishbase_div <-getNodeSet(fishbase, "//div ") 
fishbase_span <- getNodeSet(fishbase, "//span ") 
```

In the next two figures we can see the differences after executing the command `getNodeSet(fishbase, "//div ")`. A similar result is obtained for `getNodeSet(fishbase, "//span ")`. 



Extract before the command `getNodeSet()`.
![](../data/images/before.png)

Extract after the command `getNodeSet()`.
![](../data/images/after.png)


Hence, `getNodeSet()` will identify all the sections for a given tag and separate them out into a list.

---

### 3.4 Getting a numerical value from FishBase

We now want to get the maximal length of the *Coregonus lavaretus*. Some types of XML nodes have no children nodes, but are leaf nodes and simply contain text. So the function `xmlValue()` provides access to their raw contents. 

Now let us briefly discuss _regular expressions_. A _regular expression_ is a pattern that describes a set of strings. So we will use `regexec()` to search for matches to argument pattern within each element of a character vector. In this case, we are going to look for the pattern "Max length", because, after this pattern, we can read its value as shown in the next figure.

![Maximal length.](../data/images/length.png)


```R
fish_length <- xmlValue(fishbase_span[[which.max(sapply(lapply(fishbase_span,xmlValue) 
, function(x){regexec(pattern="Max length", x)[[1]][1]}))]])
```


```R
fish_length
```


'\r\n\t\t\t\t\tMaturity: Lm27.1, range 40 - ? cm Max length : 73.0 cm TL male/unsexed; (Ref. 40637); max. published weight: 10.0 kg (Ref. 35388)\t\t\t\t'


We then use the function `substr()`, which you already encountered in previous chapters, to extract a part (the length we are looking for) of the character vector. We need to find the interval (section) we want to extract. With `regexec()` we are looking for the pattern _Max length_. To get the fish length we will extract the letters from position 13 (which is a 7) to position 16 (which is a 0). You can also count these positions from the image above starting from 'M' in Max length. Keep in mind that every blank space is also counted. Another way to do that is based on a trial and error basis. 


```R
max_length <- substr(fish_length, regexec(pattern="Max length", fish_length)[[1]][1]+13
, regexec(pattern="Max length", fish_length)[[1]][1]+16)
max_length
```


'73.0'


Now we should convert the value of the maximal length (which is a character) into a number with the function `as.double()`. We convert it into a double format in order to simplify things and make it machine readable. For example, if an integer value is followed by any character let's say 'R', we would be able to read that value without an error if it is in double format. It increases the flexibility in data reading.


```R
max_length <- as.double(max_length, base = 0L)
max_length
```


73


We see the maximal length of the *Coregonus lavaretus* is 73 cm. One can now start to assemble/collect those numbers into "suitable" data containers or formats. We will put all the data into a dataframe in the later sections.

---

<div style="border: 2px black solid; border-radius: 7px; padding:10px">
    <b>IUCN Red List</b><br/>
    
Before we go on we would like to introduce the <b>IUCN</b> status of species. IUCN (International Union for Conservation of Nature’s) Red List of Threatened Species, also called <b>IUCN Red List</b> has evolved to become the world’s most comprehensive information source on the global extinction risk status of animal, fungus and plant species.
 
The IUCN Red List is a critical indicator of the health of the world’s biodiversity. Far more than a list of species and their status, it is a powerful tool to inform and catalyse action for biodiversity conservation and policy change, critical to protecting the natural resources we need to survive. It provides information about the range, population size, habitat and ecology, use and/or trade, threats, and conservation actions that will help inform necessary conservation decisions.

IUCN Red List is one of the most well-known objective assessment systems for classifying the status of plants, animals, and other organisms threatened with extinction. IUCN unveiled this assessment system in 1994. It contains explicit criteria and categories to classify the conservation status of individual species on the basis of their probability of extinction. After a species is evaluated by the IUCN, it is placed into one of eight categories based on its current conservation status as shown in the figure.

![IUCN status.](../data/images/iucn.png)
</div>

---

### 3.5 Getting a block of text value from FishBase

In this part, we are going to get the IUCN Status of *Coregonus lavaretus*. We are going to use the function `which()` to find the position of the elements we are looking for. We use `regexec()` to search for matches to the argument pattern within each element of a character vector. In this case, the pattern is IUCN. The pattern can be found, as for the maximal length, on the webpage from which we are getting the data.


```R
w_IUCN  <-which(sapply(lapply(fishbase_div,xmlValue),function(x)
  {regexec(pattern="IUCN", x)[[1]][1]})>0)
```

Now if w_IUCN is empty, we set it as NA. Otherwise we use the function `xmlValue()` to get the value at the node.


```R
if(length(w_IUCN)==0){ 
  IUCN_status="NA"
} else {
  d1_IUCN  <- xmlValue(fishbase_div[[w_IUCN[length(w_IUCN)]]])
} 
```

In the next  we can see the output of *d1_IUCN*.


```R
d1_IUCN 
```


<span style=white-space:pre-wrap>'\r\n\t\t\t\t\r\n\t\t\t\t\tIUCN Red List Status   (Ref. 120744)\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\t  Vulnerable (VU) (D2); Date assessed: 01 January 2008\t\t\t\t\r\n\t\t\t\t'</span>


This all looks a little confusing and we only really need a tiny part, namely _VU_, of the above information. So we are going to use `unlist()` to produce a vector which contains all the atomic components that occurs in the pattern (the pattern `[[:alpha:]]+` is used to get the alphabetic characters) and `regmatches()` is used to extract or replace matched substrings from  data obtained by `gregexpr()`. The function `gregexpr()`does the same thing as `regexec()`, except that its returned object is a list rather than a vector.


```R
IUCN <- unlist(regmatches(d1_IUCN,gregexpr(pattern= "[[:alpha:]]+)",   d1_IUCN)))
IUCN
```


'VU)'


As you see to get to IUCN Status we need to remove the _")"_. So we will use the function `str_replace()`. The `pattern="[[:punct:]]"` is used to remove punctuation.


```R
IUCN_status <- str_replace(IUCN,pattern="[[:punct:]]","" )
IUCN_status
```


'VU'


Hence the IUCN Status is _VU_ (vulnerable) which is saved as a character. In this case, we will keep it as a character. 

---

### 3.6 Reading a table from a website

The next step is to read a table from a website. We are going to get information about the eggs of the *Coregonus lavaretus*. 
We will use the function `getHTMLLinks()` to retrieve either the links within an HTML document or the collection of names of external files referenced in an HTML document.


```R
link_list <- getHTMLLinks(fishbase, externalOnly = TRUE, xpQuery = "//a/@href"
, baseURL = docName(fishbase))
```

In the next figure we can see an extract of _link_list_.

![link_list.](../data/images/eggs.png)

As highlighted, we need to look for the pattern _FishEggInfoSummary_ and to do that we use the function `grep()` (similar to what we did before with `gregexpr()`).


```R
eggs_link <- link_list[grep("FishEggInfoSummary",link_list)]
```

![](../data/images/outputlink.png)

The two extracts above look like they are the same object. To check if two objects are identical we use the logic operator `==`.


```R
eggs_link[1]==eggs_link[2]
```


TRUE


Since, the output is TRUE. This indicates that the two objects are identical, now we can arbitrarily select the first or second object.


```R
eggs_link <-eggs_link[1]
```

<div style="background-color:rgba(0, 200, 0, 0.5);border-radius: 7px; padding:10px">
    <b>Checkpoint</b><br/>
    Call the variable <i>egg_link</i>, you will notice the link begins with '..' (see above in the image). Your task is to remove these dots and so the link begins with '/Reproduction/.. '. 
    
_Note:_ make sure you run the code you will need it in the subsequent code. 
    
_Hint:_ make use of function 'str_replace()'.
    
</div>


```R
# your code
```


```R
### Solution

# replace '..' or here effectively remove '..'
eggs_link <- str_replace(eggs_link, "..", "" )
eggs_link

```


'/Reproduction/FishEggInfoSummary.php?ID=232&amp;GenusName=Coregonus&amp;SpeciesName=lavaretus&amp;fc=76&amp;StockCode=246'


Similarly to what we did previously, we can get the content of the webpage with the function `getURLContent()`.


```R
url_egg <- paste ("http://www.fishbase.org/",eggs_link,sep="")
egg_content <- getURLContent(url_egg, followlocation=TRUE, .encoding="CE_UTF8")
```

In the next figure we can see an extract of _egg_content_.

![Extract of egg_content.](../data/images/extract_tt.png)

We will use `readHTMLTable()` to read in the table in the document.


```R
egg_table <- readHTMLTable(egg_content,header=TRUE,colClasses=NULL,skip.rows=integer(),
                       stringsAsFactors=FALSE,trim=TRUE,elFun=xmlValue,
                       as.data.frame=TRUE,which=integer())[[1]]
egg_table
```


<table>
<caption>A data.frame: 7 × 2</caption>
<thead>
	<tr><th scope=col>Main Ref.</th><th scope=col></th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>Place of Development </td><td>      </td></tr>
	<tr><td>Shape of Egg         </td><td>      </td></tr>
	<tr><td>Attributes           </td><td>      </td></tr>
	<tr><td>Color of Eggs        </td><td>      </td></tr>
	<tr><td>Color of Oil Globule </td><td>      </td></tr>
	<tr><td>Additional Characters</td><td>      </td></tr>
	<tr><td>Get Information on   </td><td>Scirus</td></tr>
</tbody>
</table>



Now we can extract information from the table. By using the function `which()` to find the information we are looking for. As we are looking for the shape of the egg, we will pass this value.


```R
egg_shape = egg_table[which(egg_table[,1] == "Shape of Egg"),2] # Shape of Egg

egg_shape
```


''


In case there is no information about a feature (hence the character is empty). We set the feature as NA.


```R
if(egg_shape == "") {egg_shape = "NA"}
```

By calling the variable _egg_shape_ again we see NA as output which indicates that the field that provides the data about the shape of an egg is empty.


```R
egg_shape
```


'NA'


We have no informations about the shape of the eggs of the <i>Coregonus lavaretus</i>.

Now we can put all the information into a data structure for the <b>Coregonus lavaretus</b>. In order to do that we will use the function `tibble()`.


```R
data_species <- tibble("Coregonus-lavaretus", max_length, IUCN_status, egg_shape)
data_species
```


<table>
<caption>A tibble: 1 × 4</caption>
<thead>
	<tr><th scope=col>"Coregonus-lavaretus"</th><th scope=col>max_length</th><th scope=col>IUCN_status</th><th scope=col>egg_shape</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>Coregonus-lavaretus</td><td>73</td><td>VU</td><td>NA</td></tr>
</tbody>
</table>




```R
names(data_species)[1] <- "Species"
data_species
```


<table>
<caption>A tibble: 1 × 4</caption>
<thead>
	<tr><th scope=col>Species</th><th scope=col>max_length</th><th scope=col>IUCN_status</th><th scope=col>egg_shape</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>Coregonus-lavaretus</td><td>73</td><td>VU</td><td>NA</td></tr>
</tbody>
</table>



In this way, we can now fetch the data about other species by repeating the same process and adding it to our _data_species_ tibble. 

<div style="background-color:rgba(0, 200, 0, 0.5);border-radius: 7px; padding:10px">
    <b>Checkpoint</b><br/>

Now your task is to create a new variable _'egg_color'_ and fetch its data and add it to the table (follow the same process used for 'egg_shape'). 
</div>


```R
# your code 
```


```R
### Solution

# extract the info
egg_color = egg_table[which(egg_table[,1] == "Color of Eggs"),2]

# set no information to NA
if(egg_color == "") {egg_color = "NA"}
data_species <- tibble("Coregonus-lavaretus", max_length, IUCN_status, egg_shape, egg_color)
data_species

```


<table>
<caption>A tibble: 1 × 5</caption>
<thead>
	<tr><th scope=col>"Coregonus-lavaretus"</th><th scope=col>max_length</th><th scope=col>IUCN_status</th><th scope=col>egg_shape</th><th scope=col>egg_color</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>Coregonus-lavaretus</td><td>73</td><td>VU</td><td>NA</td><td>NA</td></tr>
</tbody>
</table>



---

### 3.7 R interface to FishBase

In this subsection, we want to get data using an API. We use the R package `rfishbase` to get data from https://www.fishbase.de. The package `rfishbase` makes it relatively easy to look up for information on the most well-known fish species. As we saw in the previous sections, web scraping can be tedious. The `rfishbase` package simplifies the data extraction process but also has some limits as we will see later in the section.

Now we can get information on *Coregonus lavaretus* with this new package. 


```R
suppressWarnings(suppressMessages(
data_CL<- species("Coregonus lavaretus") 
))  
data_CL
```


<table>
<caption>A tibble: 1 × 100</caption>
<thead>
	<tr><th scope=col>SpecCode</th><th scope=col>Species</th><th scope=col>SpeciesRefNo</th><th scope=col>Author</th><th scope=col>FBname</th><th scope=col>PicPreferredName</th><th scope=col>PicPreferredNameM</th><th scope=col>PicPreferredNameF</th><th scope=col>PicPreferredNameJ</th><th scope=col>FamCode</th><th scope=col>⋯</th><th scope=col>Profile</th><th scope=col>PD50</th><th scope=col>Emblematic</th><th scope=col>Entered</th><th scope=col>DateEntered</th><th scope=col>Modified</th><th scope=col>DateModified</th><th scope=col>Expert</th><th scope=col>DateChecked</th><th scope=col>TS</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dttm&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dttm&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dttm&gt;</th><th scope=col>&lt;lgl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>232</td><td>Coregonus lavaretus</td><td>5204</td><td>(Linnaeus, 1758)</td><td>European whitefish</td><td>Colav_u4.jpg</td><td>NA</td><td>NA</td><td>NA</td><td>76</td><td>⋯</td><td>NA</td><td>0.5</td><td>0</td><td>2</td><td>1990-10-17</td><td>393</td><td>2017-03-06</td><td>1</td><td>1994-03-08</td><td>NA</td></tr>
</tbody>
</table>



We will start by getting the maximal length of the species. We'll use the function `species()` to do this. In order to do that we look for the column _'Length'_ in the data.


```R
length_max_CL <- species("Coregonus lavaretus", fields=c("Length"))
length_max_CL <- length_max_CL$Length
length_max_CL
```


73


As you might recall, this is the same value we got in section 3.4.

In the next step we are interested to get the information about the diet of *Coregonus lavaretus*. To get the diet information, we will use the function `diet_items()`. The function `diet_items()` allows us to access the table on the food items of the chosen species.


```R
food <- diet_items("Coregonus lavaretus")
head(food)
```


<table>
<caption>A tibble: 6 × 15</caption>
<thead>
	<tr><th scope=col>autoctr</th><th scope=col>DietCode</th><th scope=col>FoodI</th><th scope=col>FoodII</th><th scope=col>FoodIII</th><th scope=col>Stage</th><th scope=col>DietPercent</th><th scope=col>ItemName</th><th scope=col>Comment</th><th scope=col>DietSpeccode</th><th scope=col>DietSpeccodeSLB</th><th scope=col>AlphaCode</th><th scope=col>PreyTroph</th><th scope=col>PreySeTroph</th><th scope=col>PreyRemark</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;lgl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>122522</td><td>1</td><td>zoobenthos</td><td>mollusks     </td><td>bivalves      </td><td>n.a./others</td><td> 1.0</td><td>gastropods         </td><td>0.8% bivalves/gastropods incl.  0.1% euphausiids, 0.1% unid. organisms                                                                                                              </td><td> NA</td><td>NA</td><td>GAS</td><td>NA</td><td>NA</td><td>NA</td></tr>
	<tr><td>101628</td><td>1</td><td><span style=white-space:pre-wrap>nekton    </span></td><td><span style=white-space:pre-wrap>finfish      </span></td><td><span style=white-space:pre-wrap>bony fish     </span></td><td>juv./adults</td><td>18.1</td><td>Trisopterus minutus</td><td>1% &lt;i&gt;Rhinonemus minutus&lt;/i&gt;,4.1% &lt;i&gt;Trisopterus minutus&lt;/i&gt;, 0.2% Gobiidae, 3.3% &lt;i&gt;Trisopterus esmarkii&lt;/i&gt;,0.8% &lt;i&gt;Lumpenus&lt;/i&gt;,1.3% &lt;i&gt;Scyliorhinus canicula&lt;/i&gt;,7.4% unid. Fish</td><td>481</td><td>NA</td><td>POD</td><td>NA</td><td>NA</td><td>NA</td></tr>
	<tr><td>107359</td><td>1</td><td>zoobenthos</td><td>benth. crust.</td><td>lobsters      </td><td>juv./adults</td><td>12.6</td><td>Galatheidae        </td><td>Galatheidae                                                                                                                                                                         </td><td> NA</td><td>NA</td><td>LOQ</td><td>NA</td><td>NA</td><td>NA</td></tr>
	<tr><td>119985</td><td>1</td><td>zoobenthos</td><td>worms        </td><td>polychaetes   </td><td>juv./adults</td><td> 2.3</td><td>polychaetes        </td><td>NA                                                                                                                                                                                  </td><td> NA</td><td>NA</td><td>WOR</td><td>NA</td><td>NA</td><td>NA</td></tr>
	<tr><td>116667</td><td>1</td><td>zoobenthos</td><td>benth. crust.</td><td>shrimps/prawns</td><td>juv./adults</td><td> 5.0</td><td>Pandalidae         </td><td>Pandalidae                                                                                                                                                                          </td><td> NA</td><td>NA</td><td>PDZ</td><td>NA</td><td>NA</td><td>NA</td></tr>
	<tr><td>112452</td><td>1</td><td>zoobenthos</td><td>benth. crust.</td><td>shrimps/prawns</td><td>juv./adults</td><td> 5.3</td><td><span style=white-space:pre-wrap>Macropipus         </span></td><td><span style=white-space:pre-wrap>0.5% &lt;i&gt;Crangon&lt;/i&gt;,    0.5% &lt;i&gt;Processa&lt;/i&gt;,    2.3% &lt;i&gt;Macropipus&lt;/i&gt;,    0.7% Majidae, 0.1% Paguridae                                                                            </span></td><td> NA</td><td>NA</td><td>NA </td><td>NA</td><td>NA</td><td>NA</td></tr>
</tbody>
</table>



From the table food we can exract the column named _'FoodII'_. As you already know, the simplest way to extract a column from a table is to use the `$` operator between the table name and column name respectively.



```R
food_II <- food$FoodII
```

We can now extract the first and second elements of the column _'food_II'_ to get a short overview of the diet of *Coregonus lavaretus*.


```R
food_II[1]
food_II[2]
```


'mollusks'



'finfish'


As an output we get 'mollusks'and 'finfish'.

In the same way, can get the information about the predators. 

<div style="background-color:rgba(0, 200, 0, 0.5);border-radius: 7px; padding:10px">
    <b>Checkpoint</b><br/>
     Now your next task is to get the information about predators. In order to obtain the information you can simply use the function 'predators()'. 
</div>


```R
# your code
```


```R
### Solution

# use function 'predators()' for Coregonus lavaretus
pred <- predators("Coregonus lavaretus")

# you can look at the column 'PredatorName' to get the names of the predators
pred_name <- pred$PredatorName
pred_name
```


<style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>'Coregonus peled'</li><li>'Esox lucius'</li><li>'Sander lucioperca'</li><li>'Lampetra fluviatilis'</li><li>'Salmo trutta trutta'</li><li>'Coregonus lavaretus'</li><li>'Phryganea sp.'</li></ol>



So we obtained list of predators of *Coregonus lavaretus*.
For other functions in the R package `rfishbase` check https://cran.r-project.org/web/packages/rfishbase/rfishbase.pdf. 

**Limits of API:** One of the major limitations of an API is that we can only use already implemented functions. For example, in our case, we cannot get the IUCN Status of a given species. In order to obtain the IUCN Status we must use another API, namely the package `rredlist`. But to have access to the _rredlist_ API we need a key (a key serves as an authentication barrier to have access to the API and often comes as a paid service). Hence, we will not do that in this exercise.


---

### 3.8 Summary

<div style="border: 2px black solid; border-radius: 7px; padding:10px">
    
- In this third section we learned how to extract data from a website. 
    
- We saw that this can be done either with web scraping or using APIs. 
    
- We saw that web scraping can be tedious, but on the other hand we also saw some limitations of APIs.
</div>

Next, we look at another case study on species richness and red list species proportions.

---

## 4. Case Study: Species richness and Red List species proportions

To proceed with the case study we need to prepare our dataset. First, we need to get a list of species for the dataset. In this subsection we will see how to get all the species in a given family.

---

### 4.1 Getting all the species in a family 

In this section, we want to get all the species of the family of **Salmonidae**. As before for flexibility reasons, we will create an object _y_ with the name of the family and use `paste()` to get the link.


```R
y <- "Salmonidae"
  
url2<-paste("http://www.fishbase.org/Nomenclature/FamilySearchList.php?Family=", y,sep="") 
```

We then use `getURLContent()` to get the content of the link _url2_ and save it in the variable _Content_Sal_.


```R
Content_Sal <- getURLContent(url2, followlocation=TRUE) 
```

Next we create a dataframe using `data.frame()` and get a list of variables. We will extract the variables with the same number of rows and unique row names. The function `readHTMLTable()` (from earlier) helps to extract data from HTML tables in an HTML document.



```R
z <- data.frame(readHTMLTable(Content_Sal))
```

Then we can extract the species from the given family with `as.character()`. We use `z[,1]` to get the first column which is the column with the scientific names of species. 


```R
sp_per_family <- as.character(z[,1])
```

Now we can print the first element of the column with the scientific names.


```R
sp_per_family[1]
```


'Brachymystax lenok'


Using `str_replace()` function we can substitute the empty space between the Genus and the Species with a `"-"`.


```R
sp_per_family <- str_replace(sp_per_family, " ", "-")
```

Finally we can print the first element of _sp_per_family_ again.


```R
sp_per_family[1]
```


'Brachymystax-lenok'


---

### 4.2 Getting the IUCN Status of a list of species

In this section, we are going to get the IUCN Status for a given List of species using a 'for loop'. We will extract the IUCN Status of all the species from the Netherlands.
First, we are going to upload the dataset containing the list of the species and some other data that is going to be useful for the next sections. We will do that by using the function `read_csv()`. This data came from https://www.nature.com/articles/sdata2017141 by cropping it to Western Europe.


```R
suppressWarnings(suppressMessages(
dataset <- read_csv("../data/dataset2.csv")))
```

As we only need data related to Netherlands, we will use the function `grep()` and pass _Netherlands_ as an argument.


```R
subset <- dataset[grep("Netherlands", dataset$Country),]

```

Let's first briefly discuss how to construct a 'for loop', since it's been a while since you used it in previous chapters. To get the IUCN Status of a list of species we always change the value of _x_ (the species) and run the code, but if we have to do that for many species it will be very long and tedious. In this case 'for loops' come in handy. In a 'for loop' the variable _x_ runs over the vector (here each species) and returns the IUCN Status. Before getting the IUCN status let's just go over an easy example, such as printing the integers from 1 to 10 using a 'for loop'. In this case, we iterate over the vector 1:10. 


```R
for(j in 1:10) {
print(j) # this prints the value of j for that given loop
}
```

    [1] 1
    [1] 2
    [1] 3
    [1] 4
    [1] 5
    [1] 6
    [1] 7
    [1] 8
    [1] 9
    [1] 10


Now we can make a 'for loop' in order to get the IUCN status of all the species in the above subset (Netherlands) of the initial dataset. We are going to iterate over the column of the dataset with the valid FishBase species names. The code lines inside the loop are exactly a copy-paste of what we had for the *Coregonus lavaretus*, but in this case, we have to look for other species. In the last line of the code below, we created a new column in the data frame _subset_ in order to save the IUCN Status. The 'i' in `subset$IUCN[i]` is used to save the IUCN status of the species we are iterating over in the 'for loop'. It will save the results of the species one by one. 


```R
i <- 0
for (x in subset$X6.Fishbase.Valid.Species.Name) {
i <- i + 1
url3 <- paste("http://www.fishbase.de/summary/",x,".html",sep="")                 # we call the url
fish_species <- htmlParse(getURLContent(url3, followlocation=TRUE))              # get the content
fish_species_div <-getNodeSet(fish_species, "//div ")                            # get the node with the fish species (the nodeSet gets all the values inside the //div tag)
w_IUCN  <-which(sapply(lapply(fish_species_div,xmlValue),function(x)            # look for the IUCN content
  {regexec(pattern="IUCN", x)[[1]][1]})>0)
if(length(w_IUCN)==0){                                                        # here we assign NA if the fileds are empty
  IUCN_status="NA"
} else {                                                                     # else we read the information and iterate over the fields
  d1_IUCN  <- xmlValue(fish_species_div[[w_IUCN[length(w_IUCN)]]])
  IUCN <- unlist(regmatches(d1_IUCN,gregexpr(pattern= "[[:alpha:]]+)",     
  d1_IUCN)))
  IUCN_status <- sub(pattern="[[:punct:]]",replacement="",IUCN[1] ) 
} 
print(IUCN_status)
subset$IUCN[i] <- IUCN_status # make a new column in 'subset' containing the IUCN status
}

```

    [1] "LC"


    Warning message:
    “Unknown or uninitialised column: `IUCN`.”


    [1] "LC"
    [1] "LC"
    [1] "LC"
    [1] "LC"
    [1] "LC"
    [1] "NT"
    [1] NA
    [1] NA


We can see which IUCN statuses are present in Netherlands by using the `unique()` function.


```R
unique(subset$IUCN)
```


<style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>'LC'</li><li>'NT'</li><li>NA</li></ol>



---

### 4.3 Proportion of species in Netherlands

We will now plot the proportion of species in each category for the Netherlands. So let us calculate the number of species in each category (from above we just have 2 outputs, _LC_ & _NT_). In general in other countries, we also have other IUCN Status, for example, VU. In this section, we will focus only on the two listed statuses. Using the function `length()` we can obtain the number of species in each category.


```R
number_lc <- length(which(subset$IUCN == "LC"))
number_nt <- length(which(subset$IUCN == "NT"))

# print the values for LC, using 'paste0()'' to help visualise the results.
paste0("LC:", " ", number_lc)

# print the values for NT
paste0("NT:", " ", number_nt)
```


'LC: 6'



'NT: 1'


We are ready to plot this as pie chart.


```R
slices <- c(number_lc, number_nt)
lbls <- c("LC","NT")
pie(slices, labels = lbls,font.main = 1, 
main = "Proportion of species per IUCN Status in Netherlands", col=c("red", "yellow")) 
```


![png](output_172_0.png)


### 4.4 Cleaning the data with the IUCN Status

In this part of the tutorial, we have to clean the data that we will use for the correlations. We have provided the dataset already with the IUCN status since the code needs a lot of time to run, but the procedure is exactly the same as we did in the previous sections. So, let's load the dataset, it's called _datasetIUCN_.


```R
suppressWarnings(suppressMessages(
dataset_IUCN <- read_csv("../data/datasetIUCN.csv")
))
```

In the previous sections, we saw that for some species we did not have information on the IUCN status. In these cases, we set the IUCN status as NA. We can check for possible NA values by calling the `unique()` function. 


```R
unique(dataset_IUCN$IUCN)
```


<style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>'LC'</li><li>'CR'</li><li>'lc'</li><li>'VU'</li><li>NA</li><li>'EX'</li><li>'NT'</li><li>'DD'</li><li>'EN'</li><li>'EW'</li></ol>



<div style="background-color:rgba(0, 200, 0, 0.5);border-radius: 7px; padding:10px">
    <b>Checkpoint</b><br/>
     Now your task is to remove the row with the IUCN status as NA. You can use the function `subset()` to get the subset of the dataset with information about the IUCN status.
</div>




```R
# your code
```


```R
### Solution

# subset the data without NAs (!=NA; not equal to NA), so this effectively removes the NAs
dataset_IUCN_NA <- subset(dataset_IUCN, dataset_IUCN$IUCN != "NA")

# check it worked
unique(dataset_IUCN_NA$IUCN)
```


<style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>'LC'</li><li>'CR'</li><li>'lc'</li><li>'VU'</li><li>'EX'</li><li>'NT'</li><li>'DD'</li><li>'EN'</li><li>'EW'</li></ol>



Next, we will load the shapefile of glacial basins using the package <b>rgdal</b> and the function `readOGR()`. We fetch the data from different fish basins across Europe in the _basin_shapefile_ from the data stored in the basins folder. 


```R
basin_shapefile <- readOGR("../data/basins")
```

    OGR data source with driver: ESRI Shapefile 
    Source: "/work/04_data_scraping/data/basins", layer: "basins_glaciation"
    with 250 features
    It has 11 fields


In order to plot the basins on the map we will use `fortify()` function on the _basin_shapefile_. This function helps to convert a lines and points object to a data frame for ggplot. We will store this dataframe as _fort_basin_.


```R
fort_basin <- fortify(basin_shapefile)
head(fort_basin)
```

    Regions defined for each Polygons
    



<table>
<caption>A data.frame: 6 × 7</caption>
<thead>
	<tr><th></th><th scope=col>long</th><th scope=col>lat</th><th scope=col>order</th><th scope=col>hole</th><th scope=col>piece</th><th scope=col>id</th><th scope=col>group</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;fct&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>-5.658333</td><td>36.05417</td><td>1</td><td>FALSE</td><td>1</td><td>0</td><td>0.1</td></tr>
	<tr><th scope=row>2</th><td>-5.668877</td><td>36.05473</td><td>2</td><td>FALSE</td><td>1</td><td>0</td><td>0.1</td></tr>
	<tr><th scope=row>3</th><td>-5.672790</td><td>36.05777</td><td>3</td><td>FALSE</td><td>1</td><td>0</td><td>0.1</td></tr>
	<tr><th scope=row>4</th><td>-5.677210</td><td>36.05890</td><td>4</td><td>FALSE</td><td>1</td><td>0</td><td>0.1</td></tr>
	<tr><th scope=row>5</th><td>-5.681123</td><td>36.06193</td><td>5</td><td>FALSE</td><td>1</td><td>0</td><td>0.1</td></tr>
	<tr><th scope=row>6</th><td>-5.685544</td><td>36.06307</td><td>6</td><td>FALSE</td><td>1</td><td>0</td><td>0.1</td></tr>
</tbody>
</table>



We can visualise our _basin_shapefile_ using `ggplot()`.


```R
ggplot() + geom_polygon(data = fort_basin, aes(x = long, y = lat, group = group), colour = "black", fill = NA)

```


![png](output_186_0.png)


### 4.5 Maps of species richness and Red List species proportions

We now want to map the species richness and Red List species proportions.
Let us first calculate the species richness and proportion of Red List species per basin. Species richness is the number of species pro basin and the proportion of red list species is the ratio between the number of species in the red list and the number of species pro basin. We will iterate over the vector with the basin names and we will use the function `nrow()` to find the number of occurrences.  


```R
for (x in as.character(basin_shapefile$BasinName)) {
    
  # we now restrict to dataset to the basin x
  dataset_basins <- dataset_IUCN[dataset_IUCN$X1.Basin.Name==x,]
    
  # number of species in the given country x
  n1 <- nrow(dataset_basins)
    
  # we now restrict to dataset to the country x and Red List
  dataset_c_IUCN <- dataset_basins[grep("VU|EN|EX|EW|CR", dataset_basins$IUCN),] 
    
  # number of species in the given country x with given IUCN Status
  n2 <- nrow(dataset_c_IUCN)
    
  # compute the proportion 
  basin_shapefile$proportion[basin_shapefile$BasinName==x] <- n2/n1
  basin_shapefile$richness[basin_shapefile$BasinName==x] <- n1  
}
```

We can see the newly computed data for the columns _'proportion'_ and _'richness'_ with respect to each basin in the _basin_shapefile_. 


```R
head(basin_shapefile@data)
```


<table>
<caption>A data.frame: 6 × 13</caption>
<thead>
	<tr><th></th><th scope=col>BasinName</th><th scope=col>Country</th><th scope=col>Ecoregion</th><th scope=col>Endorheic</th><th scope=col>Out_Longit</th><th scope=col>Out_Latit</th><th scope=col>Med_Longit</th><th scope=col>Med_Latit</th><th scope=col>Surf_area</th><th scope=col>ice_dist</th><th scope=col>temp</th><th scope=col>proportion</th><th scope=col>richness</th></tr>
	<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>0</th><td>Valle           </td><td>Spain  </td><td>Palearctic</td><td>NA</td><td> -5.721676</td><td>36.06629</td><td> -5.7671465</td><td>36.11803</td><td>  146.6600</td><td>1592409.4</td><td>18.031604</td><td>0.25000000</td><td> 8</td></tr>
	<tr><th scope=row>1</th><td>Laxa.Leirarsveit</td><td>Iceland</td><td>Palearctic</td><td>NA</td><td>-21.874556</td><td>64.39327</td><td>-21.6432853</td><td>64.44753</td><td>  187.1266</td><td> 368687.7</td><td> 2.507056</td><td>       NaN</td><td> 0</td></tr>
	<tr><th scope=row>2</th><td>Andakilsa       </td><td>Iceland</td><td>Palearctic</td><td>NA</td><td>-21.781420</td><td>64.54236</td><td>-21.4828095</td><td>64.49633</td><td>  219.6690</td><td> 387360.4</td><td> 3.113605</td><td>       NaN</td><td> 0</td></tr>
	<tr><th scope=row>3</th><td>Aa              </td><td>France </td><td>Palearctic</td><td>NA</td><td>  2.098209</td><td>51.01710</td><td>  2.0492197</td><td>50.78136</td><td> 1200.3646</td><td> 261541.3</td><td>10.529269</td><td>0.06666667</td><td>30</td></tr>
	<tr><th scope=row>4</th><td>Adour           </td><td>France </td><td>Palearctic</td><td>NA</td><td> -1.498072</td><td>43.53500</td><td> -0.5065982</td><td>43.42947</td><td>16911.1610</td><td> 875198.0</td><td>12.460502</td><td>0.08888889</td><td>45</td></tr>
	<tr><th scope=row>5</th><td>Agly            </td><td>France </td><td>Palearctic</td><td>NA</td><td>  3.046515</td><td>42.81225</td><td>  2.6066918</td><td>42.81292</td><td> 1125.2007</td><td>1060457.1</td><td>13.120775</td><td>0.09090909</td><td>22</td></tr>
</tbody>
</table>



Next we get the map of Europe. We will read the data in the _continent_shapefile_ and then will extract the continent 'Europe' map. If you call the variable _europe_, you will see that the dataframe is empty. This is because it extracts only the map data which we can simply plot by passing the europe as an argument in `plot()` function. We will do it in the upcoming code cells.


```R
continents <- readOGR('../data/continent_shapefile')
europe <- continents[continents$CONTINENT=='Europe',]
```

    Warning message in readOGR("../data/continent_shapefile"):
    “First layer europe_map read; multiple layers present in
    /work/04_data_scraping/data/continent_shapefile, check layers with ogrListLayers()”


    OGR data source with driver: ESRI Shapefile 
    Source: "/work/04_data_scraping/data/continent_shapefile", layer: "europe_map"
    with 53 features
    It has 94 fields


**So now let us plot the proportions of Red List species.** 

First, we will create a new column _'proportion_colour'_ and in this column, we will store the colours. Then we will break the proportions into 10 different parts by grouping the values in the proportion column into 10 using the `cut()` function. Then we get rid of the index vector using `as.numeric()` and get all the values as numeric values. We used the `rev()` function to reverse the colours, red colour indicates the species that are getting distinct and have very less proportion pro basin and yellow indicates the species with comparatively more proportion in the basin. Finally, we store these colours to the column proportion_colour. The colour indicates the proportion of Red List species occurring in the corresponding basin. We do the same for the species richness.


```R
basin_shapefile$proportion_colour <- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$proportion, breaks = 10))]
```

Now we create a new column in the _fort_basin_ dataframe and map the values of _'proportion_colour'_ into the new color column. We are doing this to have the colour (species proportion divided into 10 parts) and lat-long values in one datframe which helps to plot the graph using `ggplot()`. 


```R
fort_basin$color <- fort_basin$id    # create a new column color

for (i in as.numeric(unique(fort_basin$id))) # map the values into color column by id
{
   fort_basin$color[fort_basin$id == i] <- basin_shapefile@data$proportion_colour[i+1]
}
```

Remember above we extracted the continent _'europe'_ from the _continent_shapefile_, here we will use `fortiy()` on the europe dataset to plot it using ggplot also.


```R
fort_europe <- fortify(europe)
```

    Regions defined for each Polygons
    


We will plot the _fort_europe_ and _fort_basin_ data on the map.


```R
ggplot(fort_basin, aes(x = long, y = lat, group = group)) +
  geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = 'white') +
  geom_polygon(fill = fort_basin$color, colour = "black")+
  xlim(-25, 28)
```


![png](output_201_0.png)


From the graph, we see that the proportion of Red List species is highest in South-Western Europe. The aim of the Red List is to inform decision-makers about potentially endangered species, i.e. species whose population size has been rapidly declining during the last decades or the species that only occur in small numbers at present. The proportion of species on the Red List of each region, therefore, gives an indication of the risk of species going extinct in a region and represents an important tool for conservation strategies.

**We will repeat the above steps all together for plotting the species richness on map.**


```R
# break the richness  of the species into 10 parts and then we assign colours to each part 
basin_shapefile$richness_colour <- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$richness, breaks = 10))]

fort_basin$rich_color <- fort_basin$id    # create a new column rich_color

# mapping the values from basin_shapefile to fort_basin

for (i in as.numeric(unique(fort_basin$id))) 
{
   fort_basin$rich_color[fort_basin$id == i] <- basin_shapefile@data$richness_colour[i+1]
}

# plot

ggplot(fort_basin, aes(x = long, y = lat, group = group)) +
  geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = 'white') +
  geom_polygon(fill = fort_basin$rich_color, colour = "black")+
  xlim(-25, 28)
```


![png](output_204_0.png)


### 4.6 Investigate the relationship of basin size and species richness

In the previous section, you observed the spatial patterns of fish diversity across Europe. In this section, we will try to explain these patterns. To achieve this, we will correlate the fish species richness of each basin to the surface area of the corresponding area to see how the species richness varies with respect to the surface area of the basin. We begin with plotting a simple scatterplot. We will log transform the data we have on surface area as it helps to make data conform to normality and also helps to deal with the outliers and skeweness in the data. Then we will plot this data against species richness. Since the data-deficient basins show zero observations in the dataframe, we will remove those first. 


```R
basin_shapefile <- basin_shapefile[basin_shapefile$richness!=0,]
```

Now we will create a new dataframe with the _surface area_ and _richness_. We'll the columns as _'Basin_area'_ and _'Species_richness'_ respectively.


```R
bs_sr <- tibble(basin_shapefile@data$Surf_area, basin_shapefile@data$richness )
names(bs_sr)[1]<- 'Basin_area'
names(bs_sr)[2]<- 'Species_richness'


```

We make a simple scratterplot with a regression line to visualise the relationship.


```R
ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) +
geom_point() +
geom_smooth(method='lm', color="red", size=0.5, se=FALSE)+
xlab("Basin Area") +
ylab("Species Richness") +
theme_classic()
```

    `geom_smooth()` using formula 'y ~ x'
    



![png](output_211_1.png)


The plot shows a positive correlation between basin area and fish richness, meaning that we expect to see a higher richness in larger basins. This pattern is commonly observed in ecology and one explanation for this is that larger areas provide different habitat types (niches), which allows more species to co-exist.

As a next step, we will create a simple model of this relationship. This allows us to make predictions on the richness of fish species in other basins based on the basin area.


```R
#create a linear model
richness_model <- lm(richness~log(Surf_area), data=basin_shapefile@data)
```

We now want to calculate the confidence intervals of the model to have a better idea of the uncertainty of the model. We will then coerce the basin surface, the model fit and the confidence interval into a new data frame called *model_df*.


```R
#calculate the confidence intervals
model_df <- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval='confidence')))
names(model_df)[1] <- 'log_area'
head(model_df)
```


<table>
<caption>A data.frame: 6 × 4</caption>
<thead>
	<tr><th></th><th scope=col>log_area</th><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>0</th><td>4.988117</td><td> 7.215144</td><td> 4.416531</td><td>10.01376</td></tr>
	<tr><th scope=row>3</th><td>7.090381</td><td>20.001125</td><td>18.488035</td><td>21.51421</td></tr>
	<tr><th scope=row>4</th><td>9.735729</td><td>36.090151</td><td>34.001427</td><td>38.17887</td></tr>
	<tr><th scope=row>5</th><td>7.025717</td><td>19.607838</td><td>18.069980</td><td>21.14570</td></tr>
	<tr><th scope=row>6</th><td>7.492151</td><td>22.444692</td><td>21.046504</td><td>23.84288</td></tr>
	<tr><th scope=row>7</th><td>6.602807</td><td>17.035701</td><td>15.301404</td><td>18.77000</td></tr>
</tbody>
</table>



The final step is  to plot the model fit and prediction intervals over the data. To get nice lines in the plot, the *model_df* dataframe needs to be ordered by the basin area first.


```R
ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) +
geom_point() +
geom_line(aes(x = model_df$log_area, y= model_df$fit ), color = "red")+
geom_line(aes(x = model_df$log_area, y= model_df$lwr ), color = "red", linetype = "dotted")+
geom_line(aes(x = model_df$log_area, y= model_df$upr ), color = "red", linetype = "dotted")+
xlab("Basin Area") +
ylab("Species Richness") +
theme_classic()
```


![png](output_217_0.png)


<div style="background-color:rgba(0, 200, 0, 0.5);border-radius: 7px; padding:10px">
    <b>Checkpoint</b><br/>
     You have now seen how you can calculate and plot the confidence interval of your data. Based on your model you can also create a so called <b>prediction interval</b> which gives an estimate of the range in which the model will most likely predict the y-values (for a given x-value). In our case the prediction interval would indicate in which range the model would expect the fish richness to be for a given basin surface area. Do you think that these prediction intervals will be broader or narrower than the confidence interval? Try to write the code for calculating and plotting the prediction intervals by yourself.
</div>




```R
# your code
```


```R
### Solution

#prediction intervals

# make a dataframe of area and richness model predictions
model_df_prediction <- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval='prediction')))
names(model_df_prediction)[1] <- 'log_area' 

# order the basins by size
model_df_prediction <- model_df_prediction[order(model_df_prediction$log_area),]

# plot the model
plot(log(basin_shapefile$Surf_area), basin_shapefile$richness, pch=16, cex=1, col=rgb(0,0,0,0.6), xlab='Basin area', ylab='Basin species richness')
lines(model_df_prediction$log_area, model_df_prediction$fit, col='skyblue3', lwd=2)
lines(model_df_prediction$log_area, model_df_prediction$lwr, col='skyblue2', lwd=2, lty=3)
lines(model_df_prediction$log_area, model_df_prediction$upr, col='skyblue2', lwd=2, lty=3)


```

    Warning message in predict.lm(richness_model, interval = "prediction"):
    “predictions on current data refer to _future_ responses
    ”



![png](output_220_1.png)


---

## 5. References

* Automated Data Collection with R, S. Munzert, C. Rubba, P. Meißner and D. Nyhuis
* XML and Web Technologies for Data Sciences with R, D. Nolan, D. Temple Lang
* http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/styled-4/styled-6/code-13/
* https://ourcodingclub.github.io/tutorials/webscraping/
* https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/

---

## Exercise


For this week's exercise open up the Rstudio environment. Remember to save all your changes to this notebook using git status, git add <filename>, git commit -m "your comment", git push.
    
Exercise 04 is about getting data from the web and extracting usefull insights from it. 
    
Get in touch with your teaching assistant if you have any further questions.

---
