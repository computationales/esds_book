# Data Scraping {#ch-04}


## Introduction
This chapter covers _data scraping_ in R and yes, this is not a typo. _Scraping_ means to gather or to extract whereas _scrapping_ means to get rid of something. The goal of the sections below is to gather and convert online data into a structured format that can be easily accessed and modified in R. As a case study, we will scrape data from a website containing various information about fish species, called [FishBase](https://www.fishbase.in/search.php).

We will first learn how to access the website and how to extract useful useful information from plain text. Then, we will look at how to access a table that is embedded in the website and how to generate a new table in R which holds all information we want to extract. Next up is cleaning up the scraped data for visualization and analysis. At the end we will conduct a case study where we create a model to make predictions about fish species richness.  


### Theory

Let us first have a brief review of the main concept of the lecture. The web is the largest source of information and often free to access. In some cases the information is already presented in a nice format and can be easily copied into an excel spreadsheet. However, this is often not the case and data-sets are only presented in a format that is not easy to download or modify. Manually copying data from different sub pages and sections is a tedious, slow and error prone approach. We therefore need a more automated and efficient technique to access such data.

Web scraping is a popular technique to extract information directly from a website by accessing its underlying HTML code. Scraping allows us to gather this unstructured data from many websites and put it all together in a ready-to-use data format. This structured data can then be further used as training, validation or test data sets for our machine learning algorithms. 

**Important Concepts of Websites**

- **HTTP**: The  **H**yper**t**ext **T**ransfer **P**rotocol is a widely used protocol for information systems. Hypertext documents include hyperlinks that can be clicked to access other resources. Put simply, HTTP is the foundation of how information, i.e., data, is communicated in the world wide web. 

- **HTML**: **H**yper**t**ext **M**arkup **L**anguage is the coding language in which most websites are written. This includes elements like formatting or structuring and is often combined using CSS or JavaScript. HTML is organized using tags, which are surrounded by `< >`. Each HTML document is made of elements that are specified using tags. HTML elements and HTML tags are often confused. Tags are used to open and close the object, whereas elements include both tags and its content.

  Let’s consider an example with the `<h1>` tag: `<h1> Title of the document </h1>` is an element, and `<h1>`, `</h1>` - are the tags enclosing it. The `<>` symbol is used to open a tag or an element and `</>` is used to close it. HTML documents have a hierarchical or tree like structure with different types of nodes as described in \@ref(fig:node-tree). The rectangular boxes are referred to as nodes. The *Text* node is also called as a child node of the *Element* node and also a leaf node as it only contains text and no links to further nodes. Both of the *Element* nodes that are attached to the *Root Element* node are called sibling nodes of the *Root Element* node. When we do web scraping we go through such a hierarchical structure to get the content (here text in the *Test* node). Keep in mind that every HTML document can vary with respect to its structure. This example is to just to provide you some knowledge about HTML document and its structure.

```{r node-tree,echo=FALSE, fig.cap='Visualization of the structure of an HTML document.'}
knitr::include_graphics(rep("./figures/node-tree.png"))
```

- **XML**: The E**x**tensible **M**arkup **L**anguage has some similarities to HTML but the format is generally easier to read for machines. While HTML has a number of pre-defined tags, one can create ("extend") new tags as needed . This allows to define and control the meaning of the elements contained in a document or text.

- **API**: An **A**pplication **P**rogramming **I**nterface is a set of procedures and communication protocols that provide access to the data of an application, operating system or other services. Both APIs and web scraping are used to retrieve data from websites, but their methodology differs substantially. APIs give us direct access to the data we would want, but they are limited to the corresponding website. As a result, we might find us in a scenario where there might not be an API to access the data we want. In these scenarios, web scrapping would allow us to access the data as long as it is available on a website. Hence APIs are very source/website specific and we can only do what is already implemented but in a clean fashion, while scrapping is more flexible and can be applied (nearly) everywhere but we have to handle all the formatting, inconsistencies, extraction, etc. by yourself.

Next, we will demonstrate the principles of web scraping using a simple case study. We will extract fish occurrence data from an online database and perform some basic correlations with the obtained data. 


## Tutorial
### R-Packages and Functions

We will now load all packages necessary to perform web scraping on FishBase, namely `RCurl` and `XML`. `RCurl` provides functions to allow us to compose general HTTP requests and provides convenient functions to fetch URLs via get and post requests and process the results returned by the web server.
`XML` give us approaches for both reading (get request) and creating (post request) XML and HTML documents, both locally and on the web via HTTP. Later in this tutorial, we will create some spatial visualization and load the packages `raster`, `sf` and `rgdal` to do so. Before we proceed further to accessing FishBase, we will have a quick look at the useful functions `lapply()` and `sapply()`.

```{r eval=TRUE, message=FALSE, results='hide'}
lib_vec <- c("RCurl", "XML", "raster", "rgdal", "rfishbase", "tidyverse", "sf")
sapply(lib_vec, library, character.only = TRUE) # See below for how sapply() works
help(package = "XML") # Use this code line in the RStudio console to learn more about loaded packages
```

```{r}
help(package = "XML") # Use this code line in the RStudio console to learn more about loaded packages
```


Sidenote: For data scraping in R one can also use 'rjson' to convert R objects into JSON objects and vice-versa. Another option we will use later are APIs. To get information about the loaded packages we can type the following command in the console:

#### lapply() {-}

`lapply()` allows us to apply the same function to a vector of objects. The respective arguments are `X` (the vector or object we give as input) and `FUN` (the function which is applied to each element of `X`). The output of `lapply()` is a list of the same length as `X` where each element of `X` is the result of applying `FUN` to the corresponding element of `X`. `X` is a vector (atomic, list or data frame) or an expression object. The `l` in `lapply()` thus stands for list.  

A simple example is to change the string value of a matrix to lower case with the R-function `tolower()` function. First, we set up a vector with string objects that we want to apply the function on. Then we create a pipe that feeds this vector into `lapply()` which applies `tolower`. The output is a list of the original two string objects but now written in lower case.

```{r}
fish <- c("FAMILY","SPECIES")
fish %>% lapply(tolower) # Note that tolower() must not be written with brackets
```

#### sapply() {-}

`sapply()` takes a list, vector or data frame as input and gives a vector or matrix as output. This is very useful if we have to use another function which only accepts a vector as input but does not accept a list. Note that we can apply the same function using `lapply()` or `sapply` but their outputs are not the same.

As an example we can have a look at the cars data where the speed and respective stopping distance are saved. We can apply the `min()` function to both features of the data set to get their minimal values. If we are using `lapply()`, we get a list that is accessible using `$`. With `sapply()` we get a named vector which can be seen at the headings `speed` and `dist` in the output below (those headings are of course the same as the variable names in the outputted list of `lapply()`).

```{r}
(df <- head(cars))
df %>% sapply(min)
df %>% lapply(min)
```

### The FishBase website

[FishBase](https://www.fishbase.in/search.php) is a global species database of fish species. It provides data of various fish species, including information on taxonomy, geographical distribution, biometrics and morphology, behaviour and habitats, ecology and population dynamics as well as reproductive, metabolic and genetic data. Different tools, such as trophic pyramids, identification keys, biogeographical modelling and fishery statistics can be accessed on the website. Furthermore, direct species level links to information in other databases such as LarvalBase, GenBank, the IUCN Red List and the Catalog of Fishes exist. As of November 2018, FishBase included descriptions of 34,000 species and subspecies.

```{r screen,echo=FALSE, fig.cap='Screenshot of the FishBase webpage for the species *Coregonus lavaretus*, a member of the family Salmonidae. As can be read in the distribution paragraph, it is widespread in freshwater systems from central and northwest Europe to Siberia.'}
knitr::include_graphics(rep("./figures/screen.png"))
```

As shown in Figure \@ref(fig:screen), the website contains lots of information for all the different species and this information is stored in various different data types. A few examples of these data types are:

- Numbers in different formats and units (temperature ranges, latitudinal distribution)
- Text blocks (description of the distribution)
- Tables (fecundity, larvae information)
- Pictures and videos (of the species, embedded into HTML code)

For a better understanding of the following R code, you are strongly encouraged to have a look at the [FishBase](https://fishbase.de) website in your browser. In the next sections you will learn how to deal with these different complex data types. A careful approach is required when extracting and downloading such information. Understanding the structure of the website is an important first step and it will save you coding time. So, always identify the relevant information you want to extract first and then write a suitable extraction code which fits the needs of your machine learning algorithm.

### Accessing FishBase

We are now ready to extract data from the *Coregonus lavaretus* website on [FishBase](https://fishbase.de). For this we first create a string object which holds the name of the fish profile we want to access. Using a variable to do this, adds flexibility in functions as will be shown below. If we want to get data about other species we can simply assign a new value to the object `x`with the desired species name. 

```{r}
x <- "Coregonus-lavaretus"
x
```

Next, we use the function `paste()` to create an object holding the URL to the fish profile. Attaching strings like this is called concatenate and will be used quite often in later tutorials, so remember this wording. To create the URL we do not have to add any separation between the arguments, so we use `sep = ""`. Using the flexibility of using `x` allows us here to add any species name directly into `paste()` instead of deleting and adding another Latin name every time. 


```{r}
url <- paste("http://www.fishbase.de/summary/", x ,".html",sep="")
url
```

For `url` we could also directly use *http://www.fishbase.de/summary/Coregonus-lavaretus* but then we would loose flexibility if we want to look for information about other species.


**Checkpoint**
Try to do the same for the URL of other species by changing the code above.

**Solution**
```{r, eval = FALSE}
# Change the object x_end to desired species name
x_ex<- "Salvelinus alpinus"

# Concatenate the URK
url_ex <- paste("http://www.fishbase.de/summary/",x_ex,".html",sep="")
```


To access the URL and its HTML documents, we will use various functions from the `XML` package. First, we want to get the URL content using `getURLContent()` which takes a URL as input argument. Then, we use `htmlParse()` to read the HTML document from the URL content into an R object. To get help on functions we can always use `help("htmlParse")`.

```{r}
fishbase <- url %>% getURLContent(followlocation = TRUE) %>% htmlParse() 
```

The HTML document saved in `fishbase` is quite long to show it entirely but a snapshot is displayed in Figure \@ref(fig:fishbase). 

```{r fishbase, echo=FALSE, fig.cap='Extract from the HTML document saved in the variable `fishbase`.'}
knitr::include_graphics(rep("./figures/fishbase.png"))
```

Somewhere in this gigantic object we can find the relevant information we are looking for. For example, Figure \@ref(fig:maxle) below shows the extract where the maximal lenght of the fish is documented. Check out the online profile to double check whether the value is correct.

```{r maxle, echo=FALSE, fig.cap='Extract of `fishbase` documenting maximal length of *Coregonus lavaretus*.'}
knitr::include_graphics(rep("./figures/fishbase_maxle.png"))
```

Highlighted in red are the opening and closing tags of the element where the information on the maximal length is stored. Knowing these tags, we can identify where our information is stored and how to extract it. In this example, we need to target either the `span` or `\div` tags. In order to extract the information inside these two tags, we use the function `getNodeSet()`. This function finds XML nodes that match a particular criterion. `span` is used for a small chunk of HTML inside a line whereas `div` is used to group larger chunks of code.

```{r}
fishbase_div <- fishbase %>% getNodeSet("//div ") 
fishbase_span <- fishbase %>% getNodeSet("//span ") 
```

In the next two Figures \@ref(fig:before) and \@ref(fig:after) we can see the differences of what is stored in variable `fishbase` and `fishbase_div`. The difference to `fishbase_span` is similar. We see that instead of having this one long unstructured extract in Figure \@ref(fig:before), we have now a list of objects that we easily access. In Figure \@ref(fig:after), the list can be identified by the `[[22]]` in the top left corner which marks the position of the object within the list. In short, `getNodeSet()` identifies all the sections for a given tag (i.e., nodes), separates them and gathers them in a list.

```{r before, echo=FALSE, fig.cap='`fishbase` before the command `getNodeSet()`'}
knitr::include_graphics(rep("./figures/before.png"))
```

```{r after, echo=FALSE, fig.cap='`fishbase` after the command `getNodeSet()`'}
knitr::include_graphics(rep("./figures/after.png"))
```

### Scraping Numbers

Next, we now want to extract the maximal body length of *Coregonus lavaretus* out of the list of nodes we created in the previous section. For this, we will use the function `xmlValue()` which allows us to access the text in the nodes by converting them into strings. To proceed further, we briefly have a look at regular expressions.

A *regular expression* is basically a sequence of characters (think of a certain word or a number) that can be searched for in a string (which is nothing else but a sequence of characters). The `regexec()` (for **reg**ular **ex**pression) function allows us to search for a pattern within a string. The output of `regexec()` gives you different information. For now it enough to know that it returns a list where the first object holds the position of the pattern in the string or simply a `-1` if the pattern is not found. In our case, we are going to look for the pattern "Max length" because after this pattern, the value we are looking for is saved (see Figure \@ref(fig:length)).

```{r length, echo=FALSE, fig.cap='In blue highlighted is the regular expression on the FishBase website which we are looking for.'}
knitr::include_graphics(rep("./figures/length.png"))
```

The code to identify the position where the regular expression *Max Length* is located is rather complex, so here is a step-by-step breakdown. You can run each part of the pipe below one-by-one to better understand what is happening.

```{r}
pos <- fishbase_span %>% lapply(xmlValue) %>%  # 1. Convert all list objects in fishbase_span into strings
  regexec(pattern =" Max length") %>%          # 2. Search strings for pattern 
  which.max()                                  # 3. Return position in list where highest number is found
                                               # (just one way to identify the node with relevant information)
pos
```

Now we know that our regular expression *Max length* can be found in node number 42. To get access to the text in this node we simply access `fishbase_span` via the list notation `[[ ]]` and turn the object into a string and save it as `fish_length`. As you can see in the output, the information on max length has been found correctly.

```{r}
fish_length <- fishbase_span[[pos]] %>% xmlValue()
fish_length
```

We now use the function `substr()`, which allows to extract a section of a string that lies between two characters. Think of this as extracting a number interval in a series of numbers. Unfortuntaly, identifying this start and end has to be done somewhat by hand. In Figure \@ref(fig:length) you can see that, starting from the *'M'* in *Max Length*, there are 13 characters until the value begins with a *7* and 16 characters until it ends with a *0*. Keep in mind that every blank space is also counted as character. Knowing this, we can first use `regexec()` to get the character position where *Max Length* starts and just add 13, respectively 16 to the value.

Two things to note here: (i) Keep in mind that every blank space is also counted. (ii) Note that the output of `regexec()` is a list where the first object is the character position where the pattern begins in the string. Thus, we need to access this position number by using `[[1]][1]`. Another way instead of using `regexec()` is just by typing in some numbers into `substr()` and find the relevant characters by trial and error.

```{r}
start_M <- fish_length %>%  regexec(pattern= "Max length")
max_length <- fish_length %>% substr(start_M[[1]][1] + 13,
                                     start_M[[1]][1] + 16)
max_length
```

At last, we have to convert the value of the maximal length (which is a string of characters) into a number using the function `as.double()`. This converts the value into a machine readable format which makes things easier later. We see from the output below that we correctly exctracted a maximal length of *Coregonus lavaretus* of 73 cm. Now, we can start thinking about looping this approach to extract the maximal length of various species and collect them in a nicely formatted data frame.

To demonstrate fexibility in data reading: This entire approach can also be used to for example to identify any numeric value that is followed by any one or enclosed by any two characters.

```{r}
max_length <- as.double(max_length)
max_length
```

### Scraping Text Snippetrs
#### International Union for Conservation of Nature {-}

Another interesting information on FishBase is the IUCN (International Union for Conservation of Nature) status of each species. The IUCN Red List of threatened Species has evolved to become the world’s most comprehensive information source on the global extinction risk status of animal, fungus and plant species.
 
The IUCN Red List is a critical indicator of world’s biodiversity and has been established in 1994. It contains explicit criteria and categories to classify the conservation status of individual species on the basis of their probability of extinction. After a species is evaluated by the IUCN, it is placed into one of eight categories based on its current conservation status as shown in the figure.

Far more than a list of species and their status, it is a powerful tool to inform and catalyze action for biodiversity conservation and policy change, critical to protecting the natural resources we depend on for survival. The IUCN also provides information about the range, population size, habitat and ecology, use and/or trade, threats, and conservation actions that will help inform necessary conservation decisions.

```{r iucn,echo=FALSE, fig.cap='Structure of IUCN categories.'}
knitr::include_graphics(rep("./figures/iucn.png"))
```

#### Extracting IUCN Status {-}

Now that we learned how to extract values, we can move on to extracting text snippets! In this part, we are going to get the IUCN Status of *Coregonus lavaretus*. In contrary to above, we are going to use the function `which()` (not `which.max()`) to find the position of the elements we are looking for. We use `regexec()` to search for matches of our pattern within each element of a character vector. In this case, the pattern is simply *IUCN*. As for the maximal length, we can look up this pattern on the website we are scraping from.

```{r}
IUCN_pos <- which(                 # which() gives us the numbers of nodes where pattern was found
  fishbase_div %>%                 # pipe from above to find pattern put into which()
    lapply(xmlValue) %>%   
    regexec(pattern = "IUCN")      # end of pipe
  > 0                              # check for which() to only give positive numbers for nodes
)                                  # we do this because if pattern is not found, regexec returns a -1
   
IUCN_pos                           # pattern can be found in nodes 5, 14 and 24
```

If the pattern cannot be found in `fishbase_div `, the variable `w_IUCN` will be empty (we use `fishbase_div` here because the pattern was not found in `fishbase_span`). To avoid complications later, we better do a quick check and set the variable to `NA` if it is empty and else use `xmlValue()` to get the value at the last node where the pattern was found.

```{r}
if(length(IUCN_pos)==0){              # if which() from above returned 0 (no node with >0)
  IUCN_stat = "NA"                    # set status to NA
} else {                              # else
  IUCN_xml <- fishbase_div[[          # access fishbase_div nodes with [[
    IUCN_pos[length(IUCN_pos)]]] %>%  # access last node where pattern was found, length(IUCN_pos) = 24
    xmlValue()                        # feed node into xmlValue() to return readable string
}

IUCN_xml
```

The `IUCN_xml` looks a little confusing and we only need a tiny part of it, namely *VU*. We can see that in this node *VU* has a unique character pattern which does not appear twice: The closing bracket `)`is right after `U`, an alphabetical character. We can use this pattern to extract *VU*! Here, the `str_extract()` function from tidyverse comes in very handy. We can directly specify the pattern we are looking for and extract the substring from the original string. Note that for generalization, we need to specify `[[:alpha:]]` for any alphabetical character `+[)]` for the closing bracket.

However, if we do so, we still extract the bracket as well. To get rid of it, we can simply remove it by replacing it with "nothing" using `str_replace()`. Instead of specifying the closing bracket, we use `[:punct:]` which generalizes punctuations like `. , : ; ? !` etc. As we see in the output below, the IUCN status *VU* has been correctly extracted - nice! Plus, we don't need to convert it and can use it as string later on.

```{r}
IUCN_sta <- IUCN_xml %>%
  str_extract(pattern = "[:alpha:]+[)]") %>%
  str_replace(pattern = "[:punct:]", replacement = "")

IUCN_sta
```

### Scraping Tables

The next step is to read a table from a website. Here, we are going to get information on the eggs of  *Coregonus lavaretus*. Have a look at its [profile](https://www.fishbase.de/summary/Coregonus-lavaretus.html). In section **Life cycle and mating behavior** you can see that the table for information on eggs is a link and not a table directly. Foruntatley for us, we can use the function `getHTMLLinks()` to retrieve links within an HTML document or the collection of names of external files referenced in an HTML document. In Figure \@ref(fig:eggs), an extract is shown of the list with more than 100 links found on the profile page.

We see that either we could directly access link number 100 or search for the substring *FishEggInfoSummary* to look for other links. In fact, we will find two links with this substring (see Figure \@ref(fig:outputlink). For this reason, we quickly check if the links are identical of if they lead to different pages. We can do this by using the logic operator `==`. The link comparison gives `TRUE` as output meaning that both links are completely identical and it does not matter which one we use. Let us save the first one in `egg_link`.

```{r}
egg_link <- fishbase %>% getHTMLLinks() %>%
  str_subset(pattern = "FishEggInfoSummary")

egg_link[1] == egg_link[2]
egg_link <- egg_link[1]
```
```{r eggs,echo=FALSE, fig.cap='Extract of the list of links found on the FishBase profile of *Coregonus lavaretus*.'}
knitr::include_graphics(rep("./figures/eggs.png"))
```
```{r outputlink,echo=FALSE, fig.cap='List of links that have the substring *FishEggInfoSummary*.'}
knitr::include_graphics(rep("./figures/outputlink.png"))
```


**Checkpoint**
Call the variable *egg_link*, you will notice the link begins with '..' (see above \@ref(fig:outputlink)). Try to remove these dots so that the link begins with '/Reproduction/.. '. As a hint, you can use the function `str_replace()` as was done above. You will need this code later, so make sure that you run it.

```{r, eval = FALSE}
# your code
```

**Solution**
```{r}
egg_link <- egg_link %>% str_replace("..", "" )
egg_link
```


As you can see, this is no proper URL yet. Thus, we first need to create a working URL to access its content. Similarly to what we did previously, we do this using the function `getURLContent()`. Since we know that this link leads to a table, we can pipe the URL content into `readHTMLTable()`. The output here is a list and we have access the first object as done below.

```{r}
egg_tab <- paste("http://www.fishbase.de/", egg_link, sep="") %>%
  getURLContent(followlocation=TRUE, .encoding="CE_UTF8") %>%
  readHTMLTable()

egg_tab <- egg_tab[[1]] # To save the list object as a data frame
egg_tab
```

Now we can extract information from this table by using the function `which()`. We want to find the position in this table, where *Shape of Egg* is defined. To identify the right row, we look in the first column for the respective string and extract the value stored in the second column of this row. To automatize, we add a check to see whether any information was extracted at all from the table. If not (as it is the case here), we just set `egg_shape` to NA.

```{r}
egg_shape <- egg_tab[                     # Accessing rows of egg_tab (remember: df[rows, cols])
  which(egg_tab[, 1] == "Shape of Egg"),  # Get number of row where "Shape of Egg" appears in the first column
  2]                                      # Access the second column

egg_shape

if(egg_shape == "") {egg_shape = "NA"}
egg_shape
```

Now we can put all the information into a data frame for the entry *Coregonus lavaretus* by using the function `tibble()`. Having done this entire data scraping for one species, we can start to automatize the process for multiple species and simply append the data to our data frame.

```{r}
species_df <- tibble(Species = "Coregonus-lavaretus", Length = max_length, IUCN = IUCN_sta, Egg = egg_shape)
species_df
```


**Checkpoint**
Try to create and add a new variable *egg_color* to the data frame and feed it with the respective information from our `egg_table` (follow the same process used for 'egg_shape'). 

```{r, eval = FALSE}
# your code 
```

**Solution**
```{r}
egg_color = egg_tab[which(egg_tab[, 1] == "Color of Eggs"), 2]  # extract information
if(egg_color == ""){egg_color = "NA"}                           # set missing information to NA
species_df <- tibble(species_df, Color = egg_color)             # Add new variable to data frane
species_df

```


### The Fishbase Package
#### Using an API {-}

 As we saw in the previous sections, web scraping can be tedious. In this subsection, we want to get data using an API which is a much easier way. We use the R package `rfishbase` to get data from [https://www.fishbase.de](https://www.fishbase.de). This package makes it very easy to look up for information on the most well-known fish species. It simplifies the data extraction process but also has some limits as we will see below. You ca have a look at the functions built into `rfishbase` by typing `help(package = "rfishbase")` into your RStudio console or have a look at the [RDocumentation](https://www.rdocumentation.org/packages/rfishbase/versions/3.0.4). Most functions have straight forward names and have a sring as input which holds the Latin name of the wanted fish species.
 
Let us get some information on *Coregonus lavaretus* with this new package! We see that the super short code `species("Coregonus lavaretus")`provides us with 101 variables with entries for our fish species - think about the hand-written web scraping code this would require! To directly asses the maximal length of the species, we can directly use the `$` notation. As you might recall, this is the same length as we got above.

```{r}
CL <- species("Coregonus lavaretus")
CL$Length
```

In the next step we are interested to get information about the diet of *Coregonus lavaretus* by using the function `diet_items()`. It allows us to access the table on the food items of the chosen species. To extract information on food, we can save the diet items in a table and use the tidyverse notation plus the respective variables *FoodI*, *FoodII*:

```{r}
food <- diet_items("Coregonus lavaretus") %>% as_tibble
food %>% dplyr::select(FoodI, FoodII) %>% head() # We're using dplyr:: here to avoid package conflicts
```

**Checkpoint**
Now your next task is to get information on predators (hint: the function is exactly named like that).

```{r, eval = FALSE}
# your code
```

**Solution**
```{r, eval}
# use function 'predators()' for Coregonus lavaretus
pre <- predators("Coregonus lavaretus")
pre %>% dplyr::select(PredatorName) %>% head()
```



#### Limits of APIs {-}

One of the major limitations of an API is that we can only use already implemented functions. For example, in our case, we cannot get the IUCN Status of a given species because it is not built into `rfishbase`. In order to obtain the IUCN Status we must use another API, namely the package `rredlist`. Unfortunately, to have access to the *rredlist* API we need an authentication key which we only get for a small fee. As it is with other things in life, either you try to put in the work yourself or you can pay someone to do so (except for the amazing open source community).

### Summary

- In this third section we learned how to extract data from a website. 
- We saw that this can be done either with web scraping or using APIs. 
- We saw that web scraping can be tedious, but we learned about some limitations of APIs.


## Case Study: Species Richness and Red List species proportions

*Please note that the code below is written in base R and not tidyverse which makes the codes a bit more difficult to read. But do not worry about this and simply just enjoy the beauty of what R is capable of.*

Next, we look at another case study on species richness and red list species proportions. To proceed with the case study we need to prepare our dataset. First, we need to get a list of species for the dataset. In this subsection we will see how to get all the species in a given family.


### Creating List of Species

In this section, we want to get all the species of the family of *Salmonidae*. As we did above, for flexibility we will create an object _x_ with the name of the family and use `paste()` to get the link.
We then use `getURLContent()` to get the content of the link `url` and save it in `con_sal`.

```{r}
x <- "Salmonidae"
url <- paste("http://www.fishbase.de/Nomenclature/FamilySearchList.php?Family=", x, sep="") 
con_sal <- getURLContent(url, followlocation = TRUE) 
```

Next we create a dataframe using `data.frame()` and get a list of variables. We will extract the variables with the same number of rows and unique row names. The function `readHTMLTable()` (from earlier) helps to extract data from HTML tables in an HTML document. Then we can extract the species from the given family with `as.character()`. We use `z[,1]` to get the first column which is the column with the scientific names of species. Now we can print the first element of the column with the scientific names.

```{r}
df <- data.frame(readHTMLTable(con_sal))
sp_per_family <- as.character(df[,1])
sp_per_family[1]
```

Using `str_replace()` function we can substitute the empty space between the Genus and the Species with a `"-"`. Finally we can print the first element of _sp_per_family_ again.

```{r}
sp_per_family <- str_replace(sp_per_family, " ", "-")
sp_per_family[1]
```


### Extracting IUCN Status for all species

In this section, we are going to get the IUCN Status for a given List of species using a 'for loop'. We will extract the IUCN Status of all the species from the Netherlands.
First, we are going to upload the dataset containing the list of the species and some other data that is going to be useful for the next sections. We will do that by using the function `read_csv()`. This data is taken from this [nature article](https://www.nature.com/articles/sdata2017141) by cropping it to Western Europe. As we only need data related to Netherlands, we will use the function `grep()` and pass _Netherlands_ as an argument.

```{r message=FALSE, warning=FALSE}
dataset <- read_csv("./data/dataset2.csv")
subset <- dataset[grep("Netherlands", dataset$Country),]
```

Let us first briefly discuss how to construct a 'for loop', since it's been a while since you used it in previous chapters (also see Chapter Chapter \@ref(ch-05) for examples). To get the IUCN Status of a list of species we always change the value of *x* (the species) and run the code, but if we have to do that for many species it will be very long and tedious. In this case 'for loops' come in handy. In a 'for loop' the variable *x* runs over the vector (here each species) and returns the IUCN Status. Before getting the IUCN status we will go over an easy example, such as printing the integers from 1 to 10 using a 'for loop'. In this case, we iterate over the vector 1:10. 

```{r}
for(j in 1:10) {
print(j) # this prints the value of j for that given loop
}
```

Now we can make a 'for loop' in order to get the IUCN status of all the species in the above subset (Netherlands) of the initial dataset. We are going to iterate over the column of the dataset with the valid FishBase species names. The code lines inside the loop are exactly a copy-paste of what we had for the *Coregonus lavaretus*, but in this case, we have to look for other species. In the last line of the code below, we created a new column in the data frame *subset* in order to save the IUCN Status. The 'i' in `subset$IUCN[i]` is used to save the IUCN status of the species we are iterating over in the 'for loop'. It will save the results of the species one by one. 

```{r message=FALSE, warning=FALSE}
i <- 0
for(x in subset$X6.Fishbase.Valid.Species.Name) {
  i <- i + 1
  
  url <- paste("http://www.fishbase.de/summary/",x,".html",sep="")       # we call the url
  fish_species <- htmlParse(getURLContent(url, followlocation=TRUE))     # get the content
  fish_species_div <-getNodeSet(fish_species, "//div ")                  # get the nodes with species
  w_IUCN  <- which(sapply(lapply(fish_species_div,xmlValue),function(x)  # look for the IUCN pattern 
    {regexec(pattern="IUCN", x)[[1]][1]})>0)
  
  if(length(w_IUCN)==0){                                                 # NA if no IUCN status
    IUCN_status="NA"
} else {                                                                 # else access information
    d1_IUCN  <- xmlValue(fish_species_div[[w_IUCN[length(w_IUCN)]]])
    IUCN <- unlist(regmatches(d1_IUCN,gregexpr(pattern= "[[:alpha:]]+)",     
    d1_IUCN)))
    IUCN_status <- sub(pattern="[[:punct:]]",replacement="",IUCN[1] ) 
}
  
print(IUCN_status)
subset$IUCN[i] <- IUCN_status # make a new column in 'subset' containing the IUCN status
}

```

We can see which IUCN statuses are present in Netherlands by using the `unique()` function. There are LC for least concern, VU for vulnerable and of course NA for unknown values.

```{r}
unique(subset$IUCN)
```


### Proportion of species in Netherlands

We will now plot the proportion of species in each category for the Netherlands. So let us calculate the number of species in each category (from above we just have 2 outputs,* *LC* & *NT*). In general in other countries, we also have other IUCN Status, for example, VU. In this section, we will focus only on the two listed statuses. Using the function `length()` we can obtain the number of species in each category.

```{r}
number_lc <- length(which(subset$IUCN == "LC"))
number_vu <- length(which(subset$IUCN == "VU"))
number_na <- length(which(is.na(subset$IUCN)))

# print the values for NT, VU and NA
paste0("LC:", " ", number_lc)
paste0("VU:", " ", number_vu)
paste0("NA:", " ", number_na)
```

We are ready to plot this as pie chart.


```{r}
slices <- c(number_lc, number_vu, number_na)
lbls <- c("LC","NT", "NA")
pie(slices, labels = lbls, font.main = 1, 
main = "Proportion of species per IUCN Status in Netherlands", col=c("red", "yellow", "grey")) 
```

### Cleaning data with IUCN Status

In this part of the tutorial, we have to clean the data that we will use for the correlations. We have provided the dataset already with the IUCN status since the code needs a lot of time to run, but the procedure is exactly the same as we did in the previous sections. So, let us load the dataset, it's called *datasetIUCN*.

In the previous sections, we saw that for some species we did not have information on the IUCN status. In these cases, we set the IUCN status as NA. We can check for possible NA values by calling the `unique()` function. 


```{r, message=FALSE, warning=FALSE}
dataset_IUCN <- read_csv("./data/datasetIUCN.csv")
unique(dataset_IUCN$IUCN)
```

**Checkpoint**
Now your task is to remove the row with the IUCN status as NA. You can use the function `subset()` to get the subset of the dataset with information about the IUCN status.

```{r, eval = FALSE}
# your code
```

**Solution**
```{r}
# subset the data without NAs (!=NA; not equal to NA), so this effectively removes NAs
dataset_IUCN_NA <- subset(dataset_IUCN, dataset_IUCN$IUCN != "NA")

# check if it worked
unique(dataset_IUCN_NA$IUCN)
```
Next, we will load the shapefile of glacial basins using the package `rgdal` and the function `readOGR()`. We fetch the data from different fish basins across Europe in the `basin_shapefile` from the data stored in the basins folder. In order to plot the basins on the map we will use `fortify()` function on the `basin_shapefile`. This function helps to convert a lines-and-points object into a data frame for ggplot. We will store this dataframe as `fort_basin`.

```{r message=FALSE, warning=FALSE}
basin_shapefile <- readOGR("./data/basins")
fort_basin <- fortify(basin_shapefile)
head(fort_basin)
```

We can visualise our `basin_shapefile` using `ggplot()`.


```{r, eval = FALSE}
ggplot() + geom_polygon(data = fort_basin, aes(x = long, y = lat, group = group), colour = "black", fill = NA)
```

```{r europe,echo=FALSE, fig.cap='Plot of `fort_basin`.'}
knitr::include_graphics(rep("./figures/output_186_0.png"))
```


### Maps of species richness and Red List species proportions
#### Preparing data {-}

We now want to map the species richness and Red List species proportions. Let us first calculate the species richness and proportion of Red List species per basin. Species richness is the number of species pro basin and the proportion of red list species is the ratio between the number of species in the red list and the number of species pro basin. We will iterate over the vector with the basin names and we will use the function `nrow()` to find the number of occurrences.  


```{r}
for (x in as.character(basin_shapefile$BasinName)) {
    
  # we now restrict to dataset to the basin x
  dataset_basins <- dataset_IUCN[dataset_IUCN$X1.Basin.Name==x,]
    
  # number of species in the given country x
  n1 <- nrow(dataset_basins)
    
  # we now restrict to dataset to the country x and Red List
  dataset_c_IUCN <- dataset_basins[grep("VU|EN|EX|EW|CR", dataset_basins$IUCN),] 
    
  # number of species in the given country x with given IUCN Status
  n2 <- nrow(dataset_c_IUCN)
    
  # compute the proportion 
  basin_shapefile$proportion[basin_shapefile$BasinName==x] <- n2/n1
  basin_shapefile$richness[basin_shapefile$BasinName==x] <- n1  
}
```

We can see the newly computed data for the columns *'proportion'* and *'richness'* with respect to each basin in the `basin_shapefile`. 

```{r}
head(basin_shapefile@data)
```
Next we get the map of Europe. We will read the data in `continent_shapefile` and then will extract the continent 'Europe' map. If you call the variable `europe` you will see that the dataframe is empty. This is because it extracts only the map data which we can simply plot by passing the europe as an argument in `plot()` function. We will do it in the upcoming code cells.

```{r message=FALSE, warning=FALSE}
continents <- readOGR('./data/continent_shapefile')
europe <- continents[continents$CONTINENT == 'Europe',]
```

    Warning message in readOGR("../data/continent_shapefile"):
    “First layer europe_map read; multiple layers present in
    /work/04_data_scraping/data/continent_shapefile, check layers with ogrListLayers()”


    OGR data source with driver: ESRI Shapefile 
    Source: "/work/04_data_scraping/data/continent_shapefile", layer: "europe_map"
    with 53 features
    It has 94 fields

#### Plotting data {-}

So now let us plot the proportions of Red List species. First, we will create a new column _'proportion_colour'_ and in this column, we will store the colours. Then we will break the proportions into 10 different parts by grouping the values in the proportion column into 10 using the `cut()` function. Then we get rid of the index vector using `as.numeric()` and get all the values as numeric values. We used the `rev()` function to reverse the colours, red colour indicates the species that are getting distinct and have very less proportion pro basin and yellow indicates the species with comparatively more proportion in the basin. Finally, we store these colours to the column proportion_colour. The colour indicates the proportion of Red List species occurring in the corresponding basin. We do the same for the species richness.

```{r}
basin_shapefile$proportion_colour <- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$proportion, breaks = 10))]
```

Now we create a new column in the _fort_basin_ dataframe and map the values of _'proportion_colour'_ into the new color column. We are doing this to have the colour (species proportion divided into 10 parts) and lat-long values in one datframe which helps to plot the graph using `ggplot()`. Remember, above we extracted the continent *'europe'* from the `continent_shapefile`, here we will use `fortiy()` on the europe dataset to plot it. In detail, we will plot the `fort_europe` and `fort_basin` data on the map.

```{r, eval = FALSE, message=FALSE, warning=FALSE}
fort_basin$color <- fort_basin$id   # create a new column color

for (i in as.numeric(unique(fort_basin$id))){   # map the values into color column by id
   fort_basin$color[fort_basin$id == i] <- basin_shapefile@data$proportion_colour[i+1]
}

fort_europe <- fortify(europe)
```

```{r, eval = FALSE}
ggplot(fort_basin, aes(x = long, y = lat, group = group)) +
  geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = 'white') +
  geom_polygon(fill = fort_basin$color, colour = "black")+
  xlim(-25, 28)
```

```{r europe2,echo=FALSE, fig.cap='Proportions of Red List species in Europe.'}
knitr::include_graphics(rep("./figures/output_201_0.png"))
```

From the Figure \@ref(fig:europe2), we see that the proportion of Red List species is highest in South-Western Europe. The aim of the Red List is to inform decision-makers about potentially endangered species, i.e. species whose population size has been rapidly declining during the last decades or the species that only occur in small numbers at present. The proportion of species on the Red List of each region, therefore, gives an indication of the risk of species going extinct in a region and represents an important tool for conservation strategies.

#### Repeat for mapping species richness {-}

We will repeat the above steps all together for plotting the species richness on map.

```{r, eval = FALSE}
# break the richness  of the species into 10 parts and then we assign colors to each part 
basin_shapefile$richness_colour <- rev(heat.colors(11))[as.numeric(cut(basin_shapefile$richness, breaks = 10))]

fort_basin$rich_color <- fort_basin$id    # create a new column rich_color

# mapping the values from basin_shapefile to fort_basin

for (i in as.numeric(unique(fort_basin$id))) 
{
   fort_basin$rich_color[fort_basin$id == i] <- basin_shapefile@data$richness_colour[i+1]
}

# plot
ggplot(fort_basin, aes(x = long, y = lat, group = group)) +
  geom_polygon(data = fort_europe, aes(x = long, y = lat, group = group), colour = 'white') +
  geom_polygon(fill = fort_basin$rich_color, colour = "black")+
  xlim(-25, 28)
```

```{r europe3,echo=FALSE, fig.cap='Species richness in Europe.'}
knitr::include_graphics(rep("./figures/output_204_0.png"))
```

### Relation of basin size and species richness

In the previous section, you observed the spatial patterns of fish diversity across Europe. In this section, we will try to explain these patterns. To achieve this, we will correlate the fish species richness of each basin to the surface area of the corresponding area to see how the species richness varies with respect to the surface area of the basin. We begin with plotting a simple scatterplot. We will log transform the data we have on surface area as it helps to make data conform to normality and also helps to deal with the outliers and skeweness in the data. Then we will plot this data against species richness. Since the data-deficient basins show zero observations in the dataframe, we will remove those first. 

Now we will create a new dataframe with the _surface area_ and _richness_. We'll rename the columns as _'Basin_area'_ and _'Species_richness'_ respectively. We make a simple scatterplot with a regression line to visualise the relationship.


```{r}
basin_shapefile <- basin_shapefile[basin_shapefile$richness!=0,]

bs_sr <- tibble(basin_shapefile@data$Surf_area, basin_shapefile@data$richness )
names(bs_sr)[1]<- 'Basin_area'
names(bs_sr)[2]<- 'Species_richness'

ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) +
geom_point() +
geom_smooth(method='lm', color="red", size=0.5, se=FALSE)+
xlab("Basin Area") +
ylab("Species Richness") +
theme_classic()
```

The plot shows a positive correlation between basin area and fish richness, meaning that we expect to see a higher richness in larger basins. This pattern is commonly observed in ecology and one explanation for this is that larger areas provide different habitat types (niches), which allows more species to co-exist.

As a next step, we will create a simple model of this relationship. This allows us to make predictions on the richness of fish species in other basins based on the basin area.

```{r}
richness_model <- lm(richness~log(Surf_area), data=basin_shapefile@data) #create a linear model
```

We now want to calculate the confidence intervals of the model to have a better idea of the uncertainty of the model. We will then coerce the basin surface, the model fit and the confidence interval into a new data frame called *model_df*.

```{r}
#calculate the confidence intervals
model_df <- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval='confidence')))
names(model_df)[1] <- 'log_area'
head(model_df)
```
The final step is  to plot the model fit and prediction intervals over the data. To get nice lines in the plot, the *model_df* dataframe needs to be ordered by the basin area first.

```{r}
ggplot(bs_sr, aes(x = log(Basin_area), y = Species_richness)) +
geom_point() +
geom_line(aes(x = model_df$log_area, y= model_df$fit ), color = "red")+
geom_line(aes(x = model_df$log_area, y= model_df$lwr ), color = "red", linetype = "dotted")+
geom_line(aes(x = model_df$log_area, y= model_df$upr ), color = "red", linetype = "dotted")+
xlab("Basin Area") +
ylab("Species Richness") +
theme_classic()
```


**Checkpoint**
You have now seen how you can calculate and plot the confidence interval of your data. Based on your model you can also create a so-called **prediction interval** which gives an estimate of the range in which the model will most likely predict the y-values (for a given x-value). In our case the prediction interval would indicate in which range the model would expect the fish richness to be for a given basin surface area. Do you think that these prediction intervals will be broader or narrower than the confidence interval? Try to write the code for calculating and plotting the prediction intervals by yourself.

```{r, eval = FALSE}
# your code
```

**Solution**
```{r, message=FALSE, warning=FALSE}
#prediction intervals

# make a dataframe of area and richness model predictions
model_df_prediction <- as.data.frame(cbind(log(basin_shapefile$Surf_area), predict(richness_model, interval='prediction')))
names(model_df_prediction)[1] <- 'log_area' 

# order the basins by size
model_df_prediction <- model_df_prediction[order(model_df_prediction$log_area),]

# plot the model
plot(log(basin_shapefile$Surf_area), basin_shapefile$richness, pch=16, cex=1, col=rgb(0,0,0,0.6), xlab='Basin area', ylab='Basin species richness')
lines(model_df_prediction$log_area, model_df_prediction$fit, col='skyblue3', lwd=2)
lines(model_df_prediction$log_area, model_df_prediction$lwr, col='skyblue2', lwd=2, lty=3)
lines(model_df_prediction$log_area, model_df_prediction$upr, col='skyblue2', lwd=2, lty=3)
```



### Additional Resources

* http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/styled-4/styled-6/code-13/
* https://ourcodingclub.github.io/tutorials/webscraping/
* https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/

## Exercise

For this week's exercise open up the Rstudio environment. Remember to save all your changes to this notebook using git status, git add <filename>, git commit -m "your comment", git push.
    
Today's exercise is about getting data from the web and extracting useful insights from it. 
    
Get in touch with your teaching assistant if you have any further questions.

This exercise is about getting the data from the web. Once you have finished the exercise please 'knit' this document to create an .html file, export it and upload it to moodle. 

### Part 1: Get the data

a. Following the steps described in the tutorial, get the maximal length of the species *Salvelinus alpinus* and the *Salmo trutta* using an API. After fetching the values create a table to store these values in one dataframe. 

```{r eval = F}
# enter your solution here
```

b. Get the **IUCN Status** and **eggs shape** for *Salvelinus alpinus* and *Salmo trutta* using web scraping. Again follow the same steps you learned in the tutorial. In the end, add the new information to the dataframe.

```{r eval = F}
# enter your solution here
```


### Part 2: Get all the species in a family and the IUCN status

a. Your next task is to get all the species in the family _'Neoscopelidae'_ and print the first 5 elements of the family.

```{r eval = F}
# enter your solution here
```

b. Extract the 'Native Exotic Status' of all the species from France. Then end get the unique values of the Native Exotic Status. For this task you will need to use the csv file _'dataset2'_.

```{r eval = F}
# enter your solution here
```

c. Now, use the information you got above to plot the proportion of species in each category for France.

```{r eval = F}
# enter your solution here
```


### Part 3: Extract the temperature of the species  

Extract the mean temperature for **Coregonus lavaretus** and **Salvelinus alpinus** using the function `stocks()` from `rfishbase` package and store the values in a new dataframe.

```{r eval = F}
# enter your solution here
```